{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahdrakes/mldl-final/blob/noah/mm_violence_det_model0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDOr6KirJvUe"
      },
      "source": [
        "# Multi-Modal Violence Detection Network\n",
        "\n",
        "original src code: https://github.com/Roc-Ng/XDVioDet.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhzPvDXTKKqM"
      },
      "source": [
        "### Copying Training and Testing Data\n",
        "\n",
        "The folders are pretty large (~40/50GB) so it takes a while to copy all of the data over.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4DG4K3wYK8P",
        "outputId": "cbbe01cc-7ff6-436d-c261-4783d69d23d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /mydrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/mydrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88GWL7hscvcg",
        "outputId": "47ac5aef-5997-4c3e-ece3-82407fd8172b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/mydrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /mydrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nr7SpsS-2P6x"
      },
      "outputs": [],
      "source": [
        "!unzip final_dl.zip -d /content/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zWKH9vsRatb"
      },
      "source": [
        "may need to change directory depending on where you upload the data to google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CHQ29LtvAsNj"
      },
      "outputs": [],
      "source": [
        "# !cp -r /mydrive/MyDrive/final_dl ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N1LDIKfKhpP"
      },
      "source": [
        "## 1. Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9acUBEhKqjC"
      },
      "source": [
        "### A) Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "IPMWSbm0LBGY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc, precision_recall_curve\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def test(dataloader, model, device, gt):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pred = torch.zeros(0).to(device)\n",
        "        pred2 = torch.zeros(0).to(device)\n",
        "        for i, input in enumerate(dataloader):\n",
        "            input = input.to(device)\n",
        "            logits, logits2 = model(inputs=input, seq_len=None)\n",
        "            logits = torch.squeeze(logits)\n",
        "            sig = torch.sigmoid(logits)\n",
        "            sig = torch.mean(sig, 0)\n",
        "            pred = torch.cat((pred, sig))\n",
        "            '''\n",
        "            online detection\n",
        "            '''\n",
        "            logits2 = torch.squeeze(logits2)\n",
        "            sig2 = torch.sigmoid(logits2)\n",
        "            sig2 = torch.mean(sig2, 0)\n",
        "\n",
        "            sig2 = torch.unsqueeze(sig2, 1) ##for audio\n",
        "            pred2 = torch.cat((pred2, sig2))\n",
        "\n",
        "            # print(\"pred:, \", pred)\n",
        "            # print(\"pred2:, \", pred2)\n",
        "\n",
        "        pred = list(pred.cpu().detach().numpy())\n",
        "        pred2 = list(pred2.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred, 16))\n",
        "        pr_auc = auc(recall, precision)\n",
        "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred2, 16))\n",
        "        pr_auc2 = auc(recall, precision)\n",
        "        return pr_auc, pr_auc2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLV269m9K7L3"
      },
      "source": [
        "### B) Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ej2MARCmKzvk"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def random_extract(feat, t_max):\n",
        "   r = np.random.randint(len(feat)-t_max)\n",
        "   return feat[r:r+t_max]\n",
        "\n",
        "def uniform_extract(feat, t_max):\n",
        "   r = np.linspace(0, len(feat)-1, t_max, dtype=np.uint16)\n",
        "   return feat[r, :]\n",
        "\n",
        "def pad(feat, min_len):\n",
        "    if np.shape(feat)[0] <= min_len:\n",
        "       return np.pad(feat, ((0, min_len-np.shape(feat)[0]), (0, 0)), mode='constant', constant_values=0)\n",
        "    else:\n",
        "       return feat\n",
        "\n",
        "def process_feat(feat, length, is_random=True):\n",
        "    if len(feat) > length:\n",
        "        if is_random:\n",
        "            return random_extract(feat, length)\n",
        "        else:\n",
        "            return uniform_extract(feat, length)\n",
        "    else:\n",
        "        return pad(feat, length)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKIYnHpLDKS"
      },
      "source": [
        "### C) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "98z1xwOKKueM"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "\n",
        "# from utils import process_feat\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, args, transform=None, mode='train'):\n",
        "        self.modality = args.modality\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args: Arguments containing dataset paths and configuration\n",
        "            transform: Optional transforms to apply\n",
        "            mode: One of ['train', 'val', 'test'] to specify the dataset split\n",
        "        \"\"\"\n",
        "\n",
        "        if mode == 'test':\n",
        "            self.rgb_list_file = args.test_rgb_list\n",
        "            self.flow_list_file = args.test_flow_list\n",
        "            self.audio_list_file = args.test_audio_list\n",
        "        elif mode == 'val':\n",
        "            self.rgb_list_file = args.val_rgb_list\n",
        "            self.flow_list_file = args.val_flow_list\n",
        "            self.audio_list_file = args.val_audio_list\n",
        "        else: # train\n",
        "            self.rgb_list_file = args.train_rgb_list\n",
        "            self.flow_list_file = args.train_flow_list\n",
        "            self.audio_list_file = args.train_audio_list\n",
        "\n",
        "\n",
        "        self.max_seqlen = args.max_seqlen\n",
        "        self.tranform = transform\n",
        "        self.test_mode = (mode == 'test')\n",
        "        self.normal_flag = '_label_A'\n",
        "        self._parse_list()\n",
        "\n",
        "    def _parse_list(self):\n",
        "        if self.modality == 'AUDIO':\n",
        "            self.list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'RGB':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            print(\"here\")\n",
        "            # print(self.list)\n",
        "        elif self.modality == 'FLOW':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX2':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX3':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        else:\n",
        "            assert 1 > 2, 'Modality is wrong!'\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.normal_flag in self.list[index]:\n",
        "            label = 0.0\n",
        "        else:\n",
        "            label = 1.0\n",
        "\n",
        "        if self.modality == 'AUDIO':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'RGB':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'FLOW':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'MIX':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2), axis=1)\n",
        "            else:\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX2':\n",
        "\n",
        "\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                min_len = min(features1.shape[0], features2.shape[0])\n",
        "                features1 = features1[:min_len]\n",
        "                features2 = features2[:min_len]\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "            features = np.concatenate((features1, features2), axis=1)\n",
        "\n",
        "        elif self.modality == 'MIX3':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index // 5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2), axis=1)\n",
        "            else:\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            features3 = np.array(np.load(self.audio_list[index // 5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2, features3), axis=1)\n",
        "            else:\n",
        "                features = np.concatenate((features1[:-1], features2, features3[:-1]), axis=1)\n",
        "        else:\n",
        "            print(\"WHAT IS WRONG\")\n",
        "            raise ValueError(\"Modality is wrong!\")\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.tranform is not None:\n",
        "            features = self.tranform(features)\n",
        "\n",
        "        # Handle test mode\n",
        "        if self.test_mode:\n",
        "            return features\n",
        "        else:\n",
        "            # Process features for training/validation\n",
        "            features = process_feat(features, self.max_seqlen, is_random=False)\n",
        "            return features, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvvMYO_1LXTV"
      },
      "source": [
        "### D) Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3bYn0UsFKWTy"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "from torch import FloatTensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(in_features, out_features).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
        "        self.a = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(2*out_features, 1).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        h = torch.mm(input, self.W)\n",
        "        N = h.size()[0]\n",
        "\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
        "\n",
        "class linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(linear, self).__init__()\n",
        "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.register_parameter('bias', None)\n",
        "        stdv = 1. / sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "    def forward(self, x):\n",
        "        x = x.matmul(self.weight)\n",
        "        return x\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=False, residual=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "        elif (in_features == out_features):\n",
        "            self.residual = lambda x: x\n",
        "        else:\n",
        "            # self.residual = linear(in_features, out_features)\n",
        "            self.residual = nn.Conv1d(in_channels=in_features, out_channels=out_features, kernel_size=5, padding=2)\n",
        "    def reset_parameters(self):\n",
        "        # stdv = 1. / sqrt(self.weight.size(1))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # To support batch operations\n",
        "        support = input.matmul(self.weight)\n",
        "        output = adj.matmul(support)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        if self.in_features != self.out_features and self.residual:\n",
        "            input = input.permute(0,2,1)\n",
        "            res = self.residual(input)\n",
        "            res = res.permute(0,2,1)\n",
        "            output = output + res\n",
        "        else:\n",
        "            output = output + self.residual(input)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "######################################################\n",
        "\n",
        "class SimilarityAdj(Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(SimilarityAdj, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight0 = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.weight1 = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # stdv = 1. / sqrt(self.weight0.size(1))\n",
        "        nn.init.xavier_uniform_(self.weight0)\n",
        "        nn.init.xavier_uniform_(self.weight1)\n",
        "\n",
        "    def forward(self, input, seq_len):\n",
        "        # To support batch operations\n",
        "        soft = nn.Softmax(1)\n",
        "        theta = torch.matmul(input, self.weight0)\n",
        "        phi = torch.matmul(input, self.weight0)\n",
        "        phi2 = phi.permute(0, 2, 1)\n",
        "        sim_graph = torch.matmul(theta, phi2)\n",
        "\n",
        "        theta_norm = torch.norm(theta, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        phi_norm = torch.norm(phi, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        x_norm_x = theta_norm.matmul(phi_norm.permute(0, 2, 1))\n",
        "        sim_graph = sim_graph / (x_norm_x + 1e-20)\n",
        "\n",
        "        output = torch.zeros_like(sim_graph)\n",
        "        if seq_len is None:\n",
        "            for i in range(sim_graph.shape[0]):\n",
        "                tmp = sim_graph[i]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = sim_graph[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "class DistanceAdj(Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DistanceAdj, self).__init__()\n",
        "        self.sigma = Parameter(FloatTensor(1))\n",
        "        self.sigma.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, batch_size, max_seqlen):\n",
        "        # To support batch operations\n",
        "        self.arith = np.arange(max_seqlen).reshape(-1, 1)\n",
        "        dist = pdist(self.arith, metric='cityblock').astype(np.float32)\n",
        "        self.dist = torch.from_numpy(squareform(dist)).to('cuda')\n",
        "        self.dist = torch.exp(-self.dist / torch.exp(torch.tensor(1.)))\n",
        "        self.dist = torch.unsqueeze(self.dist, 0).repeat(batch_size, 1, 1).to('cuda')\n",
        "        return self.dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44eeeOUcLquB"
      },
      "source": [
        "### E) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q8Rjvb-yKOeZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as torch_init\n",
        "import os\n",
        "# from layers import GraphConvolution, SimilarityAdj, DistanceAdj\n",
        "\n",
        "\n",
        "def weight_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
        "        torch_init.xavier_uniform_(m.weight)\n",
        "        # m.bias.data.fill_(0.1)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        n_features = args.feature_size\n",
        "        n_class = args.num_classes\n",
        "\n",
        "        self.conv1d1 = nn.Conv1d(in_channels=n_features, out_channels=512, kernel_size=1, padding=0)\n",
        "        self.conv1d2 = nn.Conv1d(in_channels=512, out_channels=128, kernel_size=1, padding=0)\n",
        "        self.conv1d3 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=5, padding=2)\n",
        "        self.conv1d4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
        "        # Graph Convolution\n",
        "        self.gc1 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc2 = GraphConvolution(32, 32, residual=True)\n",
        "        self.gc3 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc4 = GraphConvolution(32, 32, residual=True)\n",
        "        self.gc5 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc6 = GraphConvolution(32, 32, residual=True)\n",
        "        self.simAdj = SimilarityAdj(n_features, 32)\n",
        "        self.disAdj = DistanceAdj()\n",
        "\n",
        "        self.classifier = nn.Linear(32*3, n_class)\n",
        "        self.approximator = nn.Sequential(nn.Conv1d(128, 64, 1, padding=0), nn.ReLU(),\n",
        "                                          nn.Conv1d(64, 32, 1, padding=0), nn.ReLU())\n",
        "        self.conv1d_approximator = nn.Conv1d(32, 1, 5, padding=0)\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.apply(weight_init)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, inputs, seq_len):\n",
        "        x = inputs.permute(0, 2, 1)  # for conv1d\n",
        "        x = self.relu(self.conv1d1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.conv1d2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        logits = self.approximator(x)\n",
        "        logits = F.pad(logits, (4, 0))\n",
        "        logits = self.conv1d_approximator(logits)\n",
        "        logits = logits.permute(0, 2, 1)\n",
        "        x = x.permute(0, 2, 1)  # b*t*c\n",
        "\n",
        "        ## gcn\n",
        "        scoadj = self.sadj(logits.detach(), seq_len)\n",
        "        adj = self.adj(inputs, seq_len)\n",
        "        disadj = self.disAdj(x.shape[0], x.shape[1])\n",
        "        x1_h = self.relu(self.gc1(x, adj))\n",
        "        x1_h = self.dropout(x1_h)\n",
        "        x2_h = self.relu(self.gc3(x, disadj))\n",
        "        x2_h = self.dropout(x2_h)\n",
        "        x3_h = self.relu(self.gc5(x, scoadj))\n",
        "        x3_h = self.dropout(x3_h)\n",
        "        x1 = self.relu(self.gc2(x1_h, adj))\n",
        "        x1 = self.dropout(x1)\n",
        "        x2 = self.relu(self.gc4(x2_h, disadj))\n",
        "        x2 = self.dropout(x2)\n",
        "        x3 = self.relu(self.gc6(x3_h, scoadj))\n",
        "        x3 = self.dropout(x3)\n",
        "        x = torch.cat((x1, x2, x3), 2)\n",
        "        x = self.classifier(x)\n",
        "        return x, logits\n",
        "\n",
        "    def sadj(self, logits, seq_len):\n",
        "        lens = logits.shape[1]\n",
        "        soft = nn.Softmax(1)\n",
        "        logits2 = self.sigmoid(logits).repeat(1, 1, lens)\n",
        "        tmp = logits2.permute(0, 2, 1)\n",
        "        adj = 1. - torch.abs(logits2 - tmp)\n",
        "        self.sig = lambda x:1/(1+torch.exp(-((x-0.5))/0.1))\n",
        "        adj = self.sig(adj)\n",
        "        output = torch.zeros_like(adj)\n",
        "        if seq_len is None:\n",
        "            for i in range(logits.shape[0]):\n",
        "                tmp = adj[i]\n",
        "                adj2 = soft(tmp)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = adj[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = soft(tmp)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "        return output\n",
        "\n",
        "\n",
        "    def adj(self, x, seq_len):\n",
        "        soft = nn.Softmax(1)\n",
        "        x2 = x.matmul(x.permute(0,2,1)) # B*T*T\n",
        "        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        x_norm_x = x_norm.matmul(x_norm.permute(0,2,1))\n",
        "        x2 = x2/(x_norm_x+1e-20)\n",
        "        output = torch.zeros_like(x2)\n",
        "        if seq_len is None:\n",
        "            for i in range(x.shape[0]):\n",
        "                tmp = x2[i]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = x2[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE8lvyeyTk0C",
        "outputId": "7aaaea0c-4443-4d71-eb00-aaa01e97667f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory: /mydrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Current directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z-JVZlBNeg4"
      },
      "source": [
        "## Args\n",
        "\n",
        "Here are the default args that were obtained via cmd line arg parser. I just created a class 'Args' that holds the default config for the model.\n",
        "\n",
        "I think the most important args:\n",
        "\n",
        "*`Modality`*: Determines whether we want to use either audio alone, video alone, both audio and video, audio, video, and flow, etc. for training\n",
        "\n",
        "*`List`*: point to the list containing filenames for all training and testing data.\n",
        "\n",
        "*`workers`*: I believe this is the number of individual threads/processes running during training or testing. In ther model it was set to 4 by defualt but that spit out an error so it lowered it to 1. Prob a sign that we need to do heavy downsampling to compensate for lack of parallel processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0g_4ciyOM8tl"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.modality = 'MIX2'\n",
        "        # Original paths\n",
        "        self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "        self.flow_list = '/content/final_dl/list/flow.list'\n",
        "        self.audio_list = '/content/final_dl/list/audio.list'\n",
        "\n",
        "        # Train paths\n",
        "        self.train_rgb_list = '/content/final_dl/list/rgb_train.list'\n",
        "        self.train_flow_list = '/content/final_dl/list/flow_train.list'\n",
        "        self.train_audio_list = '/content/final_dl/list/audio_train.list'\n",
        "\n",
        "        # Val paths\n",
        "        self.val_rgb_list = '/content/final_dl/list/rgb_val.list'\n",
        "        self.val_flow_list = '/content/final_dl/list/flow_val.list'\n",
        "        self.val_audio_list = '/content/final_dl/list/audio_val.list'\n",
        "\n",
        "        # Test paths\n",
        "        self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "        self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "        self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "\n",
        "        self.gt = '/content/final_dl/list/gt.npy'\n",
        "        self.gpus = 1\n",
        "        self.lr = 0.0001\n",
        "        self.batch_size = 128\n",
        "        self.workers = 1  # Reduced from 4 to avoid memory issues\n",
        "        self.model_name = 'wsanodet'\n",
        "        self.pretrained_ckpt = None\n",
        "        self.feature_size = 1152  # 1024 + 128\n",
        "        self.num_classes = 1\n",
        "        self.dataset_name = 'XD-Violence'\n",
        "        self.max_seqlen = 200\n",
        "        self.max_epoch = 50\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xM7Rywzs4s8"
      },
      "source": [
        "## Val Split For MultiModal Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NRVNY_NEcEB5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "def get_video_id(filepath):\n",
        "    \"\"\"Extract video ID from filepath based on common prefix before _label\n",
        "    e.g., \"/path/to/video123_label_A.npy\" -> \"video123\"\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(filepath)\n",
        "    if '_label' in filename:\n",
        "        return filename.split('_label')[0]\n",
        "    return filename.split('.')[0]\n",
        "\n",
        "def find_matching_files():\n",
        "    \"\"\"\n",
        "    Find and align RGB and audio feature files.\n",
        "    Returns dict mapping video IDs to their RGB and audio paths\n",
        "    \"\"\"\n",
        "    rgb_path = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
        "    audio_path = \"/content/final_dl/list/xx/train\"\n",
        "\n",
        "    # Get all files\n",
        "    rgb_files = glob.glob(os.path.join(rgb_path, \"*.npy\"))\n",
        "    audio_files = glob.glob(os.path.join(audio_path, \"*.npy\"))\n",
        "\n",
        "    # Create mappings that preserve the 5:1 ratio\n",
        "    rgb_map = {}\n",
        "    for f in rgb_files:\n",
        "        vid_id = get_video_id(f)\n",
        "        if vid_id not in rgb_map:\n",
        "            rgb_map[vid_id] = []\n",
        "        rgb_map[vid_id].append(f)\n",
        "\n",
        "    audio_map = {get_video_id(f): f for f in audio_files}\n",
        "\n",
        "    # Find common video IDs\n",
        "    common_ids = set(rgb_map.keys()) & set(audio_map.keys())\n",
        "\n",
        "    # Create aligned mapping\n",
        "    aligned_files = {\n",
        "        vid_id: {\n",
        "            'rgb': sorted(rgb_map[vid_id]),  # Sort to maintain consistent ordering\n",
        "            'audio': audio_map[vid_id],\n",
        "            'is_normal': '_label_A' in rgb_map[vid_id][0]  # Check first RGB file for label\n",
        "        }\n",
        "        for vid_id in common_ids\n",
        "    }\n",
        "\n",
        "    print(f\"Found {len(aligned_files)} aligned RGB-Audio pairs\")\n",
        "    return aligned_files\n",
        "\n",
        "def create_splits(aligned_files, train_ratio=0.8, seed=42):\n",
        "    \"\"\"Split the video IDs first, then we'll expand to files in write_list_files\"\"\"\n",
        "    random.seed(seed)\n",
        "    video_ids = list(aligned_files.keys())\n",
        "    train_size = int(len(video_ids) * train_ratio)\n",
        "    train_ids = random.sample(video_ids, train_size)\n",
        "    val_ids = [vid for vid in video_ids if vid not in train_ids]\n",
        "\n",
        "    return {\n",
        "        'train': train_ids,\n",
        "        'val': val_ids\n",
        "    }\n",
        "\n",
        "def write_list_files(split_data, aligned_files, output_dir=\"/content/final_dl/list\"):\n",
        "    \"\"\"Write list files with audio files repeated to match RGB structure\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for split_name, video_ids in split_data.items():\n",
        "        # RGB list - one entry per frame\n",
        "        rgb_path = os.path.join(output_dir, f'rgb_{split_name}.list')\n",
        "        with open(rgb_path, 'w') as f:\n",
        "            for vid_id in video_ids:\n",
        "                for rgb_file in aligned_files[vid_id]['rgb']:\n",
        "                    f.write(f\"{rgb_file}\\n\")\n",
        "\n",
        "        # Audio list - one entry per video (not per frame)\n",
        "        audio_path = os.path.join(output_dir, f'audio_{split_name}.list')\n",
        "        with open(audio_path, 'w') as f:\n",
        "            for vid_id in video_ids:\n",
        "                audio_file = aligned_files[vid_id]['audio']\n",
        "                f.write(f\"{audio_file}\\n\")  # Write once only\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJzaD9WsT3YV"
      },
      "source": [
        "## Create Dataloaders for multimodal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVCYAPTSTuQY",
        "outputId": "39f38a91-27a7-4f3d-ee7c-b73a1f0a28c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3953 aligned RGB-Audio pairs\n",
            "<__main__.Args object at 0x7cc79fb472e0>\n",
            "Creating data loaders...\n",
            "audio size\n",
            "3162\n",
            "rgb size\n",
            "15815\n",
            "Train loader created with 15815 samples\n",
            "Validation loader created with 3955 samples\n",
            "Test loader created with 4000 samples\n"
          ]
        }
      ],
      "source": [
        "aligned_files = find_matching_files()\n",
        "\n",
        "    # Create train/val splits\n",
        "split_data = create_splits(aligned_files)\n",
        "\n",
        "    # Write list files\n",
        "write_list_files(split_data, aligned_files)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "def create_data_loaders(args):\n",
        "    \"\"\"\n",
        "    Create train, validation and test data loaders\n",
        "    \"\"\"\n",
        "\n",
        "    print(args)\n",
        "    print(\"Creating data loaders...\")\n",
        "\n",
        "    # Create train loader\n",
        "    train_dataset = Dataset(args, mode='train')\n",
        "\n",
        "    print(\"audio size\")\n",
        "    print(len(train_dataset.audio_list))\n",
        "\n",
        "    print(\"rgb size\")\n",
        "    print(len(train_dataset.list))\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    print(f\"Train loader created with {len(train_dataset)} samples\")\n",
        "\n",
        "    # Create validation loader\n",
        "    val_dataset = Dataset(args, mode='val')\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,  # No need to shuffle validation data\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    print(f\"Validation loader created with {len(val_dataset)} samples\")\n",
        "\n",
        "    # Create test loader with smaller batch size as per original code\n",
        "    test_dataset = Dataset(args, mode='test')\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=5,  # Using smaller batch size for testing\n",
        "        shuffle=False,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    print(f\"Test loader created with {len(test_dataset)} samples\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = create_data_loaders(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kh7Sa6Iu23Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsN1t5SrTQoU"
      },
      "source": [
        "## VAL SPLIT FOR SINGLE MODALITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y7aaqgH5TP66"
      },
      "outputs": [],
      "source": [
        "def create_single_modality_data_loaders(args, modality='AUDIO'):\n",
        "    \"\"\"\n",
        "    Create train, validation and test data loaders for a single modality\n",
        "    \"\"\"\n",
        "    print(f\"Creating {modality} data loaders...\")\n",
        "\n",
        "    # Create new args with only needed attributes\n",
        "    args_new = Args()\n",
        "    args_new.modality = modality\n",
        "\n",
        "    # List files needed for train/val/test splits\n",
        "    if modality == 'AUDIO':\n",
        "        args_new.train_audio_list = args.train_audio_list\n",
        "        args_new.val_audio_list = args.val_audio_list\n",
        "        args_new.test_audio_list = args.test_audio_list\n",
        "    elif modality == 'RGB':\n",
        "        args_new.train_rgb_list = args.train_rgb_list\n",
        "        args_new.val_rgb_list = args.val_rgb_list\n",
        "        args_new.test_rgb_list = args.test_rgb_list\n",
        "    elif modality == 'FLOW':\n",
        "        args_new.train_flow_list = args.train_flow_list\n",
        "        args_new.val_flow_list = args.val_flow_list\n",
        "        args_new.test_flow_list = args.test_flow_list\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataset = Dataset(args_new, mode='train')\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args_new.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args_new.workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    print(f\"Train loader created with {len(train_dataset)} samples\")\n",
        "\n",
        "    val_dataset = Dataset(args_new, mode='val')\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=args_new.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args_new.workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    print(f\"Validation loader created with {len(val_dataset)} samples\")\n",
        "\n",
        "    test_dataset = Dataset(args_new, mode='test')\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=args_new.workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    print(f\"Test loader created with {len(test_dataset)} samples\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNM_LV6qT_q0"
      },
      "source": [
        "## Create data loader for specified modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OtwEJzeJTr-i"
      },
      "outputs": [],
      "source": [
        "# For audio only\n",
        "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='AUDIO')\n",
        "\n",
        "# For RGB only\n",
        "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='RGB')\n",
        "\n",
        "# For flow only\n",
        "## CURRENTLY, FLOW IS NOT SUPPORTED, BUT IMPLEMENTING IT WOULD NOT BE THAT CHALLENGING. YOU WOULD SIMPLY HAVE TO\n",
        "## ADJUST THE CODE A FEW CELLS ABOVE SO TO WRITE AN EQUIVALENT OF \"find_matching_files\" FOR FLOW DATA\n",
        "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='FLOW')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LWZIdanUgWCK",
        "outputId": "a1fdc0ac-c23c-42a5-f89b-c9f3d37d992b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting individual samples:\n",
            "\n",
            "Sample 0:\n",
            "RGB file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__0.npy\n",
            "RGB shape: (540, 1024)\n",
            "Audio file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__vggish.npy\n",
            "Audio shape: (540, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 1:\n",
            "RGB file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__1.npy\n",
            "RGB shape: (540, 1024)\n",
            "Audio file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__vggish.npy\n",
            "Audio shape: (540, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 2:\n",
            "RGB file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__2.npy\n",
            "RGB shape: (540, 1024)\n",
            "Audio file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__vggish.npy\n",
            "Audio shape: (540, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 3:\n",
            "RGB file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__3.npy\n",
            "RGB shape: (540, 1024)\n",
            "Audio file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__vggish.npy\n",
            "Audio shape: (540, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 4:\n",
            "RGB file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__4.npy\n",
            "RGB shape: (540, 1024)\n",
            "Audio file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__vggish.npy\n",
            "Audio shape: (540, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 5:\n",
            "RGB file: v=iTvBb-HRbKs__#1_label_B1-0-0__0.npy\n",
            "RGB shape: (297, 1024)\n",
            "Audio file: v=iTvBb-HRbKs__#1_label_B1-0-0__vggish.npy\n",
            "Audio shape: (297, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 6:\n",
            "RGB file: v=iTvBb-HRbKs__#1_label_B1-0-0__1.npy\n",
            "RGB shape: (297, 1024)\n",
            "Audio file: v=iTvBb-HRbKs__#1_label_B1-0-0__vggish.npy\n",
            "Audio shape: (297, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 7:\n",
            "RGB file: v=iTvBb-HRbKs__#1_label_B1-0-0__2.npy\n",
            "RGB shape: (297, 1024)\n",
            "Audio file: v=iTvBb-HRbKs__#1_label_B1-0-0__vggish.npy\n",
            "Audio shape: (297, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 8:\n",
            "RGB file: v=iTvBb-HRbKs__#1_label_B1-0-0__3.npy\n",
            "RGB shape: (297, 1024)\n",
            "Audio file: v=iTvBb-HRbKs__#1_label_B1-0-0__vggish.npy\n",
            "Audio shape: (297, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 9:\n",
            "RGB file: v=iTvBb-HRbKs__#1_label_B1-0-0__4.npy\n",
            "RGB shape: (297, 1024)\n",
            "Audio file: v=iTvBb-HRbKs__#1_label_B1-0-0__vggish.npy\n",
            "Audio shape: (297, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 10:\n",
            "RGB file: v=XAZIC6RlSXs__#1_label_A__0.npy\n",
            "RGB shape: (981, 1024)\n",
            "Audio file: v=XAZIC6RlSXs__#1_label_A__vggish.npy\n",
            "Audio shape: (981, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 11:\n",
            "RGB file: v=XAZIC6RlSXs__#1_label_A__1.npy\n",
            "RGB shape: (981, 1024)\n",
            "Audio file: v=XAZIC6RlSXs__#1_label_A__vggish.npy\n",
            "Audio shape: (981, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 12:\n",
            "RGB file: v=XAZIC6RlSXs__#1_label_A__2.npy\n",
            "RGB shape: (981, 1024)\n",
            "Audio file: v=XAZIC6RlSXs__#1_label_A__vggish.npy\n",
            "Audio shape: (981, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 13:\n",
            "RGB file: v=XAZIC6RlSXs__#1_label_A__3.npy\n",
            "RGB shape: (981, 1024)\n",
            "Audio file: v=XAZIC6RlSXs__#1_label_A__vggish.npy\n",
            "Audio shape: (981, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 14:\n",
            "RGB file: v=XAZIC6RlSXs__#1_label_A__4.npy\n",
            "RGB shape: (981, 1024)\n",
            "Audio file: v=XAZIC6RlSXs__#1_label_A__vggish.npy\n",
            "Audio shape: (981, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 15:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__0.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 16:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__1.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 17:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__2.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 18:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__3.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 19:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__4.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 20:\n",
            "RGB file: v=H9Hta84rfGw__#1_label_B4-0-0__0.npy\n",
            "RGB shape: (288, 1024)\n",
            "Audio file: v=H9Hta84rfGw__#1_label_B4-0-0__vggish.npy\n",
            "Audio shape: (288, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 21:\n",
            "RGB file: v=H9Hta84rfGw__#1_label_B4-0-0__1.npy\n",
            "RGB shape: (288, 1024)\n",
            "Audio file: v=H9Hta84rfGw__#1_label_B4-0-0__vggish.npy\n",
            "Audio shape: (288, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 22:\n",
            "RGB file: v=H9Hta84rfGw__#1_label_B4-0-0__2.npy\n",
            "RGB shape: (288, 1024)\n",
            "Audio file: v=H9Hta84rfGw__#1_label_B4-0-0__vggish.npy\n",
            "Audio shape: (288, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 23:\n",
            "RGB file: v=H9Hta84rfGw__#1_label_B4-0-0__3.npy\n",
            "RGB shape: (288, 1024)\n",
            "Audio file: v=H9Hta84rfGw__#1_label_B4-0-0__vggish.npy\n",
            "Audio shape: (288, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 24:\n",
            "RGB file: v=H9Hta84rfGw__#1_label_B4-0-0__4.npy\n",
            "RGB shape: (288, 1024)\n",
            "Audio file: v=H9Hta84rfGw__#1_label_B4-0-0__vggish.npy\n",
            "Audio shape: (288, 128)\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Testing the val splitter, no need to run this\n",
        "\n",
        "def inspect_batch_files(args, batch_size=1, num_batches=5):\n",
        "    \"\"\"\n",
        "    Inspect the first few batches to see which files are being loaded and their dimensions\n",
        "    \"\"\"\n",
        "    #from torch.utils.data import DataLoader\n",
        "    #from dataset import Dataset  # Your dataset class\n",
        "\n",
        "    dataset = Dataset(args, mode='train')\n",
        "\n",
        "    print(\"Inspecting individual samples:\")\n",
        "    for i in range(min(25, len(dataset))):\n",
        "        try:\n",
        "            # Get the filepaths that would be loaded\n",
        "            rgb_path = dataset.list[i].strip('\\n')\n",
        "            audio_path = dataset.audio_list[i//5].strip('\\n')\n",
        "\n",
        "            # Load the features\n",
        "            features1 = np.array(np.load(rgb_path), dtype=np.float32)\n",
        "            features2 = np.array(np.load(audio_path), dtype=np.float32)\n",
        "\n",
        "            print(f\"\\nSample {i}:\")\n",
        "            print(f\"RGB file: {os.path.basename(rgb_path)}\")\n",
        "            print(f\"RGB shape: {features1.shape}\")\n",
        "            print(f\"Audio file: {os.path.basename(audio_path)}\")\n",
        "            print(f\"Audio shape: {features2.shape}\")\n",
        "\n",
        "            # Try the concatenation\n",
        "            try:\n",
        "                if features1.shape[0] != features2.shape[0]:\n",
        "                    print(\"⚠️ Dimension mismatch!\")\n",
        "                    if features1.shape[0] - 1 == features2.shape[0]:\n",
        "                        print(\"Would work with [:-1] slice\")\n",
        "                    features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "                    print(\"Concatenation successful after adjustment\")\n",
        "            except ValueError as e:\n",
        "                print(f\"❌ Concatenation failed: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading sample {i}: {str(e)}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Run the inspection\n",
        "args.modality = \"MIX2\"\n",
        "inspect_batch_files(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NCyNXLPN5u0"
      },
      "source": [
        "## Testing PreTrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "collapsed": true,
        "id": "nkT8DU7V3HOz",
        "outputId": "22d2b212-4dc2-42b0-baae-c978a33a3ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-6958b6b31f24>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  {k.replace('module.', ''): v for k, v in torch.load('/content/final_dl/wsanodet_mix2.pkl').items()})\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-6-c430d01cb6ef>\", line 83, in __getitem__\n    features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 427, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/media/peng/Samsung_T5/i3d-features/RGBTest/Bad.Boys.1995__#01-11-55_01-12-40_label_G-B2-B6__0.npy'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6958b6b31f24>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mpr_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr_auc_online\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr_auc_online\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5225f3c40b5e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(dataloader, model, device, gt)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-6-c430d01cb6ef>\", line 83, in __getitem__\n    features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 427, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/media/peng/Samsung_T5/i3d-features/RGBTest/Bad.Boys.1995__#01-11-55_01-12-40_label_G-B2-B6__0.npy'\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "# from model import Model\n",
        "# from dataset import Dataset\n",
        "# from test import test\n",
        "# import option\n",
        "import time\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  test_loader = DataLoader(Dataset(args, mode='test'),\n",
        "                            batch_size=5, shuffle=False,\n",
        "                            num_workers=args.workers, pin_memory=True)\n",
        "  model = Model(args)\n",
        "  model = model.to(device)\n",
        "  # had to change path to \"/content/final_dl/wsanodet_mix2.pkl\"\n",
        "  model_dict = model.load_state_dict(\n",
        "      {k.replace('module.', ''): v for k, v in torch.load('/content/final_dl/wsanodet_mix2.pkl').items()})\n",
        "\n",
        "  gt = np.load(args.gt)\n",
        "  st = time.time()\n",
        "  pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "  print('Time:{}'.format(time.time()-st))\n",
        "  print('offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIbLkkivMhx"
      },
      "source": [
        "how to save a model for the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7mSdUwSvLTq"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"/content/test.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lKM8M6MJlaBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0S5vwXjQzID"
      },
      "source": [
        "# Training HLNET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Lv0i1DYkQ2Qj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def CLAS(logits, label, seq_len, criterion, device, is_topk=True):\n",
        "    logits = logits.squeeze()\n",
        "    instance_logits = torch.zeros(0).to(device)  # tensor([])\n",
        "    for i in range(logits.shape[0]):\n",
        "        if is_topk:\n",
        "            tmp, _ = torch.topk(logits[i][:seq_len[i]], k=int(seq_len[i]//16+1), largest=True)\n",
        "            tmp = torch.mean(tmp).view(1)\n",
        "        else:\n",
        "            tmp = torch.mean(logits[i, :seq_len[i]]).view(1)\n",
        "        instance_logits = torch.cat((instance_logits, tmp))\n",
        "\n",
        "    instance_logits = torch.sigmoid(instance_logits)\n",
        "\n",
        "    clsloss = criterion(instance_logits, label)\n",
        "    return clsloss\n",
        "\n",
        "\n",
        "def CENTROPY(logits, logits2, seq_len, device):\n",
        "    instance_logits = torch.tensor(0).to(device)  # tensor([])\n",
        "    for i in range(logits.shape[0]):\n",
        "        tmp1 = torch.sigmoid(logits[i, :seq_len[i]]).squeeze()\n",
        "        tmp2 = torch.sigmoid(logits2[i, :seq_len[i]]).squeeze()\n",
        "        loss = torch.mean(-tmp1.detach() * torch.log(tmp2))\n",
        "        instance_logits = instance_logits + loss\n",
        "    instance_logits = instance_logits/logits.shape[0]\n",
        "    return instance_logits\n",
        "\n",
        "\n",
        "def train(dataloader, model, optimizer, criterion, device, is_topk):\n",
        "    with torch.set_grad_enabled(True):\n",
        "        model.train()\n",
        "        for i, (input, label) in enumerate(dataloader):\n",
        "            seq_len = torch.sum(torch.max(torch.abs(input), dim=2)[0]>0, 1)\n",
        "            input = input[:, :torch.max(seq_len), :]\n",
        "            input, label = input.float().to(device), label.float().to(device)\n",
        "            logits, logits2 = model(input, seq_len)\n",
        "            clsloss = CLAS(logits, label, seq_len, criterion, device, is_topk)\n",
        "            clsloss2 = CLAS(logits2, label, seq_len, criterion, device, is_topk)\n",
        "            croloss = CENTROPY(logits, logits2, seq_len, device)\n",
        "\n",
        "            total_loss = clsloss + clsloss2 + 5*croloss\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcLzmEr4HIQY",
        "outputId": "8f41022e-ac93-485c-828d-e6b64887c671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/audio.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/audio.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/list/xx/train\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/audio.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mubL6MwpIS8k",
        "outputId": "7f156762-c665-4054-8f99-796cdf2c24b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/rgb.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/rgb.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/rgb.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYz5pRU7Iyf8"
      },
      "source": [
        "## test (ignore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "hTEZP14qIyOD",
        "outputId": "d7e566c9-975e-4280-e323-7809187f6c05"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-8-9ae98fe16be2>\", line 83, in __getitem__\n    features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 427, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/media/peng/Samsung_T5/i3d-features/RGBTest/Before.Sunrise.1995__#00-04-20_00-05-35_label_A__3.npy'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6ea78ee82da1>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-8-9ae98fe16be2>\", line 83, in __getitem__\n    features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 427, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/media/peng/Samsung_T5/i3d-features/RGBTest/Before.Sunrise.1995__#00-04-20_00-05-35_label_A__3.npy'\n"
          ]
        }
      ],
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "      self.modality = 'MIX2'\n",
        "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "      self.flow_list = '/content/final_dl/list/flow.list'\n",
        "      self.audio_list = '/content/final_dl/list/audio.list'\n",
        "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "      self.gt = '/content/final_dl/list/gt.npy'\n",
        "      self.gpus = 1\n",
        "      self.lr = 0.0001\n",
        "      self.batch_size = 128\n",
        "      self.workers = 1\n",
        "      self.model_name = 'wsanodet'\n",
        "      self.pretrained_ckpt = None\n",
        "      self.feature_size = 1152  # 1024 + 128\n",
        "      self.num_classes = 1\n",
        "      self.dataset_name = 'XD-Violence'\n",
        "      self.max_seqlen = 200\n",
        "      self.max_epoch = 50\n",
        "\n",
        "  # Create an instance of the Args class\n",
        "args = Args()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "test_loader = DataLoader(Dataset(args, mode='test'),\n",
        "                          batch_size=5, shuffle=True,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (input,label) in enumerate(test_loader):\n",
        "    input = input.to(device)\n",
        "\n",
        "    print(input.shape)\n",
        "    ############\n",
        "    ### NOTE: ## setting seq_len to None pads training data in the sequence dim to 200\n",
        "    ############\n",
        "    logits, logits2 = model(inputs=input, seq_len=None)\n",
        "    # print(logits, logits2)\n",
        "    if i == 2:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM88zu_-ADPw"
      },
      "source": [
        "## Training HL NET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "pFJMkujfFROw",
        "outputId": "7396ffa2-1486-44dc-df23-b9d7c0a61d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1d1.weight\n",
            "conv1d1.bias\n",
            "conv1d2.weight\n",
            "conv1d2.bias\n",
            "conv1d3.weight\n",
            "conv1d3.bias\n",
            "conv1d4.weight\n",
            "conv1d4.bias\n",
            "gc1.weight\n",
            "gc1.residual.weight\n",
            "gc1.residual.bias\n",
            "gc2.weight\n",
            "gc3.weight\n",
            "gc3.residual.weight\n",
            "gc3.residual.bias\n",
            "gc4.weight\n",
            "gc5.weight\n",
            "gc5.residual.weight\n",
            "gc5.residual.bias\n",
            "gc6.weight\n",
            "simAdj.weight0\n",
            "simAdj.weight1\n",
            "disAdj.sigma\n",
            "classifier.weight\n",
            "classifier.bias\n",
            "approximator.0.weight\n",
            "approximator.0.bias\n",
            "approximator.2.weight\n",
            "approximator.2.bias\n",
            "conv1d_approximator.weight\n",
            "conv1d_approximator.bias\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-6-c430d01cb6ef>\", line 83, in __getitem__\n    features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 427, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/media/peng/Samsung_T5/i3d-features/RGBTest/Bad.Boys.1995__#01-11-55_01-12-40_label_G-B2-B6__0.npy'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6baa33c54768>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mis_topk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mpr_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr_auc_online\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Random initalization: offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr_auc_online\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5225f3c40b5e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(dataloader, model, device, gt)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-6-c430d01cb6ef>\", line 83, in __getitem__\n    features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 427, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/media/peng/Samsung_T5/i3d-features/RGBTest/Bad.Boys.1995__#01-11-55_01-12-40_label_G-B2-B6__0.npy'\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "# import option\n",
        "\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# torch.multiprocessing.set_start_method('spawn')\n",
        "# setup_seed(2333)\n",
        "# args = option.parser.parse_args()\n",
        "\n",
        "!export TORCH_USE_CUDA_DSA=ON\n",
        "device = torch.device(\"cuda\")\n",
        "train_loader = DataLoader(Dataset(args, mode='train'),\n",
        "                          batch_size=args.batch_size, shuffle=True,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "test_loader = DataLoader(Dataset(args, mode='test'),\n",
        "                          batch_size=5, shuffle=False,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "for name, value in model.named_parameters():\n",
        "    print(name)\n",
        "approximator_param = list(map(id, model.approximator.parameters()))\n",
        "approximator_param += list(map(id, model.conv1d_approximator.parameters()))\n",
        "base_param = filter(lambda p: id(p) not in approximator_param, model.parameters())\n",
        "\n",
        "if not os.path.exists('./ckpt'):\n",
        "    os.makedirs('./ckpt')\n",
        "optimizer = optim.Adam([{'params': base_param},\n",
        "                        {'params': model.approximator.parameters(), 'lr': args.lr / 2},\n",
        "                        {'params': model.conv1d_approximator.parameters(), 'lr': args.lr / 2},\n",
        "                        ],\n",
        "                        lr=args.lr, weight_decay=0.000)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "is_topk = True\n",
        "gt = np.load(args.gt)\n",
        "pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "print('Random initalization: offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n",
        "for epoch in range(args.max_epoch):\n",
        "    scheduler.step()\n",
        "    st = time.time()\n",
        "    train(train_loader, model, optimizer, criterion, device, is_topk)\n",
        "    if epoch % 2 == 0 and not epoch == 0:\n",
        "        torch.save(model.state_dict(), './ckpt/'+args.model_name+'{}.pkl'.format(epoch))\n",
        "\n",
        "    pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "    print('Epoch {0}/{1}: offline pr_auc:{2:.4}; online pr_auc:{3:.4}\\n'.format(epoch, args.max_epoch, pr_auc, pr_auc_online))\n",
        "torch.save(model.state_dict(), './ckpt/' + args.model_name + '.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYjtaIacOHyS"
      },
      "source": [
        "# Training VAE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRzWrGepsWur"
      },
      "source": [
        "## VAE MODEL\n",
        "\n",
        "added batch normalization (gus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fgnahlhKAtRg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Sampling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, z_means, z_log_vars):\n",
        "        epsilon = torch.randn_like(z_means, dtype=torch.float32)\n",
        "        return z_means + torch.exp(0.5 * z_log_vars) * epsilon\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1152, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Reduced number of feature maps in encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, 256, kernel_size=3, stride=2, padding=1),  # Reduced from 576\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(256, 128, kernel_size=3, stride=2, padding=1),  # Reduced from 288\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(128, 64, kernel_size=3, stride=2, padding=1),  # Reduced from 144\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        flattened_dim = 64 * 25  # Updated based on reduced features\n",
        "\n",
        "        self.lin_mean = nn.Sequential(\n",
        "            nn.Linear(flattened_dim, latent_dim),\n",
        "            nn.BatchNorm1d(latent_dim)\n",
        "        )\n",
        "\n",
        "        self.lin_log_var = nn.Sequential(\n",
        "            nn.Linear(flattened_dim, latent_dim),\n",
        "            nn.BatchNorm1d(latent_dim)\n",
        "        )\n",
        "\n",
        "        self.sampling = Sampling()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.encoder(x)\n",
        "        z_means = self.lin_mean(x)\n",
        "        z_log_vars = self.lin_log_var(x)\n",
        "        z = self.sampling(z_means, z_log_vars)\n",
        "        return z, z_means, z_log_vars\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1152, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        flattened_dim = 64 * 25  # Updated based on reduced features\n",
        "\n",
        "        self.decoder_fc = nn.Sequential(\n",
        "            nn.Linear(latent_dim, flattened_dim),\n",
        "            nn.BatchNorm1d(flattened_dim),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Reduced number of feature maps in decoder\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose1d(64, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # Reduced from 144->288\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(128, 256, kernel_size=3, stride=2, padding=1, output_padding=1),  # Reduced from 288->576\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(256, input_dim, kernel_size=3, stride=2, padding=1, output_padding=1),  # Reduced from 576->input_dim\n",
        "            nn.BatchNorm1d(input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.decoder_fc(x)\n",
        "        x = x.view(-1, 64, 25)  # Updated based on reduced features\n",
        "        x = self.decoder_conv(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1152, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_dim, input_dim, seq_len)\n",
        "        self.decoder = Decoder(latent_dim, input_dim, seq_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, z_means, z_log_vars = self.encoder(x)\n",
        "        x_reconstructed = self.decoder(z)\n",
        "        return x_reconstructed, z_means, z_log_vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AMJFLytLAtRh"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "class NormalDataset(data.Dataset):\n",
        "    def __init__(self, args, transform=None, mode='train'):\n",
        "        self.modality = args.modality\n",
        "        self.normal_flag = '_label_A'\n",
        "        self.max_seqlen = args.max_seqlen\n",
        "        self.transform = transform\n",
        "        self.test_mode = (mode == 'test')\n",
        "\n",
        "        # Set appropriate file lists based on mode\n",
        "        if mode == 'test':\n",
        "            self.rgb_list_file = args.test_rgb_list\n",
        "            self.flow_list_file = args.test_flow_list\n",
        "            self.audio_list_file = args.test_audio_list\n",
        "        elif mode == 'val':\n",
        "            self.rgb_list_file = args.val_rgb_list\n",
        "            self.flow_list_file = args.val_flow_list\n",
        "            self.audio_list_file = args.val_audio_list\n",
        "        else:  # train\n",
        "            self.rgb_list_file = args.train_rgb_list\n",
        "            self.flow_list_file = args.train_flow_list\n",
        "            self.audio_list_file = args.train_audio_list\n",
        "\n",
        "        self._parse_list()\n",
        "\n",
        "    def _parse_list(self):\n",
        "        \"\"\"Parse file lists and filter for normal samples only\"\"\"\n",
        "        def filter_normal_samples(file_list):\n",
        "            return [f for f in file_list if self.normal_flag in f]\n",
        "\n",
        "        if self.modality == 'AUDIO':\n",
        "            self.list = filter_normal_samples(list(open(self.audio_list_file)))\n",
        "        elif self.modality == 'RGB':\n",
        "            self.list = filter_normal_samples(list(open(self.rgb_list_file)))\n",
        "        elif self.modality == 'FLOW':\n",
        "            self.list = filter_normal_samples(list(open(self.flow_list_file)))\n",
        "        elif self.modality == 'MIX2':\n",
        "            # For MIX2, we need to handle the 5:1 ratio between RGB and audio\n",
        "            self.list = filter_normal_samples(list(open(self.rgb_list_file)))\n",
        "            # Filter audio list and ensure alignment\n",
        "            all_audio = list(open(self.audio_list_file))\n",
        "            self.audio_list = [f for f in all_audio if self.normal_flag in f]\n",
        "\n",
        "            # Ensure RGB and audio lists are aligned (5:1 ratio)\n",
        "            rgb_video_ids = set([self._get_video_id(f) for f in self.list])\n",
        "            audio_video_ids = set([self._get_video_id(f) for f in self.audio_list])\n",
        "            common_ids = rgb_video_ids & audio_video_ids\n",
        "\n",
        "            # Filter lists to only include common videos\n",
        "            self.list = [f for f in self.list if self._get_video_id(f) in common_ids]\n",
        "            self.audio_list = [f for f in self.audio_list if self._get_video_id(f) in common_ids]\n",
        "\n",
        "    def _get_video_id(self, filepath):\n",
        "        \"\"\"Extract video ID from filepath\"\"\"\n",
        "        filename = os.path.basename(filepath.strip('\\n'))\n",
        "        return filename.split('_label')[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.modality in ['RGB', 'FLOW', 'AUDIO']:\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'MIX2':\n",
        "            # Load RGB features\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            # Load corresponding audio features (accounting for 5:1 ratio)\n",
        "            audio_index = index // 5\n",
        "            features2 = np.array(np.load(self.audio_list[audio_index].strip('\\n')), dtype=np.float32)\n",
        "\n",
        "            # Handle potential dimension mismatch\n",
        "            if features1.shape[0] > features2.shape[0]:\n",
        "                features1 = features1[:features2.shape[0]]\n",
        "            features = np.concatenate((features1, features2), axis=1)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            features = self.transform(features)\n",
        "\n",
        "        features = process_feat(features, self.max_seqlen, is_random=not self.test_mode)\n",
        "\n",
        "        # Always return label 0 since these are normal samples\n",
        "        return features, 0.0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list)\n",
        "\n",
        "def create_normal_data_loaders(args):\n",
        "    \"\"\"Create data loaders for normal samples only\"\"\"\n",
        "    print(\"Creating normal-only data loaders...\")\n",
        "\n",
        "    # Create train loader\n",
        "    train_dataset = NormalDataset(args, mode='train')\n",
        "    train_loader = data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    print(f\"Normal train loader created with {len(train_dataset)} samples\")\n",
        "\n",
        "    # Create validation loader\n",
        "    val_dataset = NormalDataset(args, mode='val')\n",
        "    val_loader = data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    print(f\"Normal validation loader created with {len(val_dataset)} samples\")\n",
        "\n",
        "    # Create test loader\n",
        "    test_dataset = NormalDataset(args, mode='test')\n",
        "    test_loader = data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    print(f\"Normal test loader created with {len(test_dataset)} samples\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def process_feat(feat, length, is_random=True):\n",
        "    \"\"\"Process features to have consistent length\"\"\"\n",
        "    if len(feat) > length:\n",
        "        if is_random:\n",
        "            r = np.random.randint(len(feat) - length)\n",
        "            return feat[r:r + length]\n",
        "        else:\n",
        "            r = np.linspace(0, len(feat) - 1, length, dtype=np.uint16)\n",
        "            return feat[r, :]\n",
        "    else:\n",
        "        return np.pad(feat, ((0, length - len(feat)), (0, 0)), mode='constant', constant_values=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUfjc_eorOqB"
      },
      "source": [
        "## VAE Training func (Gus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6VB8JZp-AtRh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
        "    def __init__(self, patience=7, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        return self.early_stop\n",
        "\n",
        "def validate_vae(vae, val_loader, device):\n",
        "    \"\"\"Run validation loop and return average loss\"\"\"\n",
        "    vae.eval()\n",
        "    total_loss = 0\n",
        "    total_recon_loss = 0\n",
        "    total_kl_loss = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in val_loader:\n",
        "            # Only process normal samples (label == 0)\n",
        "            normal_mask = (labels == 0.0)\n",
        "            if not normal_mask.any():\n",
        "                continue\n",
        "\n",
        "            data = data[normal_mask].to(device)\n",
        "            recon_data, mu, logvar = vae(data)\n",
        "\n",
        "            # Reconstruction loss\n",
        "            recon_criterion = torch.nn.MSELoss(reduction='sum')\n",
        "            recon_loss = recon_criterion(recon_data, data)\n",
        "\n",
        "            # KL divergence loss\n",
        "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "            # Total loss\n",
        "            loss = recon_loss + kl_loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_recon_loss += recon_loss.item()\n",
        "            total_kl_loss += kl_loss.item()\n",
        "            n_samples += data.size(0)\n",
        "\n",
        "    # Calculate averages\n",
        "    if n_samples > 0:\n",
        "        avg_loss = total_loss / n_samples\n",
        "        avg_recon = total_recon_loss / n_samples\n",
        "        avg_kl = total_kl_loss / n_samples\n",
        "    else:\n",
        "        avg_loss = float('inf')\n",
        "        avg_recon = float('inf')\n",
        "        avg_kl = float('inf')\n",
        "\n",
        "    vae.train()\n",
        "    return avg_loss, avg_recon, avg_kl\n",
        "\n",
        "def train_vae(vae, train_loader, val_loader, args, save_dir='vae_checkpoints'):\n",
        "    \"\"\"Main training loop for VAE\"\"\"\n",
        "\n",
        "    # Create directory for saving checkpoints\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    vae = vae.to(device)\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=args.lr)\n",
        "    early_stopping = EarlyStopping(patience=5)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(args.max_epoch):\n",
        "        # Training\n",
        "        vae.train()\n",
        "        train_loss = 0\n",
        "        train_recon = 0\n",
        "        train_kl = 0\n",
        "        n_samples = 0\n",
        "\n",
        "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "            # Only process normal samples (label == 0)\n",
        "            normal_mask = (labels == 0.0)\n",
        "            if not normal_mask.any():\n",
        "                continue\n",
        "\n",
        "            data = data[normal_mask].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            recon_data, mu, logvar = vae(data)\n",
        "\n",
        "            # Losses\n",
        "            recon_criterion = torch.nn.MSELoss(reduction='sum')\n",
        "            recon_loss = recon_criterion(recon_data, data)\n",
        "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "            loss = recon_loss + kl_loss\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Record losses\n",
        "            train_loss += loss.item()\n",
        "            train_recon += recon_loss.item()\n",
        "            train_kl += kl_loss.item()\n",
        "            n_samples += data.size(0)\n",
        "\n",
        "        # Calculate average training losses\n",
        "        if n_samples > 0:\n",
        "            avg_train_loss = train_loss / n_samples\n",
        "            avg_train_recon = train_recon / n_samples\n",
        "            avg_train_kl = train_kl / n_samples\n",
        "        else:\n",
        "            print(\"Warning: No normal samples in training batch\")\n",
        "            continue\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_recon, val_kl = validate_vae(vae, val_loader, device)\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{args.max_epoch}:')\n",
        "        print(f'Training - Loss: {avg_train_loss:.4f}, Recon: {avg_train_recon:.4f}, KL: {avg_train_kl:.4f}')\n",
        "        print(f'Validation - Loss: {val_loss:.4f}, Recon: {val_recon:.4f}, KL: {val_kl:.4f}\\n')\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            save_path = os.path.join(save_dir, f'vae_best_{timestamp}.pt')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': vae.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_train_loss,\n",
        "                'val_loss': val_loss,\n",
        "            }, save_path)\n",
        "            print(f'Saved best model to {save_path}')\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopping(val_loss):\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    return vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "collapsed": true,
        "id": "e2kT1YQRAtRh",
        "outputId": "9a094ec3-0d2e-4b74-8eec-5e6c0886c3f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating normal-only data loaders...\n",
            "Normal train loader created with 8170 samples\n",
            "Normal validation loader created with 2075 samples\n",
            "Normal test loader created with 1500 samples\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-51ddca9b4085>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Train the VAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrained_vae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_vae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-6bea72c700fd>\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(vae, train_loader, val_loader, args, save_dir)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# Losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mrecon_criterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mrecon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecon_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecon_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     return torch._C._nn.mse_loss(\n\u001b[0m\u001b[1;32m   3793\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# adjust args needed for VAE\n",
        "args_vae = Args()\n",
        "args_vae.feature_size = 1152  # 1024 (RGB) + 128 (audio)\n",
        "args_vae.batch_size = 2\n",
        "args_vae.modality = 'MIX2'\n",
        "\n",
        "# initialize VAE with correct input dimension\n",
        "vae = VAE(latent_dim=64, input_dim=args_vae.feature_size, seq_len=200)\n",
        "\n",
        "# Create normal-only dataloaders\n",
        "normal_train_loader, normal_val_loader, normal_test_loader = create_normal_data_loaders(args_vae)\n",
        "\n",
        "# Train the VAE\n",
        "trained_vae = train_vae(vae, normal_train_loader, normal_val_loader, args_vae)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(trained_vae.state_dict(), \"/content/trained_vae.pkl\")"
      ],
      "metadata": {
        "id": "up2bKDH-lgnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fprrx9jYAtRh"
      },
      "source": [
        "## Debugging data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEvt7oBpAtRh",
        "outputId": "1e1325a4-d960-4a5d-921b-f7f1d0741775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting dataset with MIX2 modality...\n",
            "Creating NormalDataset...\n",
            "Total number of samples in dataset: 8170\n",
            "\n",
            "Number of files in main list: 8170\n",
            "Number of files in audio list: 1634\n",
            "\n",
            "Inspecting individual samples:\n",
            "\n",
            "Sample 0:\n",
            "RGB file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__0.npy\n",
            "RGB shape: (540, 1024)\n",
            "Audio file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__vggish.npy\n",
            "Audio shape: (540, 128)\n",
            "Concatenated shape: (540, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 1:\n",
            "RGB file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__1.npy\n",
            "RGB shape: (540, 1024)\n",
            "Audio file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__vggish.npy\n",
            "Audio shape: (540, 128)\n",
            "Concatenated shape: (540, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 2:\n",
            "RGB file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__2.npy\n",
            "RGB shape: (540, 1024)\n",
            "Audio file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__vggish.npy\n",
            "Audio shape: (540, 128)\n",
            "Concatenated shape: (540, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 3:\n",
            "RGB file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__3.npy\n",
            "RGB shape: (540, 1024)\n",
            "Audio file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__vggish.npy\n",
            "Audio shape: (540, 128)\n",
            "Concatenated shape: (540, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 4:\n",
            "RGB file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__4.npy\n",
            "RGB shape: (540, 1024)\n",
            "Audio file: v=fkps18H3SXY__#00-24-00_00-30-00_label_A__vggish.npy\n",
            "Audio shape: (540, 128)\n",
            "Concatenated shape: (540, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 5:\n",
            "RGB file: v=XAZIC6RlSXs__#1_label_A__0.npy\n",
            "RGB shape: (981, 1024)\n",
            "Audio file: v=XAZIC6RlSXs__#1_label_A__vggish.npy\n",
            "Audio shape: (981, 128)\n",
            "Concatenated shape: (981, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 6:\n",
            "RGB file: v=XAZIC6RlSXs__#1_label_A__1.npy\n",
            "RGB shape: (981, 1024)\n",
            "Audio file: v=XAZIC6RlSXs__#1_label_A__vggish.npy\n",
            "Audio shape: (981, 128)\n",
            "Concatenated shape: (981, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 7:\n",
            "RGB file: v=XAZIC6RlSXs__#1_label_A__2.npy\n",
            "RGB shape: (981, 1024)\n",
            "Audio file: v=XAZIC6RlSXs__#1_label_A__vggish.npy\n",
            "Audio shape: (981, 128)\n",
            "Concatenated shape: (981, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 8:\n",
            "RGB file: v=XAZIC6RlSXs__#1_label_A__3.npy\n",
            "RGB shape: (981, 1024)\n",
            "Audio file: v=XAZIC6RlSXs__#1_label_A__vggish.npy\n",
            "Audio shape: (981, 128)\n",
            "Concatenated shape: (981, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 9:\n",
            "RGB file: v=XAZIC6RlSXs__#1_label_A__4.npy\n",
            "RGB shape: (981, 1024)\n",
            "Audio file: v=XAZIC6RlSXs__#1_label_A__vggish.npy\n",
            "Audio shape: (981, 128)\n",
            "Concatenated shape: (981, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 10:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__0.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "Concatenated shape: (103, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 11:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__1.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "Concatenated shape: (103, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 12:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__2.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "Concatenated shape: (103, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 13:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__3.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "Concatenated shape: (103, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 14:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__4.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "Concatenated shape: (103, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 15:\n",
            "RGB file: v=AKS4ZC3ZdE8__#1_label_A__0.npy\n",
            "RGB shape: (24, 1024)\n",
            "Audio file: v=AKS4ZC3ZdE8__#1_label_A__vggish.npy\n",
            "Audio shape: (24, 128)\n",
            "Concatenated shape: (24, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 16:\n",
            "RGB file: v=AKS4ZC3ZdE8__#1_label_A__1.npy\n",
            "RGB shape: (24, 1024)\n",
            "Audio file: v=AKS4ZC3ZdE8__#1_label_A__vggish.npy\n",
            "Audio shape: (24, 128)\n",
            "Concatenated shape: (24, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 17:\n",
            "RGB file: v=AKS4ZC3ZdE8__#1_label_A__2.npy\n",
            "RGB shape: (24, 1024)\n",
            "Audio file: v=AKS4ZC3ZdE8__#1_label_A__vggish.npy\n",
            "Audio shape: (24, 128)\n",
            "Concatenated shape: (24, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 18:\n",
            "RGB file: v=AKS4ZC3ZdE8__#1_label_A__3.npy\n",
            "RGB shape: (24, 1024)\n",
            "Audio file: v=AKS4ZC3ZdE8__#1_label_A__vggish.npy\n",
            "Audio shape: (24, 128)\n",
            "Concatenated shape: (24, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 19:\n",
            "RGB file: v=AKS4ZC3ZdE8__#1_label_A__4.npy\n",
            "RGB shape: (24, 1024)\n",
            "Audio file: v=AKS4ZC3ZdE8__#1_label_A__vggish.npy\n",
            "Audio shape: (24, 128)\n",
            "Concatenated shape: (24, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 20:\n",
            "RGB file: v=vFPQ_NiDBIU__#00-54-00_01-00-00_label_A__0.npy\n",
            "RGB shape: (539, 1024)\n",
            "Audio file: v=vFPQ_NiDBIU__#00-54-00_01-00-00_label_A__vggish.npy\n",
            "Audio shape: (539, 128)\n",
            "Concatenated shape: (539, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 21:\n",
            "RGB file: v=vFPQ_NiDBIU__#00-54-00_01-00-00_label_A__1.npy\n",
            "RGB shape: (539, 1024)\n",
            "Audio file: v=vFPQ_NiDBIU__#00-54-00_01-00-00_label_A__vggish.npy\n",
            "Audio shape: (539, 128)\n",
            "Concatenated shape: (539, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 22:\n",
            "RGB file: v=vFPQ_NiDBIU__#00-54-00_01-00-00_label_A__2.npy\n",
            "RGB shape: (539, 1024)\n",
            "Audio file: v=vFPQ_NiDBIU__#00-54-00_01-00-00_label_A__vggish.npy\n",
            "Audio shape: (539, 128)\n",
            "Concatenated shape: (539, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 23:\n",
            "RGB file: v=vFPQ_NiDBIU__#00-54-00_01-00-00_label_A__3.npy\n",
            "RGB shape: (539, 1024)\n",
            "Audio file: v=vFPQ_NiDBIU__#00-54-00_01-00-00_label_A__vggish.npy\n",
            "Audio shape: (539, 128)\n",
            "Concatenated shape: (539, 1152)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 24:\n",
            "RGB file: v=vFPQ_NiDBIU__#00-54-00_01-00-00_label_A__4.npy\n",
            "RGB shape: (539, 1024)\n",
            "Audio file: v=vFPQ_NiDBIU__#00-54-00_01-00-00_label_A__vggish.npy\n",
            "Audio shape: (539, 128)\n",
            "Concatenated shape: (539, 1152)\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def inspect_normal_dataset(args, num_samples=25):\n",
        "    \"\"\"\n",
        "    Inspect the normal samples being loaded from the dataset\n",
        "    \"\"\"\n",
        "    print(\"Creating NormalDataset...\")\n",
        "    dataset = NormalDataset(args, mode='train')\n",
        "    print(f\"Total number of samples in dataset: {len(dataset)}\")\n",
        "    print(f\"\\nNumber of files in main list: {len(dataset.list)}\")\n",
        "    if hasattr(dataset, 'audio_list'):\n",
        "        print(f\"Number of files in audio list: {len(dataset.audio_list)}\")\n",
        "\n",
        "    print(\"\\nInspecting individual samples:\")\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        try:\n",
        "            # Get the filepaths that would be loaded\n",
        "            rgb_path = dataset.list[i].strip('\\n')\n",
        "            print(f\"\\nSample {i}:\")\n",
        "            print(f\"RGB file: {os.path.basename(rgb_path)}\")\n",
        "\n",
        "            # Load RGB features\n",
        "            features1 = np.array(np.load(rgb_path), dtype=np.float32)\n",
        "            print(f\"RGB shape: {features1.shape}\")\n",
        "\n",
        "            # If MIX2 modality, also show audio information\n",
        "            if dataset.modality == 'MIX2':\n",
        "                audio_index = i // 5\n",
        "                if audio_index < len(dataset.audio_list):\n",
        "                    audio_path = dataset.audio_list[audio_index].strip('\\n')\n",
        "                    print(f\"Audio file: {os.path.basename(audio_path)}\")\n",
        "                    features2 = np.array(np.load(audio_path), dtype=np.float32)\n",
        "                    print(f\"Audio shape: {features2.shape}\")\n",
        "\n",
        "                    # Try the concatenation\n",
        "                    try:\n",
        "                        if features1.shape[0] != features2.shape[0]:\n",
        "                            print(\"⚠️ Dimension mismatch!\")\n",
        "                            print(f\"RGB frames: {features1.shape[0]}, Audio frames: {features2.shape[0]}\")\n",
        "                            if features1.shape[0] - 1 == features2.shape[0]:\n",
        "                                print(\"Would work with [:-1] slice\")\n",
        "                                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "                                print(f\"Concatenated shape after adjustment: {features.shape}\")\n",
        "                            else:\n",
        "                                print(\"Cannot be fixed with simple slice\")\n",
        "                        else:\n",
        "                            features = np.concatenate((features1, features2), axis=1)\n",
        "                            print(f\"Concatenated shape: {features.shape}\")\n",
        "                    except ValueError as e:\n",
        "                        print(f\"❌ Concatenation failed: {str(e)}\")\n",
        "                else:\n",
        "                    print(\"⚠️ No corresponding audio file (index out of range)\")\n",
        "\n",
        "            # Check if this is actually a normal sample\n",
        "            if '_label_A' not in rgb_path:\n",
        "                print(\"⚠️ WARNING: This doesn't appear to be a normal sample!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading sample {i}: {str(e)}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Run the inspection\n",
        "print(\"Inspecting dataset with MIX2 modality...\")\n",
        "args_check = args_vae\n",
        "args_check.modality = 'MIX2'\n",
        "args_check.batch_size = 64  # Smaller batch size for inspection\n",
        "inspect_normal_dataset(args_check)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HLNET (AUDIO)"
      ],
      "metadata": {
        "id": "LsGHfTdkXUNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "impo"
      ],
      "metadata": {
        "id": "R4MzH1X20jTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "q-91smkdXmbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_audio(batch):\n",
        "    \"\"\"\n",
        "    Collate function for a batch of spectrograms.\n",
        "    Each spectrogram is of shape [time_steps, num_mels].\n",
        "    This function pads each spectrogram along the time dimension\n",
        "    so that all have the same time_steps dimension.\n",
        "    \"\"\"\n",
        "    # `batch` is a list of spectrograms: each is numpy array [T, num_mels]\n",
        "    # Find the longest time dimension in the batch\n",
        "    max_length = max(feat.shape[0] for feat in batch)\n",
        "\n",
        "    # Pad each spectrogram to max_length\n",
        "    padded_feats = []\n",
        "    for feat in batch:\n",
        "        t, m = feat.shape\n",
        "        # Create a zero array [max_length, num_mels]\n",
        "        padded = np.zeros((max_length, m), dtype=np.float32)\n",
        "        padded[:t] = feat\n",
        "        padded_feats.append(padded)\n",
        "\n",
        "    # Convert the list of numpy arrays into a single tensor\n",
        "    feats_batch = torch.tensor(padded_feats, dtype=torch.float32)\n",
        "\n",
        "    return feats_batch\n"
      ],
      "metadata": {
        "id": "myWyZhgOXYFZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arg Definition"
      ],
      "metadata": {
        "id": "UDwH_zxpZ4PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.modality = 'AUDIO'\n",
        "        # Original paths\n",
        "        self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "        self.flow_list = '/content/final_dl/list/flow.list'\n",
        "        self.audio_list = '/content/final_dl/list/audio.list'\n",
        "\n",
        "        # Train paths\n",
        "        self.train_rgb_list = '/content/final_dl/list/rgb_train.list'\n",
        "        self.train_flow_list = '/content/final_dl/list/flow_train.list'\n",
        "        self.train_audio_list = '/content/final_dl/list/audio_train.list'\n",
        "\n",
        "        # Val paths\n",
        "        self.val_rgb_list = '/content/final_dl/list/rgb_val.list'\n",
        "        self.val_flow_list = '/content/final_dl/list/flow_val.list'\n",
        "        self.val_audio_list = '/content/final_dl/list/audio_val.list'\n",
        "\n",
        "        # Test paths\n",
        "        self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "        self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "        self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "\n",
        "        self.gt = '/content/final_dl/list/gt.npy'\n",
        "        self.gpus = 1\n",
        "        self.lr = 0.0001\n",
        "        self.batch_size = 128\n",
        "        self.workers = 1  # Reduced from 4 to avoid memory issues\n",
        "        self.model_name = 'wsanodet'\n",
        "        self.pretrained_ckpt = None\n",
        "        #self.feature_size = 1152  # 1024 + 128\n",
        "        self.feature_size = 128  # 1024 + 128\n",
        "        self.num_classes = 1\n",
        "        self.dataset_name = 'XD-Violence'\n",
        "        self.max_seqlen = 200\n",
        "        self.max_epoch = 50\n",
        "\n",
        "args = Args()\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, args, transform=None, mode='train'):\n",
        "        self.modality = args.modality\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args: Arguments containing dataset paths and configuration\n",
        "            transform: Optional transforms to apply\n",
        "            mode: One of ['train', 'val', 'test'] to specify the dataset split\n",
        "        \"\"\"\n",
        "\n",
        "        if mode == 'test':\n",
        "            self.rgb_list_file = args.test_rgb_list\n",
        "            self.flow_list_file = args.test_flow_list\n",
        "            self.audio_list_file = args.test_audio_list\n",
        "        elif mode == 'val':\n",
        "            self.rgb_list_file = args.val_rgb_list\n",
        "            self.flow_list_file = args.val_flow_list\n",
        "            self.audio_list_file = args.val_audio_list\n",
        "        else: # train\n",
        "            self.rgb_list_file = args.train_rgb_list\n",
        "            self.flow_list_file = args.train_flow_list\n",
        "            self.audio_list_file = args.train_audio_list\n",
        "\n",
        "        self.max_seqlen = args.max_seqlen\n",
        "        self.tranform = transform\n",
        "        self.test_mode = (mode == 'test')\n",
        "        self.normal_flag = '_label_A'\n",
        "        self._parse_list()\n",
        "\n",
        "    def _parse_list(self):\n",
        "        if self.modality == 'AUDIO':\n",
        "            self.list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'RGB':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            print(\"here\")\n",
        "            # print(self.list)\n",
        "        elif self.modality == 'FLOW':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX2':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX3':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        else:\n",
        "            assert 1 > 2, 'Modality is wrong!'\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.normal_flag in self.list[index]:\n",
        "            label = 0.0\n",
        "        else:\n",
        "            label = 1.0\n",
        "\n",
        "        if self.modality == 'AUDIO':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'RGB':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')),dtype=np.float32)\n",
        "        elif self.modality == 'FLOW':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'MIX':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX2':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'MIX3':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            features3 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2, features3),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2, features3[:-1]), axis=1)\n",
        "        else:\n",
        "            print(\"WHAT IS WRONG\")\n",
        "            assert 1>2, 'Modality is wrong!'\n",
        "        if self.tranform is not None:\n",
        "            features = self.tranform(features)\n",
        "        if self.test_mode:\n",
        "            return features, label\n",
        "\n",
        "        else:\n",
        "            features = process_feat(features, self.max_seqlen, is_random=False)\n",
        "            return features, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list)"
      ],
      "metadata": {
        "id": "duE65FG_Z59M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data splits"
      ],
      "metadata": {
        "id": "8aFDahEDzkbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Define the input .list file containing the original file paths\n",
        "    input_list_file = \"/content/final_dl/list/audio.list\"\n",
        "\n",
        "    # Define the directory to update the paths to\n",
        "    new_directory = \"/content/final_dl/list/xx/train\"\n",
        "\n",
        "    # Define the output .list file for the updated file paths\n",
        "    output_list_file = \"/content/final_dl/list/audio.list\"\n",
        "\n",
        "    # Read the original file paths from the .list file\n",
        "    with open(input_list_file, \"r\") as file:\n",
        "        original_paths = file.readlines()\n",
        "\n",
        "    # Process and update each file path\n",
        "    updated_paths = []\n",
        "    for path in original_paths:\n",
        "        path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "        if path:  # Ensure the path is not empty\n",
        "            # Extract the filename from the original path and create a new path\n",
        "            filename = path.split(\"/\")[-1]\n",
        "            updated_path = f\"{new_directory}/{filename}\"\n",
        "            updated_paths.append(updated_path)\n",
        "\n",
        "    # Write the updated paths to the output .list file\n",
        "    with open(output_list_file, \"w\") as file:\n",
        "        file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "    print(f\"Updated paths have been written to {output_list_file}\")\n",
        "\n",
        "    # Define the input .list file containing the original file paths\n",
        "    input_list_file = \"/content/final_dl/list/rgb.list\"\n",
        "\n",
        "    # Define the directory to update the paths to\n",
        "    new_directory = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
        "\n",
        "    # Define the output .list file for the updated file paths\n",
        "    output_list_file = \"/content/final_dl/list/rgb.list\"\n",
        "\n",
        "    # Read the original file paths from the .list file\n",
        "    with open(input_list_file, \"r\") as file:\n",
        "        original_paths = file.readlines()\n",
        "\n",
        "    # Process and update each file path\n",
        "    updated_paths = []\n",
        "    for path in original_paths:\n",
        "        path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "        if path:  # Ensure the path is not empty\n",
        "            # Extract the filename from the original path and create a new path\n",
        "            filename = path.split(\"/\")[-1]\n",
        "            updated_path = f\"{new_directory}/{filename}\"\n",
        "            updated_paths.append(updated_path)\n",
        "\n",
        "    # Write the updated paths to the output .list file\n",
        "    with open(output_list_file, \"w\") as file:\n",
        "        file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "    print(f\"Updated paths have been written to {output_list_file}\")\n",
        "\n",
        "\n",
        "def test():\n",
        "    # Define the input .list file containing the original file paths\n",
        "    input_list_file = \"/content/final_dl/list/audio_test.list\"\n",
        "\n",
        "    # Define the directory to update the paths to\n",
        "    new_directory = \"/content/final_dl/list/xx/test\"\n",
        "\n",
        "    # Define the output .list file for the updated file paths\n",
        "    output_list_file = \"/content/final_dl/list/audio_test.list\"\n",
        "\n",
        "    # Read the original file paths from the .list file\n",
        "    with open(input_list_file, \"r\") as file:\n",
        "        original_paths = file.readlines()\n",
        "\n",
        "    # Process and update each file path\n",
        "    updated_paths = []\n",
        "    for path in original_paths:\n",
        "        path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "        if path:  # Ensure the path is not empty\n",
        "            # Extract the filename from the original path and create a new path\n",
        "            filename = path.split(\"/\")[-1]\n",
        "            updated_path = f\"{new_directory}/{filename}\"\n",
        "            updated_paths.append(updated_path)\n",
        "\n",
        "    # Write the updated paths to the output .list file\n",
        "    with open(output_list_file, \"w\") as file:\n",
        "        file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "    print(f\"Updated paths have been written to {output_list_file}\")\n",
        "\n",
        "    # Define the input .list file containing the original file paths\n",
        "    input_list_file = \"/content/final_dl/list/rgb_test.list\"\n",
        "\n",
        "    # Define the directory to update the paths to\n",
        "    new_directory = \"/content/final_dl/dl_files/i3d-features/RGBTest\"\n",
        "\n",
        "    # Define the output .list file for the updated file paths\n",
        "    output_list_file = \"/content/final_dl/list/rgb_test.list\"\n",
        "\n",
        "    # Read the original file paths from the .list file\n",
        "    with open(input_list_file, \"r\") as file:\n",
        "        original_paths = file.readlines()\n",
        "\n",
        "    # Process and update each file path\n",
        "    updated_paths = []\n",
        "    for path in original_paths:\n",
        "        path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "        if path:  # Ensure the path is not empty\n",
        "            # Extract the filename from the original path and create a new path\n",
        "            filename = path.split(\"/\")[-1]\n",
        "            updated_path = f\"{new_directory}/{filename}\"\n",
        "            updated_paths.append(updated_path)\n",
        "\n",
        "    # Write the updated paths to the output .list file\n",
        "    with open(output_list_file, \"w\") as file:\n",
        "        file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "    print(f\"Updated paths have been written to {output_list_file}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "def split_file(input_file, output_file_80, output_file_20):\n",
        "    # Read all lines from the input file\n",
        "    with open(input_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Shuffle the lines to randomize them\n",
        "    random.shuffle(lines)\n",
        "\n",
        "    # Calculate split index\n",
        "    split_index = int(len(lines) * 0.8)\n",
        "\n",
        "    # Split into 80% and 20%\n",
        "    lines_80 = lines[:split_index]\n",
        "    lines_20 = lines[split_index:]\n",
        "\n",
        "    # Write the 80% lines to the output file for 80%\n",
        "    with open(output_file_80, 'w') as file:\n",
        "        file.writelines(lines_80)\n",
        "\n",
        "    # Write the 20% lines to the output file for 20%\n",
        "    with open(output_file_20, 'w') as file:\n",
        "        file.writelines(lines_20)\n",
        "\n",
        "    print(f\"Split complete. {len(lines_80)} lines written to {output_file_80}.\")\n",
        "    print(f\"{len(lines_20)} lines written to {output_file_20}.\")\n",
        "\n",
        "train()\n",
        "test()\n",
        "split_file(\"/content/final_dl/list/audio.list\", \"/content/final_dl/list/audio_train.list\", \"/content/final_dl/list/audio_val.list\")\n",
        "split_file(\"/content/final_dl/list/rgb.list\", \"/content/final_dl/list/rgb_train.list\", \"/content/final_dl/list/rgb_val.list\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymT-HstE2egL",
        "outputId": "0a9b1051-a804-4732-c587-8070de25d79b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/audio.list\n",
            "Updated paths have been written to /content/final_dl/list/rgb.list\n",
            "Updated paths have been written to /content/final_dl/list/audio_test.list\n",
            "Updated paths have been written to /content/final_dl/list/rgb_test.list\n",
            "Split complete. 3163 lines written to /content/final_dl/list/audio_train.list.\n",
            "791 lines written to /content/final_dl/list/audio_val.list.\n",
            "Split complete. 15816 lines written to /content/final_dl/list/rgb_train.list.\n",
            "3954 lines written to /content/final_dl/list/rgb_val.list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "fivE5kQWJYcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "def validate(dataloader, model, criterion, device, is_topk):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        count = 0\n",
        "        for i, (input, label) in enumerate(dataloader):\n",
        "            seq_len = torch.sum(torch.max(torch.abs(input), dim=2)[0]>0, 1)\n",
        "            input = input[:, :torch.max(seq_len), :]\n",
        "            input, label = input.float().to(device), label.float().to(device)\n",
        "            logits, logits2 = model(input, seq_len)\n",
        "            clsloss = CLAS(logits, label, seq_len, criterion, device, is_topk)\n",
        "            clsloss2 = CLAS(logits2, label, seq_len, criterion, device, is_topk)\n",
        "            croloss = CENTROPY(logits, logits2, seq_len, device)\n",
        "\n",
        "            batch_loss = clsloss + clsloss2 + 5*croloss\n",
        "            total_loss += batch_loss.item()\n",
        "            count += 1\n",
        "        return total_loss / count if count > 0 else 0.0\n",
        "\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler, criterion, device, is_topk, val_loader=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    count = 0\n",
        "    for i, (input, label) in enumerate(dataloader):\n",
        "        seq_len = torch.sum(torch.max(torch.abs(input), dim=2)[0]>0, 1)\n",
        "        input = input[:, :torch.max(seq_len), :]\n",
        "        input, label = input.float().to(device), label.float().to(device)\n",
        "        logits, logits2 = model(input, seq_len)\n",
        "        clsloss = CLAS(logits, label, seq_len, criterion, device, is_topk)\n",
        "        clsloss2 = CLAS(logits2, label, seq_len, criterion, device, is_topk)\n",
        "        croloss = CENTROPY(logits, logits2, seq_len, device)\n",
        "\n",
        "        total_loss = clsloss + clsloss2 + 5*croloss\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += total_loss.item()\n",
        "        count += 1\n",
        "\n",
        "        # Print training loss every 100 steps\n",
        "        if (i + 1) % 100 == 0:\n",
        "            avg_train_loss = running_loss / count\n",
        "            print(f\"Step {i+1}: Average Training Loss: {avg_train_loss:.4f}\")\n",
        "            running_loss = 0.0\n",
        "            count = 0\n",
        "\n",
        "            # If val_loader is provided, evaluate on validation set\n",
        "            if val_loader is not None:\n",
        "                val_loss = validate(val_loader, model, criterion, device, is_topk)\n",
        "                print(f\"Step {i+1}: Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def test(dataloader, model, device):\n",
        "    gt =[]\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pred = torch.zeros(0).to(device)\n",
        "        pred2 = torch.zeros(0).to(device)\n",
        "        for i, (input, label) in enumerate(dataloader):\n",
        "            gt.append(label)\n",
        "            input = input.to(device)\n",
        "            logits, logits2 = model(inputs=input, seq_len=None)\n",
        "            logits = torch.squeeze(logits)\n",
        "            sig = torch.sigmoid(logits)\n",
        "            sig = torch.mean(sig, 0)\n",
        "            sig = sig.unsqueeze(0)\n",
        "            pred = torch.cat((pred, sig))\n",
        "            '''\n",
        "            online detection\n",
        "            '''\n",
        "            logits2 = torch.squeeze(logits2)\n",
        "            sig2 = torch.sigmoid(logits2)\n",
        "            sig2 = torch.mean(sig2, 0)\n",
        "            sig2 = sig2.unsqueeze(0)\n",
        "            pred2 = torch.cat((pred2, sig2))\n",
        "\n",
        "        pred = list(pred.cpu().detach().numpy())\n",
        "        pred2 = list(pred2.cpu().detach().numpy())\n",
        "\n",
        "        precision, recall, th = precision_recall_curve(list(gt), pred)\n",
        "        pr_auc = auc(recall, precision)\n",
        "        precision, recall, th = precision_recall_curve(list(gt), pred2)\n",
        "        pr_auc2 = auc(recall, precision)\n",
        "        return pr_auc, pr_auc2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    aligned_files = find_matching_files()\n",
        "\n",
        "    # Create train/val splits\n",
        "    split_data = create_splits(aligned_files)\n",
        "\n",
        "        # Write list files\n",
        "    write_list_files(split_data, aligned_files, \"/content/final_dl/list\")\n",
        "\n",
        "    train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality=\"AUDIO\")\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    # train_loader = DataLoader(Dataset(args, mode='train'),\n",
        "    #                         batch_size=args.batch_size, shuffle=True,\n",
        "    #                         num_workers=args.workers, pin_memory=True)\n",
        "    # test_loader = DataLoader(Dataset(args, mode='test'),\n",
        "    #                         batch_size=5, shuffle=False,\n",
        "    #                         num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = Model(args)\n",
        "    model = model.cuda()\n",
        "\n",
        "    for name, value in model.named_parameters():\n",
        "        print(name)\n",
        "    approximator_param = list(map(id, model.approximator.parameters()))\n",
        "    approximator_param += list(map(id, model.conv1d_approximator.parameters()))\n",
        "    base_param = filter(lambda p: id(p) not in approximator_param, model.parameters())\n",
        "\n",
        "    if not os.path.exists('./ckpt'):\n",
        "        os.makedirs('./ckpt')\n",
        "    optimizer = optim.Adam([{'params': base_param},\n",
        "                            {'params': model.approximator.parameters(), 'lr': args.lr / 2},\n",
        "                            {'params': model.conv1d_approximator.parameters(), 'lr': args.lr / 2},\n",
        "                            ],\n",
        "                            lr=args.lr, weight_decay=0.000)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "    is_topk = True\n",
        "    gt = np.load(args.gt)\n",
        "    # pr_auc, pr_auc_online = test(test_loader, model, device)\n",
        "    # print('Random initalization: offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n",
        "    for epoch in range(args.max_epoch):\n",
        "        st = time.time()\n",
        "        model = train(train_loader, model, optimizer, scheduler, criterion, device, is_topk)\n",
        "        if epoch % 2 == 0 and not epoch == 0:\n",
        "            torch.save(model.state_dict(), '/content/final_dl/'+args.model_name+'{}.pth'.format(epoch))\n",
        "\n",
        "        pr_auc, pr_auc_online = test(test_loader, model, device)\n",
        "        print('Epoch {0}/{1}: offline pr_auc:{2:.4}; online pr_auc:{3:.4}\\n'.format(epoch, args.max_epoch, pr_auc, pr_auc_online))\n",
        "    torch.save(model.state_dict(), '/content/ckpt/' + args.model_name + '.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "iCaDhirwzj0n",
        "outputId": "a8b38c5c-49b2-4f9b-9b96-d574a9c75a13"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3953 aligned RGB-Audio pairs\n",
            "Creating AUDIO data loaders...\n",
            "Train loader created with 3162 samples\n",
            "Validation loader created with 791 samples\n",
            "Test loader created with 800 samples\n",
            "conv1d1.weight\n",
            "conv1d1.bias\n",
            "conv1d2.weight\n",
            "conv1d2.bias\n",
            "conv1d3.weight\n",
            "conv1d3.bias\n",
            "conv1d4.weight\n",
            "conv1d4.bias\n",
            "gc1.weight\n",
            "gc1.residual.weight\n",
            "gc1.residual.bias\n",
            "gc2.weight\n",
            "gc3.weight\n",
            "gc3.residual.weight\n",
            "gc3.residual.bias\n",
            "gc4.weight\n",
            "gc5.weight\n",
            "gc5.residual.weight\n",
            "gc5.residual.bias\n",
            "gc6.weight\n",
            "simAdj.weight0\n",
            "simAdj.weight1\n",
            "disAdj.sigma\n",
            "classifier.weight\n",
            "classifier.bias\n",
            "approximator.0.weight\n",
            "approximator.0.bias\n",
            "approximator.2.weight\n",
            "approximator.2.bias\n",
            "conv1d_approximator.weight\n",
            "conv1d_approximator.bias\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-e34acc15cf82>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_topk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/final_dl/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'{}.pth'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-e34acc15cf82>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, optimizer, scheduler, criterion, device, is_topk, val_loader)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mclsloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLAS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_topk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mclsloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLAS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_topk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mcroloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCENTROPY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-92c6eaf947f4>\u001b[0m in \u001b[0;36mCLAS\u001b[0;34m(logits, label, seq_len, criterion, device, is_topk)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_topk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlargest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# See https://github.com/pytorch/pytorch/issues/75462\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4mQ0vGxrYtdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HLNET (VIDEO + AUDIO) + VAE\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D-CeVw5MOY1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n"
      ],
      "metadata": {
        "id": "M0dlomVBZqfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "\n",
        "# from utils import process_feat\n",
        "def pad(feat, min_len):\n",
        "    if np.shape(feat)[0] <= min_len:\n",
        "       return np.pad(feat, ((0, min_len-np.shape(feat)[0]), (0, 0)), mode='constant', constant_values=0)\n",
        "    else:\n",
        "       return feat\n",
        "\n",
        "def process_feat(feat, length, is_random=True):\n",
        "    if len(feat) > length:\n",
        "        if is_random:\n",
        "            return random_extract(feat, length)\n",
        "        else:\n",
        "            return uniform_extract(feat, length)\n",
        "    else:\n",
        "        return pad(feat, length)\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, args, transform=None, mode='train'):\n",
        "        self.modality = args.modality\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args: Arguments containing dataset paths and configuration\n",
        "            transform: Optional transforms to apply\n",
        "            mode: One of ['train', 'val', 'test'] to specify the dataset split\n",
        "        \"\"\"\n",
        "\n",
        "        if mode == 'test':\n",
        "            self.rgb_list_file = args.test_rgb_list\n",
        "            self.flow_list_file = args.test_flow_list\n",
        "            self.audio_list_file = args.test_audio_list\n",
        "        elif mode == 'val':\n",
        "            self.rgb_list_file = args.val_rgb_list\n",
        "            self.flow_list_file = args.val_flow_list\n",
        "            self.audio_list_file = args.val_audio_list\n",
        "        else: # train\n",
        "            self.rgb_list_file = args.train_rgb_list\n",
        "            self.flow_list_file = args.train_flow_list\n",
        "            self.audio_list_file = args.train_audio_list\n",
        "\n",
        "\n",
        "        self.max_seqlen = args.max_seqlen\n",
        "        self.tranform = transform\n",
        "        self.test_mode = (mode == 'test')\n",
        "        self.normal_flag = '_label_A'\n",
        "        self._parse_list()\n",
        "\n",
        "    def _parse_list(self):\n",
        "        if self.modality == 'AUDIO':\n",
        "            self.list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'RGB':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            print(\"here\")\n",
        "            # print(self.list)\n",
        "        elif self.modality == 'FLOW':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX2':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX3':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        else:\n",
        "            assert 1 > 2, 'Modality is wrong!'\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.normal_flag in self.list[index]:\n",
        "            label = 0.0\n",
        "        else:\n",
        "            label = 1.0\n",
        "\n",
        "        if self.modality == 'AUDIO':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'RGB':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'FLOW':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'MIX':\n",
        "             # Load RGB features\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            # Load corresponding audio features (accounting for 5:1 ratio)\n",
        "            audio_index = index // 5\n",
        "            features2 = np.array(np.load(self.audio_list[audio_index].strip('\\n')), dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "            # print(\"features1 shape: \", features1.shape)\n",
        "            # print(\"features2 shape: \", features1.shape)\n",
        "\n",
        "            # Handle potential dimension mismatch\n",
        "            if features1.shape[0] > features2.shape[0]:\n",
        "                features1 = features1[:features2.shape[0]]\n",
        "            else:\n",
        "                features2 = features2[:features1.shape[0]]\n",
        "            features = np.concatenate((features1, features2), axis=1)\n",
        "\n",
        "        elif self.modality == 'MIX2':\n",
        "\n",
        "\n",
        "            # Load RGB features\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            # Load corresponding audio features (accounting for 5:1 ratio)\n",
        "            audio_index = index // 5\n",
        "            if audio_index >= len(self.audio_list):\n",
        "              audio_index-=1\n",
        "            # print(f\"Index: {index}, Audio Index: {audio_index}, Length of audio_list: {len(self.audio_list)}\")\n",
        "            features2 = np.array(np.load(self.audio_list[audio_index].strip('\\n')), dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # print(\"features1 shape: \", features1.shape)\n",
        "            # print(\"features2 shape: \", features1.shape)\n",
        "\n",
        "            # Handle potential dimension mismatch\n",
        "            if features1.shape[0] > features2.shape[0]:\n",
        "                features1 = features1[:features2.shape[0]]\n",
        "            else:\n",
        "                features2 = features2[:features1.shape[0]]\n",
        "            features = np.concatenate((features1, features2), axis=1)\n",
        "\n",
        "        elif self.modality == 'MIX3':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index // 5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2), axis=1)\n",
        "            else:\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            features3 = np.array(np.load(self.audio_list[index // 5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2, features3), axis=1)\n",
        "            else:\n",
        "                features = np.concatenate((features1[:-1], features2, features3[:-1]), axis=1)\n",
        "        else:\n",
        "            print(\"WHAT IS WRONG\")\n",
        "            raise ValueError(\"Modality is wrong!\")\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.tranform is not None:\n",
        "            features = self.tranform(features)\n",
        "\n",
        "        # Handle test mode\n",
        "        if self.test_mode:\n",
        "            return features\n",
        "        else:\n",
        "            # Process features for training/validation\n",
        "            features = process_feat(features, self.max_seqlen, is_random=False)\n",
        "            # print(\"features.shape: \", features.shape)\n",
        "            return features, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list)"
      ],
      "metadata": {
        "id": "7_FVkEtCZsZu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v_clf1GTL1wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Vae model"
      ],
      "metadata": {
        "id": "R9DUrzJSE0mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.modality = 'MIX2'\n",
        "        # Original paths\n",
        "        self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "        self.flow_list = '/content/final_dl/list/flow.list'\n",
        "        self.audio_list = '/content/final_dl/list/audio.list'\n",
        "\n",
        "        # Train paths\n",
        "        self.train_rgb_list = '/content/final_dl/list/rgb_train.list'\n",
        "        self.train_flow_list = '/content/final_dl/list/flow_train.list'\n",
        "        self.train_audio_list = '/content/final_dl/list/audio_train.list'\n",
        "\n",
        "        # Val paths\n",
        "        self.val_rgb_list = '/content/final_dl/list/rgb_val.list'\n",
        "        self.val_flow_list = '/content/final_dl/list/flow_val.list'\n",
        "        self.val_audio_list = '/content/final_dl/list/audio_val.list'\n",
        "\n",
        "        # Test paths\n",
        "        self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "        self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "        self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "\n",
        "        self.gt = '/content/final_dl/list/gt.npy'\n",
        "        self.gpus = 1\n",
        "        self.lr = 0.0001\n",
        "        self.batch_size = 128\n",
        "        self.workers = 1  # Reduced from 4 to avoid memory issues\n",
        "        self.model_name = 'wsanodet'\n",
        "        self.pretrained_ckpt = None\n",
        "        self.feature_size = 1152  # 1024 + 128\n",
        "        self.num_classes = 1\n",
        "        self.dataset_name = 'XD-Violence'\n",
        "        self.max_seqlen = 200\n",
        "        self.max_epoch = 50\n",
        "\n",
        "args = Args()"
      ],
      "metadata": {
        "id": "C2BgXmcuO6b1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "args.modality=\"MIX2\"\n",
        "\n",
        "# Instantiate the VAE model\n",
        "vae_model = VAE(latent_dim=64, input_dim=args.feature_size, seq_len=200)\n",
        "\n",
        "# Load the model weights\n",
        "dir = \"/content/best_trained_vae.pkl\"\n",
        "vae_model.load_state_dict(torch.load(dir))\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vae_model = vae_model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "vae_model.eval()\n",
        "\n",
        "\n",
        "\n",
        "# Now vae_model is ready to be used\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Br7ppZIVEpki",
        "outputId": "e1673ff9-b3d7-4145-ebc8-29789509c28e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-f7530f009047>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  vae_model.load_state_dict(torch.load(dir))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (encoder): Encoder(\n",
              "    (encoder): Sequential(\n",
              "      (0): Conv1d(1152, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv1d(256, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): Conv1d(128, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (8): ReLU(inplace=True)\n",
              "      (9): Flatten(start_dim=1, end_dim=-1)\n",
              "    )\n",
              "    (lin_mean): Sequential(\n",
              "      (0): Linear(in_features=1600, out_features=64, bias=True)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (lin_log_var): Sequential(\n",
              "      (0): Linear(in_features=1600, out_features=64, bias=True)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (sampling): Sampling()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (decoder_fc): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=1600, bias=True)\n",
              "      (1): BatchNorm1d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (decoder_conv): Sequential(\n",
              "      (0): ConvTranspose1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
              "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): ConvTranspose1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
              "      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): ConvTranspose1d(256, 1152, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
              "      (7): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (8): Sigmoid()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modified HLNET Training\n"
      ],
      "metadata": {
        "id": "r0uL0WNZOpdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def validate(dataloader, model, criterion, device, is_topk):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        count = 0\n",
        "        for i, (input, label) in enumerate(dataloader):\n",
        "            seq_len = torch.sum(torch.max(torch.abs(input), dim=2)[0]>0, 1)\n",
        "            input = input[:, :torch.max(seq_len), :]\n",
        "            input, label = input.float().to(device), label.float().to(device)\n",
        "            logits, logits2 = model(input, seq_len)\n",
        "            clsloss = CLAS(logits, label, seq_len, criterion, device, is_topk)\n",
        "            clsloss2 = CLAS(logits2, label, seq_len, criterion, device, is_topk)\n",
        "            croloss = CENTROPY(logits, logits2, seq_len, device)\n",
        "\n",
        "            batch_loss = clsloss + clsloss2 + 5*croloss\n",
        "            total_loss += batch_loss.item()\n",
        "            count += 1\n",
        "        return total_loss / count if count > 0 else 0.0\n",
        "\n",
        "def test_hl_vae(dataloader, model, device, gt):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pred = torch.zeros(0).to(device)\n",
        "        pred2 = torch.zeros(0).to(device)\n",
        "        for i, input in enumerate(dataloader):\n",
        "\n",
        "\n",
        "            input = input.to(device)\n",
        "\n",
        "            # print(\"input shape: \", input.shape)\n",
        "            logits, logits2 = model(inputs=input, seq_len=None)\n",
        "            logits = torch.squeeze(logits)\n",
        "            sig = torch.sigmoid(logits)\n",
        "            sig = torch.mean(sig, 0)\n",
        "            pred = torch.cat((pred, sig))\n",
        "            '''\n",
        "            online detection\n",
        "            '''\n",
        "            logits2 = torch.squeeze(logits2)\n",
        "            sig2 = torch.sigmoid(logits2)\n",
        "            sig2 = torch.mean(sig2, 0)\n",
        "\n",
        "            sig2 = torch.unsqueeze(sig2, 1) ##for audio\n",
        "            pred2 = torch.cat((pred2, sig2))\n",
        "\n",
        "            # print(\"pred:, \", pred)\n",
        "            # print(\"pred2:, \", pred2)\n",
        "\n",
        "        pred = list(pred.cpu().detach().numpy())\n",
        "        pred2 = list(pred2.cpu().detach().numpy())\n",
        "\n",
        "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred, 16))\n",
        "        pr_auc = auc(recall, precision)\n",
        "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred2, 16))\n",
        "        pr_auc2 = auc(recall, precision)\n",
        "        return pr_auc, pr_auc2\n",
        "\n",
        "\n",
        "def train_hlnet_vae(dataloader, hlnet, vae, optimizer, scheduler, criterion, device, is_topk, HLNET_LOSS_WEIGHT, RECON_LOSS_WEIGHT):\n",
        "    hlnet.train()\n",
        "    vae.eval()\n",
        "    running_loss = 0.0\n",
        "    count = 0\n",
        "    for i, (input, label) in enumerate(dataloader):\n",
        "        inputcpy = input.float().to(device)\n",
        "        seq_len = torch.sum(torch.max(torch.abs(input), dim=2)[0]>0, 1)\n",
        "        input = input[:, :torch.max(seq_len), :]\n",
        "        input, label = input.float().to(device), label.float().to(device)\n",
        "        logits, logits2 = hlnet(input, seq_len)\n",
        "        clsloss = CLAS(logits, label, seq_len, criterion, device, is_topk)\n",
        "        clsloss2 = CLAS(logits2, label, seq_len, criterion, device, is_topk)\n",
        "        croloss = CENTROPY(logits, logits2, seq_len, device)\n",
        "\n",
        "        recon_loss = 0;\n",
        "\n",
        "        with torch.inference_mode():\n",
        "          recon_data, mu, logvar = vae(inputcpy)\n",
        "\n",
        "\n",
        "          # Reconstruction loss\n",
        "          recon_criterion = torch.nn.MSELoss(reduction='sum')\n",
        "          recon_loss = recon_criterion(recon_data, inputcpy)\n",
        "\n",
        "          # print(\"recon loss: \", recon_loss)\n",
        "\n",
        "\n",
        "\n",
        "          # KL divergence loss\n",
        "          # kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "          # # Total loss\n",
        "          # vae_loss = recon_loss + kl_loss\n",
        "\n",
        "        # print(\"step\")\n",
        "\n",
        "        recon_loss = recon_loss / 10000\n",
        "        total_loss = HLNET_LOSS_WEIGHT * (clsloss + clsloss2 + 5*croloss) + RECON_LOSS_WEIGHT * (recon_loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += total_loss.item()\n",
        "        count += 1\n",
        "\n",
        "        # Print training loss every 100 steps\n",
        "        if (i + 1) % 100 == 0:\n",
        "            avg_train_loss = running_loss / count\n",
        "            print(f\"Step {i+1}: Average Training Loss: {avg_train_loss:.4f}\")\n",
        "            running_loss = 0.0\n",
        "            count = 0\n",
        "\n",
        "            # If val_loader is provided, evaluate on validation set\n",
        "            if val_loader is not None:\n",
        "                val_loss = validate(val_loader, hlnet, criterion, device, is_topk)\n",
        "                print(f\"Step {i+1}: Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gHl5KgwPPZjj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Combined Model"
      ],
      "metadata": {
        "id": "f9EXkFqhOtqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weight hyperparameters\n",
        "HL_NET_LOSS_weight = 1\n",
        "RECON_LOSS_weight = .5\n",
        "\n",
        "args = Args()\n",
        "args.feature_size = 1152  # 1024 (RGB) + 128 (audio)\n",
        "args.batch_size = 128\n",
        "args.modality = 'MIX2'\n",
        "args.max_seqlen = 200\n",
        "args.workers = 1\n",
        "# args.batch_size = 5\n",
        "\n",
        "train_loader, val_loader, test_loader = create_data_loaders(args)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "for name, value in model.named_parameters():\n",
        "    print(name)\n",
        "approximator_param = list(map(id, model.approximator.parameters()))\n",
        "approximator_param += list(map(id, model.conv1d_approximator.parameters()))\n",
        "base_param = filter(lambda p: id(p) not in approximator_param, model.parameters())\n",
        "\n",
        "if not os.path.exists('./ckpt'):\n",
        "    os.makedirs('./ckpt')\n",
        "optimizer = optim.Adam([{'params': base_param},\n",
        "                        {'params': model.approximator.parameters(), 'lr': args.lr / 2},\n",
        "                        {'params': model.conv1d_approximator.parameters(), 'lr': args.lr / 2},\n",
        "                        ],\n",
        "                        lr=args.lr, weight_decay=0.000)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "is_topk = True\n",
        "gt = np.load(args.gt)\n",
        "\n",
        "print(\"shape of test set: \", len(test_loader))\n",
        "\n",
        "for epoch in range(args.max_epoch):\n",
        "    st = time.time()\n",
        "    model = train_hlnet_vae(train_loader, model, vae_model, optimizer, scheduler, criterion, device, is_topk, HL_NET_LOSS_weight, RECON_LOSS_weight)\n",
        "    print(\"here\")\n",
        "    if epoch % 2 == 0 and not epoch == 0:\n",
        "        torch.save(model.state_dict(), '/content/final_dl/'+args.model_name+'{}.pth'.format(epoch))\n",
        "\n",
        "    pr_auc, pr_auc_online = test_hl_vae(test_loader, model, device,gt)\n",
        "    print('Epoch {0}/{1}: offline pr_auc:{2:.4}; online pr_auc:{3:.4}\\n'.format(epoch, args.max_epoch, pr_auc, pr_auc_online))\n",
        "torch.save(model.state_dict(), '/content/' + args.model_name + '.pth')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GFnqtgN_Oyh6",
        "outputId": "ce57de20-2137-4479-dc50-8bd5236db000"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.Args object at 0x7cc86e0dd6f0>\n",
            "Creating data loaders...\n",
            "audio size\n",
            "3162\n",
            "rgb size\n",
            "15815\n",
            "Train loader created with 15815 samples\n",
            "Validation loader created with 3955 samples\n",
            "Test loader created with 4000 samples\n",
            "conv1d1.weight\n",
            "conv1d1.bias\n",
            "conv1d2.weight\n",
            "conv1d2.bias\n",
            "conv1d3.weight\n",
            "conv1d3.bias\n",
            "conv1d4.weight\n",
            "conv1d4.bias\n",
            "gc1.weight\n",
            "gc1.residual.weight\n",
            "gc1.residual.bias\n",
            "gc2.weight\n",
            "gc3.weight\n",
            "gc3.residual.weight\n",
            "gc3.residual.bias\n",
            "gc4.weight\n",
            "gc5.weight\n",
            "gc5.residual.weight\n",
            "gc5.residual.bias\n",
            "gc6.weight\n",
            "simAdj.weight0\n",
            "simAdj.weight1\n",
            "disAdj.sigma\n",
            "classifier.weight\n",
            "classifier.bias\n",
            "approximator.0.weight\n",
            "approximator.0.bias\n",
            "approximator.2.weight\n",
            "approximator.2.bias\n",
            "conv1d_approximator.weight\n",
            "conv1d_approximator.bias\n",
            "shape of test set:  800\n",
            "Step 100: Average Training Loss: 39.5494\n",
            "Step 100: Validation Loss: 3.3575\n",
            "here\n",
            "Epoch 0/50: offline pr_auc:0.2376; online pr_auc:0.2428\n",
            "\n",
            "Step 100: Average Training Loss: 39.5826\n",
            "Step 100: Validation Loss: 2.0053\n",
            "here\n",
            "Epoch 1/50: offline pr_auc:0.2768; online pr_auc:0.2405\n",
            "\n",
            "Step 100: Average Training Loss: 39.2949\n",
            "Step 100: Validation Loss: 1.8278\n",
            "here\n",
            "Epoch 2/50: offline pr_auc:0.3341; online pr_auc:0.2393\n",
            "\n",
            "Step 100: Average Training Loss: 39.3516\n",
            "Step 100: Validation Loss: 1.8108\n",
            "here\n",
            "Epoch 3/50: offline pr_auc:0.4064; online pr_auc:0.2404\n",
            "\n",
            "Step 100: Average Training Loss: 39.4139\n",
            "Step 100: Validation Loss: 1.7636\n",
            "here\n",
            "Epoch 4/50: offline pr_auc:0.4672; online pr_auc:0.256\n",
            "\n",
            "Step 100: Average Training Loss: 39.2377\n",
            "Step 100: Validation Loss: 1.7111\n",
            "here\n",
            "Epoch 5/50: offline pr_auc:0.5212; online pr_auc:0.2779\n",
            "\n",
            "Step 100: Average Training Loss: 39.3048\n",
            "Step 100: Validation Loss: 1.6299\n",
            "here\n",
            "Epoch 6/50: offline pr_auc:0.5825; online pr_auc:0.3066\n",
            "\n",
            "Step 100: Average Training Loss: 39.2540\n",
            "Step 100: Validation Loss: 1.5937\n",
            "here\n",
            "Epoch 7/50: offline pr_auc:0.6295; online pr_auc:0.3471\n",
            "\n",
            "Step 100: Average Training Loss: 39.1085\n",
            "Step 100: Validation Loss: 1.5557\n",
            "here\n",
            "Epoch 8/50: offline pr_auc:0.6567; online pr_auc:0.3921\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7cc7a13329e0>\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100: Average Training Loss: 39.0060\n",
            "Step 100: Validation Loss: 1.5331\n",
            "here\n",
            "Epoch 9/50: offline pr_auc:0.6797; online pr_auc:0.4377\n",
            "\n",
            "Step 100: Average Training Loss: 39.0785\n",
            "Step 100: Validation Loss: 1.5168\n",
            "here\n",
            "Epoch 10/50: offline pr_auc:0.7104; online pr_auc:0.4792\n",
            "\n",
            "Step 100: Average Training Loss: 39.0042\n",
            "Step 100: Validation Loss: 1.4815\n",
            "here\n",
            "Epoch 11/50: offline pr_auc:0.7201; online pr_auc:0.5064\n",
            "\n",
            "Step 100: Average Training Loss: 38.8671\n",
            "Step 100: Validation Loss: 1.4498\n",
            "here\n",
            "Epoch 12/50: offline pr_auc:0.7303; online pr_auc:0.5279\n",
            "\n",
            "Step 100: Average Training Loss: 38.9635\n",
            "Step 100: Validation Loss: 1.4164\n",
            "here\n",
            "Epoch 13/50: offline pr_auc:0.7326; online pr_auc:0.5392\n",
            "\n",
            "Step 100: Average Training Loss: 38.8343\n",
            "Step 100: Validation Loss: 1.3847\n",
            "here\n",
            "Epoch 14/50: offline pr_auc:0.7351; online pr_auc:0.5572\n",
            "\n",
            "Step 100: Average Training Loss: 38.8373\n",
            "Step 100: Validation Loss: 1.3622\n",
            "here\n",
            "Epoch 15/50: offline pr_auc:0.7379; online pr_auc:0.5704\n",
            "\n",
            "Step 100: Average Training Loss: 38.9313\n",
            "Step 100: Validation Loss: 1.3496\n",
            "here\n",
            "Epoch 16/50: offline pr_auc:0.743; online pr_auc:0.5842\n",
            "\n",
            "Step 100: Average Training Loss: 38.9410\n",
            "Step 100: Validation Loss: 1.3082\n",
            "here\n",
            "Epoch 17/50: offline pr_auc:0.7441; online pr_auc:0.5949\n",
            "\n",
            "Step 100: Average Training Loss: 38.6798\n",
            "Step 100: Validation Loss: 1.2946\n",
            "here\n",
            "Epoch 18/50: offline pr_auc:0.7481; online pr_auc:0.6055\n",
            "\n",
            "Step 100: Average Training Loss: 39.0155\n",
            "Step 100: Validation Loss: 1.2799\n",
            "here\n",
            "Epoch 19/50: offline pr_auc:0.7472; online pr_auc:0.6174\n",
            "\n",
            "Step 100: Average Training Loss: 38.8088\n",
            "Step 100: Validation Loss: 1.2463\n",
            "here\n",
            "Epoch 20/50: offline pr_auc:0.7468; online pr_auc:0.6257\n",
            "\n",
            "Step 100: Average Training Loss: 38.9070\n",
            "Step 100: Validation Loss: 1.2085\n",
            "here\n",
            "Epoch 21/50: offline pr_auc:0.7502; online pr_auc:0.636\n",
            "\n",
            "Step 100: Average Training Loss: 38.8748\n",
            "Step 100: Validation Loss: 1.1845\n",
            "here\n",
            "Epoch 22/50: offline pr_auc:0.7499; online pr_auc:0.641\n",
            "\n",
            "Step 100: Average Training Loss: 38.8373\n",
            "Step 100: Validation Loss: 1.1780\n",
            "here\n",
            "Epoch 23/50: offline pr_auc:0.75; online pr_auc:0.6449\n",
            "\n",
            "Step 100: Average Training Loss: 38.9492\n",
            "Step 100: Validation Loss: 1.1485\n",
            "here\n",
            "Epoch 24/50: offline pr_auc:0.7543; online pr_auc:0.6506\n",
            "\n",
            "Step 100: Average Training Loss: 38.8377\n",
            "Step 100: Validation Loss: 1.1199\n",
            "here\n",
            "Epoch 25/50: offline pr_auc:0.7557; online pr_auc:0.6562\n",
            "\n",
            "Step 100: Average Training Loss: 38.7496\n",
            "Step 100: Validation Loss: 1.1077\n",
            "here\n",
            "Epoch 26/50: offline pr_auc:0.7589; online pr_auc:0.6627\n",
            "\n",
            "Step 100: Average Training Loss: 38.7615\n",
            "Step 100: Validation Loss: 1.0798\n",
            "here\n",
            "Epoch 27/50: offline pr_auc:0.7584; online pr_auc:0.6675\n",
            "\n",
            "Step 100: Average Training Loss: 38.7682\n",
            "Step 100: Validation Loss: 1.0603\n",
            "here\n",
            "Epoch 28/50: offline pr_auc:0.762; online pr_auc:0.6716\n",
            "\n",
            "Step 100: Average Training Loss: 38.7779\n",
            "Step 100: Validation Loss: 1.0475\n",
            "here\n",
            "Epoch 29/50: offline pr_auc:0.765; online pr_auc:0.6768\n",
            "\n",
            "Step 100: Average Training Loss: 38.7800\n",
            "Step 100: Validation Loss: 1.0365\n",
            "here\n",
            "Epoch 30/50: offline pr_auc:0.7633; online pr_auc:0.6802\n",
            "\n",
            "Step 100: Average Training Loss: 38.6784\n",
            "Step 100: Validation Loss: 1.0265\n",
            "here\n",
            "Epoch 31/50: offline pr_auc:0.7671; online pr_auc:0.6852\n",
            "\n",
            "Step 100: Average Training Loss: 38.6422\n",
            "Step 100: Validation Loss: 1.0011\n",
            "here\n",
            "Epoch 32/50: offline pr_auc:0.7683; online pr_auc:0.6892\n",
            "\n",
            "Step 100: Average Training Loss: 38.6762\n",
            "Step 100: Validation Loss: 0.9923\n",
            "here\n",
            "Epoch 33/50: offline pr_auc:0.7682; online pr_auc:0.6917\n",
            "\n",
            "Step 100: Average Training Loss: 38.5565\n",
            "Step 100: Validation Loss: 0.9715\n",
            "here\n",
            "Epoch 34/50: offline pr_auc:0.7689; online pr_auc:0.6951\n",
            "\n",
            "Step 100: Average Training Loss: 38.5457\n",
            "Step 100: Validation Loss: 0.9755\n",
            "here\n",
            "Epoch 35/50: offline pr_auc:0.7684; online pr_auc:0.6971\n",
            "\n",
            "Step 100: Average Training Loss: 38.6263\n",
            "Step 100: Validation Loss: 0.9767\n",
            "here\n",
            "Epoch 36/50: offline pr_auc:0.7699; online pr_auc:0.6996\n",
            "\n",
            "Step 100: Average Training Loss: 38.6148\n",
            "Step 100: Validation Loss: 0.9573\n",
            "here\n",
            "Epoch 37/50: offline pr_auc:0.7676; online pr_auc:0.7011\n",
            "\n",
            "Step 100: Average Training Loss: 38.5767\n",
            "Step 100: Validation Loss: 0.9254\n",
            "here\n",
            "Epoch 38/50: offline pr_auc:0.7677; online pr_auc:0.7036\n",
            "\n",
            "Step 100: Average Training Loss: 38.7014\n",
            "Step 100: Validation Loss: 0.9425\n",
            "here\n",
            "Epoch 39/50: offline pr_auc:0.7703; online pr_auc:0.7072\n",
            "\n",
            "Step 100: Average Training Loss: 38.7323\n",
            "Step 100: Validation Loss: 0.9384\n",
            "here\n",
            "Epoch 40/50: offline pr_auc:0.7725; online pr_auc:0.7101\n",
            "\n",
            "Step 100: Average Training Loss: 38.7290\n",
            "Step 100: Validation Loss: 0.9146\n",
            "here\n",
            "Epoch 41/50: offline pr_auc:0.7752; online pr_auc:0.7117\n",
            "\n",
            "Step 100: Average Training Loss: 38.6797\n",
            "Step 100: Validation Loss: 0.9013\n",
            "here\n",
            "Epoch 42/50: offline pr_auc:0.7781; online pr_auc:0.7155\n",
            "\n",
            "Step 100: Average Training Loss: 38.5160\n",
            "Step 100: Validation Loss: 0.8921\n",
            "here\n",
            "Epoch 43/50: offline pr_auc:0.7761; online pr_auc:0.7148\n",
            "\n",
            "Step 100: Average Training Loss: 38.5251\n",
            "Step 100: Validation Loss: 0.9053\n",
            "here\n",
            "Epoch 44/50: offline pr_auc:0.7787; online pr_auc:0.7186\n",
            "\n",
            "Step 100: Average Training Loss: 38.6954\n",
            "Step 100: Validation Loss: 0.9046\n",
            "here\n",
            "Epoch 45/50: offline pr_auc:0.7769; online pr_auc:0.7184\n",
            "\n",
            "Step 100: Average Training Loss: 38.5515\n",
            "Step 100: Validation Loss: 0.9111\n",
            "here\n",
            "Epoch 46/50: offline pr_auc:0.7767; online pr_auc:0.7189\n",
            "\n",
            "Step 100: Average Training Loss: 38.5395\n",
            "Step 100: Validation Loss: 0.8723\n",
            "here\n",
            "Epoch 47/50: offline pr_auc:0.78; online pr_auc:0.7216\n",
            "\n",
            "Step 100: Average Training Loss: 38.6229\n",
            "Step 100: Validation Loss: 0.8699\n",
            "here\n",
            "Epoch 48/50: offline pr_auc:0.7774; online pr_auc:0.7222\n",
            "\n",
            "Step 100: Average Training Loss: 38.5185\n",
            "Step 100: Validation Loss: 0.8985\n",
            "here\n",
            "Epoch 49/50: offline pr_auc:0.7783; online pr_auc:0.7241\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hjC148BrOxDX"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}