{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahdrakes/mldl-final/blob/main/mm_violence_det_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDOr6KirJvUe"
      },
      "source": [
        "# Multi-Modal Violence Detection Network\n",
        "\n",
        "original src code: https://github.com/Roc-Ng/XDVioDet.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhzPvDXTKKqM"
      },
      "source": [
        "### Copying Training and Testing Data\n",
        "\n",
        "The folders are pretty large (~40/50GB) so it takes a while to copy all of the data over.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4DG4K3wYK8P",
        "outputId": "8ddea142-823d-4f47-a04e-17e1b3c4d4cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /mydrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/mydrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88GWL7hscvcg",
        "outputId": "2b6bfcc6-8d7f-4796-f72c-e9dc85f6caff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/mydrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /mydrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nr7SpsS-2P6x"
      },
      "outputs": [],
      "source": [
        "!unzip final_dl.zip -d /content/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zWKH9vsRatb"
      },
      "source": [
        "may need to change directory depending on where you upload the data to google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CHQ29LtvAsNj"
      },
      "outputs": [],
      "source": [
        "# !cp -r /mydrive/MyDrive/final_dl ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N1LDIKfKhpP"
      },
      "source": [
        "## 1. Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9acUBEhKqjC"
      },
      "source": [
        "### A) Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IPMWSbm0LBGY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc, precision_recall_curve\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def test(dataloader, model, device, gt):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pred = torch.zeros(0).to(device)\n",
        "        pred2 = torch.zeros(0).to(device)\n",
        "        for i, input in enumerate(dataloader):\n",
        "            input = input.to(device)\n",
        "            logits, logits2 = model(inputs=input, seq_len=None)\n",
        "            logits = torch.squeeze(logits)\n",
        "            sig = torch.sigmoid(logits)\n",
        "            sig = torch.mean(sig, 0)\n",
        "            pred = torch.cat((pred, sig))\n",
        "            '''\n",
        "            online detection\n",
        "            '''\n",
        "            logits2 = torch.squeeze(logits2)\n",
        "            sig2 = torch.sigmoid(logits2)\n",
        "            sig2 = torch.mean(sig2, 0)\n",
        "\n",
        "            sig2 = torch.unsqueeze(sig2, 1) ##for audio\n",
        "            pred2 = torch.cat((pred2, sig2))\n",
        "\n",
        "            # print(\"pred:, \", pred)\n",
        "            # print(\"pred2:, \", pred2)\n",
        "\n",
        "        pred = list(pred.cpu().detach().numpy())\n",
        "        pred2 = list(pred2.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred, 16))\n",
        "        pr_auc = auc(recall, precision)\n",
        "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred2, 16))\n",
        "        pr_auc2 = auc(recall, precision)\n",
        "        return pr_auc, pr_auc2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLV269m9K7L3"
      },
      "source": [
        "### B) Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej2MARCmKzvk"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def random_extract(feat, t_max):\n",
        "   r = np.random.randint(len(feat)-t_max)\n",
        "   return feat[r:r+t_max]\n",
        "\n",
        "def uniform_extract(feat, t_max):\n",
        "   r = np.linspace(0, len(feat)-1, t_max, dtype=np.uint16)\n",
        "   return feat[r, :]\n",
        "\n",
        "def pad(feat, min_len):\n",
        "    if np.shape(feat)[0] <= min_len:\n",
        "       return np.pad(feat, ((0, min_len-np.shape(feat)[0]), (0, 0)), mode='constant', constant_values=0)\n",
        "    else:\n",
        "       return feat\n",
        "\n",
        "def process_feat(feat, length, is_random=True):\n",
        "    if len(feat) > length:\n",
        "        if is_random:\n",
        "            return random_extract(feat, length)\n",
        "        else:\n",
        "            return uniform_extract(feat, length)\n",
        "    else:\n",
        "        return pad(feat, length)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKIYnHpLDKS"
      },
      "source": [
        "### C) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98z1xwOKKueM"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "\n",
        "# from utils import process_feat\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, args, transform=None, mode='train'):\n",
        "        self.modality = args.modality\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            args: Arguments containing dataset paths and configuration\n",
        "            transform: Optional transforms to apply\n",
        "            mode: One of ['train', 'val', 'test'] to specify the dataset split\n",
        "        \"\"\"\n",
        "\n",
        "        if mode == 'test':\n",
        "            self.rgb_list_file = args.test_rgb_list\n",
        "            self.flow_list_file = args.test_flow_list\n",
        "            self.audio_list_file = args.test_audio_list\n",
        "        elif mode == 'val':\n",
        "            self.rgb_list_file = args.val_rgb_list\n",
        "            self.flow_list_file = args.val_flow_list\n",
        "            self.audio_list_file = args.val_audio_list\n",
        "        else: # train\n",
        "            self.rgb_list_file = args.train_rgb_list\n",
        "            self.flow_list_file = args.train_flow_list\n",
        "            self.audio_list_file = args.train_audio_list\n",
        "\n",
        "        self.max_seqlen = args.max_seqlen\n",
        "        self.tranform = transform\n",
        "        self.test_mode = (mode == 'test')\n",
        "        self.normal_flag = '_label_A'\n",
        "        self._parse_list()\n",
        "\n",
        "    def _parse_list(self):\n",
        "        if self.modality == 'AUDIO':\n",
        "            self.list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'RGB':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            print(\"here\")\n",
        "            # print(self.list)\n",
        "        elif self.modality == 'FLOW':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX2':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX3':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        else:\n",
        "            assert 1 > 2, 'Modality is wrong!'\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.normal_flag in self.list[index]:\n",
        "            label = 0.0\n",
        "        else:\n",
        "            label = 1.0\n",
        "\n",
        "        if self.modality == 'AUDIO':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'RGB':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')),dtype=np.float32)\n",
        "        elif self.modality == 'FLOW':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'MIX':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX2':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'MIX3':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            features3 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2, features3),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2, features3[:-1]), axis=1)\n",
        "        else:\n",
        "            assert 1>2, 'Modality is wrong!'\n",
        "        if self.tranform is not None:\n",
        "            features = self.tranform(features)\n",
        "        if self.test_mode:\n",
        "            return features\n",
        "\n",
        "        else:\n",
        "            features = process_feat(features, self.max_seqlen, is_random=False)\n",
        "            return features, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvvMYO_1LXTV"
      },
      "source": [
        "### D) Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bYn0UsFKWTy"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "from torch import FloatTensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(in_features, out_features).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
        "        self.a = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(2*out_features, 1).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        h = torch.mm(input, self.W)\n",
        "        N = h.size()[0]\n",
        "\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
        "\n",
        "class linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(linear, self).__init__()\n",
        "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.register_parameter('bias', None)\n",
        "        stdv = 1. / sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "    def forward(self, x):\n",
        "        x = x.matmul(self.weight)\n",
        "        return x\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=False, residual=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "        elif (in_features == out_features):\n",
        "            self.residual = lambda x: x\n",
        "        else:\n",
        "            # self.residual = linear(in_features, out_features)\n",
        "            self.residual = nn.Conv1d(in_channels=in_features, out_channels=out_features, kernel_size=5, padding=2)\n",
        "    def reset_parameters(self):\n",
        "        # stdv = 1. / sqrt(self.weight.size(1))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # To support batch operations\n",
        "        support = input.matmul(self.weight)\n",
        "        output = adj.matmul(support)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        if self.in_features != self.out_features and self.residual:\n",
        "            input = input.permute(0,2,1)\n",
        "            res = self.residual(input)\n",
        "            res = res.permute(0,2,1)\n",
        "            output = output + res\n",
        "        else:\n",
        "            output = output + self.residual(input)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "######################################################\n",
        "\n",
        "class SimilarityAdj(Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(SimilarityAdj, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight0 = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.weight1 = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # stdv = 1. / sqrt(self.weight0.size(1))\n",
        "        nn.init.xavier_uniform_(self.weight0)\n",
        "        nn.init.xavier_uniform_(self.weight1)\n",
        "\n",
        "    def forward(self, input, seq_len):\n",
        "        # To support batch operations\n",
        "        soft = nn.Softmax(1)\n",
        "        theta = torch.matmul(input, self.weight0)\n",
        "        phi = torch.matmul(input, self.weight0)\n",
        "        phi2 = phi.permute(0, 2, 1)\n",
        "        sim_graph = torch.matmul(theta, phi2)\n",
        "\n",
        "        theta_norm = torch.norm(theta, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        phi_norm = torch.norm(phi, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        x_norm_x = theta_norm.matmul(phi_norm.permute(0, 2, 1))\n",
        "        sim_graph = sim_graph / (x_norm_x + 1e-20)\n",
        "\n",
        "        output = torch.zeros_like(sim_graph)\n",
        "        if seq_len is None:\n",
        "            for i in range(sim_graph.shape[0]):\n",
        "                tmp = sim_graph[i]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = sim_graph[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "class DistanceAdj(Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DistanceAdj, self).__init__()\n",
        "        self.sigma = Parameter(FloatTensor(1))\n",
        "        self.sigma.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, batch_size, max_seqlen):\n",
        "        # To support batch operations\n",
        "        self.arith = np.arange(max_seqlen).reshape(-1, 1)\n",
        "        dist = pdist(self.arith, metric='cityblock').astype(np.float32)\n",
        "        self.dist = torch.from_numpy(squareform(dist)).to('cuda')\n",
        "        self.dist = torch.exp(-self.dist / torch.exp(torch.tensor(1.)))\n",
        "        self.dist = torch.unsqueeze(self.dist, 0).repeat(batch_size, 1, 1).to('cuda')\n",
        "        return self.dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44eeeOUcLquB"
      },
      "source": [
        "### E) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8Rjvb-yKOeZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as torch_init\n",
        "import os\n",
        "# from layers import GraphConvolution, SimilarityAdj, DistanceAdj\n",
        "\n",
        "\n",
        "def weight_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
        "        torch_init.xavier_uniform_(m.weight)\n",
        "        # m.bias.data.fill_(0.1)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        n_features = args.feature_size\n",
        "        n_class = args.num_classes\n",
        "\n",
        "        self.conv1d1 = nn.Conv1d(in_channels=n_features, out_channels=512, kernel_size=1, padding=0)\n",
        "        self.conv1d2 = nn.Conv1d(in_channels=512, out_channels=128, kernel_size=1, padding=0)\n",
        "        self.conv1d3 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=5, padding=2)\n",
        "        self.conv1d4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
        "        # Graph Convolution\n",
        "        self.gc1 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc2 = GraphConvolution(32, 32, residual=True)\n",
        "        self.gc3 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc4 = GraphConvolution(32, 32, residual=True)\n",
        "        self.gc5 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc6 = GraphConvolution(32, 32, residual=True)\n",
        "        self.simAdj = SimilarityAdj(n_features, 32)\n",
        "        self.disAdj = DistanceAdj()\n",
        "\n",
        "        self.classifier = nn.Linear(32*3, n_class)\n",
        "        self.approximator = nn.Sequential(nn.Conv1d(128, 64, 1, padding=0), nn.ReLU(),\n",
        "                                          nn.Conv1d(64, 32, 1, padding=0), nn.ReLU())\n",
        "        self.conv1d_approximator = nn.Conv1d(32, 1, 5, padding=0)\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.apply(weight_init)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, inputs, seq_len):\n",
        "        x = inputs.permute(0, 2, 1)  # for conv1d\n",
        "        x = self.relu(self.conv1d1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.conv1d2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        logits = self.approximator(x)\n",
        "        logits = F.pad(logits, (4, 0))\n",
        "        logits = self.conv1d_approximator(logits)\n",
        "        logits = logits.permute(0, 2, 1)\n",
        "        x = x.permute(0, 2, 1)  # b*t*c\n",
        "\n",
        "        ## gcn\n",
        "        scoadj = self.sadj(logits.detach(), seq_len)\n",
        "        adj = self.adj(inputs, seq_len)\n",
        "        disadj = self.disAdj(x.shape[0], x.shape[1])\n",
        "        x1_h = self.relu(self.gc1(x, adj))\n",
        "        x1_h = self.dropout(x1_h)\n",
        "        x2_h = self.relu(self.gc3(x, disadj))\n",
        "        x2_h = self.dropout(x2_h)\n",
        "        x3_h = self.relu(self.gc5(x, scoadj))\n",
        "        x3_h = self.dropout(x3_h)\n",
        "        x1 = self.relu(self.gc2(x1_h, adj))\n",
        "        x1 = self.dropout(x1)\n",
        "        x2 = self.relu(self.gc4(x2_h, disadj))\n",
        "        x2 = self.dropout(x2)\n",
        "        x3 = self.relu(self.gc6(x3_h, scoadj))\n",
        "        x3 = self.dropout(x3)\n",
        "        x = torch.cat((x1, x2, x3), 2)\n",
        "        x = self.classifier(x)\n",
        "        return x, logits\n",
        "\n",
        "    def sadj(self, logits, seq_len):\n",
        "        lens = logits.shape[1]\n",
        "        soft = nn.Softmax(1)\n",
        "        logits2 = self.sigmoid(logits).repeat(1, 1, lens)\n",
        "        tmp = logits2.permute(0, 2, 1)\n",
        "        adj = 1. - torch.abs(logits2 - tmp)\n",
        "        self.sig = lambda x:1/(1+torch.exp(-((x-0.5))/0.1))\n",
        "        adj = self.sig(adj)\n",
        "        output = torch.zeros_like(adj)\n",
        "        if seq_len is None:\n",
        "            for i in range(logits.shape[0]):\n",
        "                tmp = adj[i]\n",
        "                adj2 = soft(tmp)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = adj[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = soft(tmp)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "        return output\n",
        "\n",
        "\n",
        "    def adj(self, x, seq_len):\n",
        "        soft = nn.Softmax(1)\n",
        "        x2 = x.matmul(x.permute(0,2,1)) # B*T*T\n",
        "        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        x_norm_x = x_norm.matmul(x_norm.permute(0,2,1))\n",
        "        x2 = x2/(x_norm_x+1e-20)\n",
        "        output = torch.zeros_like(x2)\n",
        "        if seq_len is None:\n",
        "            for i in range(x.shape[0]):\n",
        "                tmp = x2[i]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = x2[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE8lvyeyTk0C",
        "outputId": "7aaaea0c-4443-4d71-eb00-aaa01e97667f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /mydrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Current directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z-JVZlBNeg4"
      },
      "source": [
        "## Args\n",
        "\n",
        "Here are the default args that were obtained via cmd line arg parser. I just created a class 'Args' that holds the default config for the model.\n",
        "\n",
        "I think the most important args:\n",
        "\n",
        "*`Modality`*: Determines whether we want to use either audio alone, video alone, both audio and video, audio, video, and flow, etc. for training\n",
        "\n",
        "*`List`*: point to the list containing filenames for all training and testing data.\n",
        "\n",
        "*`workers`*: I believe this is the number of individual threads/processes running during training or testing. In ther model it was set to 4 by defualt but that spit out an error so it lowered it to 1. Prob a sign that we need to do heavy downsampling to compensate for lack of parallel processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g_4ciyOM8tl"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.modality = 'MIX2'\n",
        "        # Original paths\n",
        "        self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "        self.flow_list = '/content/final_dl/list/flow.list'\n",
        "        self.audio_list = '/content/final_dl/list/audio.list'\n",
        "\n",
        "        # Train paths\n",
        "        self.train_rgb_list = '/content/final_dl/list/rgb_train.list'\n",
        "        self.train_flow_list = '/content/final_dl/list/flow_train.list'\n",
        "        self.train_audio_list = '/content/final_dl/list/audio_train.list'\n",
        "\n",
        "        # Val paths\n",
        "        self.val_rgb_list = '/content/final_dl/list/rgb_val.list'\n",
        "        self.val_flow_list = '/content/final_dl/list/flow_val.list'\n",
        "        self.val_audio_list = '/content/final_dl/list/audio_val.list'\n",
        "\n",
        "        # Test paths\n",
        "        self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "        self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "        self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "\n",
        "        self.gt = '/content/final_dl/list/gt.npy'\n",
        "        self.gpus = 1\n",
        "        self.lr = 0.0001\n",
        "        self.batch_size = 128\n",
        "        self.workers = 1  # Reduced from 4 to avoid memory issues\n",
        "        self.model_name = 'wsanodet'\n",
        "        self.pretrained_ckpt = None\n",
        "        self.feature_size = 1152  # 1024 + 128\n",
        "        self.num_classes = 1\n",
        "        self.dataset_name = 'XD-Violence'\n",
        "        self.max_seqlen = 200\n",
        "        self.max_epoch = 50\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Val Split For MultiModal Data"
      ],
      "metadata": {
        "id": "7xM7Rywzs4s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "def get_video_id(filepath):\n",
        "    \"\"\"Extract video ID from filepath based on common prefix before _label\n",
        "    e.g., \"/path/to/video123_label_A.npy\" -> \"video123\"\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(filepath)\n",
        "    if '_label' in filename:\n",
        "        return filename.split('_label')[0]\n",
        "    return filename.split('.')[0]\n",
        "\n",
        "def find_matching_files():\n",
        "    \"\"\"\n",
        "    Find and align RGB and audio feature files.\n",
        "    Returns dict mapping video IDs to their RGB and audio paths\n",
        "    \"\"\"\n",
        "    rgb_path = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
        "    audio_path = \"/content/final_dl/list/xx/train\"\n",
        "\n",
        "    # Get all files\n",
        "    rgb_files = glob.glob(os.path.join(rgb_path, \"*.npy\"))\n",
        "    audio_files = glob.glob(os.path.join(audio_path, \"*.npy\"))\n",
        "\n",
        "    # Create mappings that preserve the 5:1 ratio\n",
        "    rgb_map = {}\n",
        "    for f in rgb_files:\n",
        "        vid_id = get_video_id(f)\n",
        "        if vid_id not in rgb_map:\n",
        "            rgb_map[vid_id] = []\n",
        "        rgb_map[vid_id].append(f)\n",
        "\n",
        "    audio_map = {get_video_id(f): f for f in audio_files}\n",
        "\n",
        "    # Find common video IDs\n",
        "    common_ids = set(rgb_map.keys()) & set(audio_map.keys())\n",
        "\n",
        "    # Create aligned mapping\n",
        "    aligned_files = {\n",
        "        vid_id: {\n",
        "            'rgb': sorted(rgb_map[vid_id]),  # Sort to maintain consistent ordering\n",
        "            'audio': audio_map[vid_id],\n",
        "            'is_normal': '_label_A' in rgb_map[vid_id][0]  # Check first RGB file for label\n",
        "        }\n",
        "        for vid_id in common_ids\n",
        "    }\n",
        "\n",
        "    print(f\"Found {len(aligned_files)} aligned RGB-Audio pairs\")\n",
        "    return aligned_files\n",
        "\n",
        "def create_splits(aligned_files, train_ratio=0.8, seed=42):\n",
        "    \"\"\"Split the video IDs first, then we'll expand to files in write_list_files\"\"\"\n",
        "    random.seed(seed)\n",
        "    video_ids = list(aligned_files.keys())\n",
        "    train_size = int(len(video_ids) * train_ratio)\n",
        "    train_ids = random.sample(video_ids, train_size)\n",
        "    val_ids = [vid for vid in video_ids if vid not in train_ids]\n",
        "\n",
        "    return {\n",
        "        'train': train_ids,\n",
        "        'val': val_ids\n",
        "    }\n",
        "\n",
        "def write_list_files(split_data, aligned_files, output_dir=\"/content/final_dl/list\"):\n",
        "    \"\"\"Write list files with audio files repeated to match RGB structure\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for split_name, video_ids in split_data.items():\n",
        "        # RGB list - one entry per frame\n",
        "        rgb_path = os.path.join(output_dir, f'rgb_{split_name}.list')\n",
        "        with open(rgb_path, 'w') as f:\n",
        "            for vid_id in video_ids:\n",
        "                for rgb_file in aligned_files[vid_id]['rgb']:\n",
        "                    f.write(f\"{rgb_file}\\n\")\n",
        "\n",
        "        # Audio list - one entry per video (not per frame)\n",
        "        audio_path = os.path.join(output_dir, f'audio_{split_name}.list')\n",
        "        with open(audio_path, 'w') as f:\n",
        "            for vid_id in video_ids:\n",
        "                audio_file = aligned_files[vid_id]['audio']\n",
        "                f.write(f\"{audio_file}\\n\")  # Write once only\n"
      ],
      "metadata": {
        "id": "NRVNY_NEcEB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataloaders for multimodal"
      ],
      "metadata": {
        "id": "gJzaD9WsT3YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aligned_files = find_matching_files()\n",
        "\n",
        "    # Create train/val splits\n",
        "split_data = create_splits(aligned_files)\n",
        "\n",
        "    # Write list files\n",
        "write_list_files(split_data, aligned_files)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "def create_data_loaders(args):\n",
        "    \"\"\"\n",
        "    Create train, validation and test data loaders\n",
        "    \"\"\"\n",
        "    print(\"Creating data loaders...\")\n",
        "\n",
        "    # Create train loader\n",
        "    train_dataset = Dataset(args, mode='train')\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    print(f\"Train loader created with {len(train_dataset)} samples\")\n",
        "\n",
        "    # Create validation loader\n",
        "    val_dataset = Dataset(args, mode='val')\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,  # No need to shuffle validation data\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    print(f\"Validation loader created with {len(val_dataset)} samples\")\n",
        "\n",
        "    # Create test loader with smaller batch size as per original code\n",
        "    test_dataset = Dataset(args, mode='test')\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=5,  # Using smaller batch size for testing\n",
        "        shuffle=False,\n",
        "        num_workers=args.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    print(f\"Test loader created with {len(test_dataset)} samples\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = create_data_loaders(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVCYAPTSTuQY",
        "outputId": "71945685-3bce-40a0-c190-9fb7c20fde9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3953 aligned RGB-Audio pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAL SPLIT FOR SINGLE MODALITY"
      ],
      "metadata": {
        "id": "CsN1t5SrTQoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_single_modality_data_loaders(args, modality='AUDIO'):\n",
        "    \"\"\"\n",
        "    Create train, validation and test data loaders for a single modality\n",
        "    \"\"\"\n",
        "    print(f\"Creating {modality} data loaders...\")\n",
        "\n",
        "    # Create new args with only needed attributes\n",
        "    args_new = Args()\n",
        "    args_new.modality = modality\n",
        "\n",
        "    # List files needed for train/val/test splits\n",
        "    if modality == 'AUDIO':\n",
        "        args_new.train_audio_list = args.train_audio_list\n",
        "        args_new.val_audio_list = args.val_audio_list\n",
        "        args_new.test_audio_list = args.test_audio_list\n",
        "    elif modality == 'RGB':\n",
        "        args_new.train_rgb_list = args.train_rgb_list\n",
        "        args_new.val_rgb_list = args.val_rgb_list\n",
        "        args_new.test_rgb_list = args.test_rgb_list\n",
        "    elif modality == 'FLOW':\n",
        "        args_new.train_flow_list = args.train_flow_list\n",
        "        args_new.val_flow_list = args.val_flow_list\n",
        "        args_new.test_flow_list = args.test_flow_list\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataset = Dataset(args_new, mode='train')\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args_new.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args_new.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    print(f\"Train loader created with {len(train_dataset)} samples\")\n",
        "\n",
        "    val_dataset = Dataset(args_new, mode='val')\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=args_new.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=args_new.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    print(f\"Validation loader created with {len(val_dataset)} samples\")\n",
        "\n",
        "    test_dataset = Dataset(args_new, mode='test')\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=5,\n",
        "        shuffle=False,\n",
        "        num_workers=args_new.workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    print(f\"Test loader created with {len(test_dataset)} samples\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "Y7aaqgH5TP66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create data loader for specified modality"
      ],
      "metadata": {
        "id": "PNM_LV6qT_q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For audio only\n",
        "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='AUDIO')\n",
        "\n",
        "# For RGB only\n",
        "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='RGB')\n",
        "\n",
        "# For flow only\n",
        "## CURRENTLY, FLOW IS NOT SUPPORTED, BUT IMPLEMENTING IT WOULD NOT BE THAT CHALLENGING. YOU WOULD SIMPLY HAVE TO\n",
        "## ADJUST THE CODE A FEW CELLS ABOVE SO TO WRITE AN EQUIVALENT OF \"find_matching_files\" FOR FLOW DATA\n",
        "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='FLOW')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "OtwEJzeJTr-i",
        "outputId": "c316512e-d61f-4270-dc0d-e0451e4e61ab",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating FLOW data loaders...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/final_dl/list/flow_train.list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-90de7e97c7de>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# For flow only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_single_modality_data_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FLOW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-deda894dcfff>\u001b[0m in \u001b[0;36mcreate_single_modality_data_loaders\u001b[0;34m(args, modality)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Create data loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     train_loader = DataLoader(\n\u001b[1;32m     28\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c5268617e9c2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, transform, mode)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_label_A'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c5268617e9c2>\u001b[0m in \u001b[0;36m_parse_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# print(self.list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'FLOW'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_list_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'MIX'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_list_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/final_dl/list/flow_train.list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the val splitter, no need to run this\n",
        "\n",
        "def inspect_batch_files(args, batch_size=1, num_batches=5):\n",
        "    \"\"\"\n",
        "    Inspect the first few batches to see which files are being loaded and their dimensions\n",
        "    \"\"\"\n",
        "    #from torch.utils.data import DataLoader\n",
        "    #from dataset import Dataset  # Your dataset class\n",
        "\n",
        "    dataset = Dataset(args, mode='train')\n",
        "\n",
        "    print(\"Inspecting individual samples:\")\n",
        "    for i in range(min(25, len(dataset))):\n",
        "        try:\n",
        "            # Get the filepaths that would be loaded\n",
        "            rgb_path = dataset.list[i].strip('\\n')\n",
        "            audio_path = dataset.audio_list[i//5].strip('\\n')\n",
        "\n",
        "            # Load the features\n",
        "            features1 = np.array(np.load(rgb_path), dtype=np.float32)\n",
        "            features2 = np.array(np.load(audio_path), dtype=np.float32)\n",
        "\n",
        "            print(f\"\\nSample {i}:\")\n",
        "            print(f\"RGB file: {os.path.basename(rgb_path)}\")\n",
        "            print(f\"RGB shape: {features1.shape}\")\n",
        "            print(f\"Audio file: {os.path.basename(audio_path)}\")\n",
        "            print(f\"Audio shape: {features2.shape}\")\n",
        "\n",
        "            # Try the concatenation\n",
        "            try:\n",
        "                if features1.shape[0] != features2.shape[0]:\n",
        "                    print(\"⚠️ Dimension mismatch!\")\n",
        "                    if features1.shape[0] - 1 == features2.shape[0]:\n",
        "                        print(\"Would work with [:-1] slice\")\n",
        "                    features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "                    print(\"Concatenation successful after adjustment\")\n",
        "            except ValueError as e:\n",
        "                print(f\"❌ Concatenation failed: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading sample {i}: {str(e)}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Run the inspection\n",
        "inspect_batch_files(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWZIdanUgWCK",
        "outputId": "9d999110-e468-4630-8098-5a3bf0e3e7dd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting individual samples:\n",
            "\n",
            "Sample 0:\n",
            "RGB file: v=x-Cgkgj5KR0__#00-15-00_00-18-00_label_A__0.npy\n",
            "RGB shape: (270, 1024)\n",
            "Audio file: v=x-Cgkgj5KR0__#00-15-00_00-18-00_label_A__vggish.npy\n",
            "Audio shape: (270, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 1:\n",
            "RGB file: v=x-Cgkgj5KR0__#00-15-00_00-18-00_label_A__1.npy\n",
            "RGB shape: (270, 1024)\n",
            "Audio file: v=x-Cgkgj5KR0__#00-15-00_00-18-00_label_A__vggish.npy\n",
            "Audio shape: (270, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 2:\n",
            "RGB file: v=x-Cgkgj5KR0__#00-15-00_00-18-00_label_A__2.npy\n",
            "RGB shape: (270, 1024)\n",
            "Audio file: v=x-Cgkgj5KR0__#00-15-00_00-18-00_label_A__vggish.npy\n",
            "Audio shape: (270, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 3:\n",
            "RGB file: v=x-Cgkgj5KR0__#00-15-00_00-18-00_label_A__3.npy\n",
            "RGB shape: (270, 1024)\n",
            "Audio file: v=x-Cgkgj5KR0__#00-15-00_00-18-00_label_A__vggish.npy\n",
            "Audio shape: (270, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 4:\n",
            "RGB file: v=x-Cgkgj5KR0__#00-15-00_00-18-00_label_A__4.npy\n",
            "RGB shape: (270, 1024)\n",
            "Audio file: v=x-Cgkgj5KR0__#00-15-00_00-18-00_label_A__vggish.npy\n",
            "Audio shape: (270, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 5:\n",
            "RGB file: Jason.Bourne.2016__#0-16-14_0-16-43_label_B4-0-0__0.npy\n",
            "RGB shape: (43, 1024)\n",
            "Audio file: Jason.Bourne.2016__#0-16-14_0-16-43_label_B4-0-0__vggish.npy\n",
            "Audio shape: (43, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 6:\n",
            "RGB file: Jason.Bourne.2016__#0-16-14_0-16-43_label_B4-0-0__1.npy\n",
            "RGB shape: (43, 1024)\n",
            "Audio file: Jason.Bourne.2016__#0-16-14_0-16-43_label_B4-0-0__vggish.npy\n",
            "Audio shape: (43, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 7:\n",
            "RGB file: Jason.Bourne.2016__#0-16-14_0-16-43_label_B4-0-0__2.npy\n",
            "RGB shape: (43, 1024)\n",
            "Audio file: Jason.Bourne.2016__#0-16-14_0-16-43_label_B4-0-0__vggish.npy\n",
            "Audio shape: (43, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 8:\n",
            "RGB file: Jason.Bourne.2016__#0-16-14_0-16-43_label_B4-0-0__3.npy\n",
            "RGB shape: (43, 1024)\n",
            "Audio file: Jason.Bourne.2016__#0-16-14_0-16-43_label_B4-0-0__vggish.npy\n",
            "Audio shape: (43, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 9:\n",
            "RGB file: Jason.Bourne.2016__#0-16-14_0-16-43_label_B4-0-0__4.npy\n",
            "RGB shape: (43, 1024)\n",
            "Audio file: Jason.Bourne.2016__#0-16-14_0-16-43_label_B4-0-0__vggish.npy\n",
            "Audio shape: (43, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 10:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__0.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 11:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__1.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 12:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__2.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 13:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__3.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 14:\n",
            "RGB file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__4.npy\n",
            "RGB shape: (103, 1024)\n",
            "Audio file: Kill.Bill.Vol.1.2003__#01-05-33_01-06-42_label_A__vggish.npy\n",
            "Audio shape: (103, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 15:\n",
            "RGB file: v=cvSRy1NOXc8__#1_label_A__0.npy\n",
            "RGB shape: (195, 1024)\n",
            "Audio file: v=cvSRy1NOXc8__#1_label_A__vggish.npy\n",
            "Audio shape: (195, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 16:\n",
            "RGB file: v=cvSRy1NOXc8__#1_label_A__1.npy\n",
            "RGB shape: (195, 1024)\n",
            "Audio file: v=cvSRy1NOXc8__#1_label_A__vggish.npy\n",
            "Audio shape: (195, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 17:\n",
            "RGB file: v=cvSRy1NOXc8__#1_label_A__2.npy\n",
            "RGB shape: (195, 1024)\n",
            "Audio file: v=cvSRy1NOXc8__#1_label_A__vggish.npy\n",
            "Audio shape: (195, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 18:\n",
            "RGB file: v=cvSRy1NOXc8__#1_label_A__3.npy\n",
            "RGB shape: (195, 1024)\n",
            "Audio file: v=cvSRy1NOXc8__#1_label_A__vggish.npy\n",
            "Audio shape: (195, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 19:\n",
            "RGB file: v=cvSRy1NOXc8__#1_label_A__4.npy\n",
            "RGB shape: (195, 1024)\n",
            "Audio file: v=cvSRy1NOXc8__#1_label_A__vggish.npy\n",
            "Audio shape: (195, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 20:\n",
            "RGB file: Brick.Mansions.2014__#00-37-48_00-38-48_label_B1-0-0__0.npy\n",
            "RGB shape: (89, 1024)\n",
            "Audio file: Brick.Mansions.2014__#00-37-48_00-38-48_label_B1-0-0__vggish.npy\n",
            "Audio shape: (89, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 21:\n",
            "RGB file: Brick.Mansions.2014__#00-37-48_00-38-48_label_B1-0-0__1.npy\n",
            "RGB shape: (89, 1024)\n",
            "Audio file: Brick.Mansions.2014__#00-37-48_00-38-48_label_B1-0-0__vggish.npy\n",
            "Audio shape: (89, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 22:\n",
            "RGB file: Brick.Mansions.2014__#00-37-48_00-38-48_label_B1-0-0__2.npy\n",
            "RGB shape: (89, 1024)\n",
            "Audio file: Brick.Mansions.2014__#00-37-48_00-38-48_label_B1-0-0__vggish.npy\n",
            "Audio shape: (89, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 23:\n",
            "RGB file: Brick.Mansions.2014__#00-37-48_00-38-48_label_B1-0-0__3.npy\n",
            "RGB shape: (89, 1024)\n",
            "Audio file: Brick.Mansions.2014__#00-37-48_00-38-48_label_B1-0-0__vggish.npy\n",
            "Audio shape: (89, 128)\n",
            "--------------------------------------------------\n",
            "\n",
            "Sample 24:\n",
            "RGB file: Brick.Mansions.2014__#00-37-48_00-38-48_label_B1-0-0__4.npy\n",
            "RGB shape: (89, 1024)\n",
            "Audio file: Brick.Mansions.2014__#00-37-48_00-38-48_label_B1-0-0__vggish.npy\n",
            "Audio shape: (89, 128)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NCyNXLPN5u0"
      },
      "source": [
        "## Testing PreTrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkT8DU7V3HOz",
        "outputId": "0daa5404-6268-4f41-88a6-74a076ea2c11",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-76-6958b6b31f24>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  {k.replace('module.', ''): v for k, v in torch.load('/content/final_dl/wsanodet_mix2.pkl').items()})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time:32.01085138320923\n",
            "offline pr_auc:0.79; online pr_auc:0.7433\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "# from model import Model\n",
        "# from dataset import Dataset\n",
        "# from test import test\n",
        "# import option\n",
        "import time\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  test_loader = DataLoader(Dataset(args, mode='test'),\n",
        "                            batch_size=5, shuffle=False,\n",
        "                            num_workers=args.workers, pin_memory=True)\n",
        "  model = Model(args)\n",
        "  model = model.to(device)\n",
        "  # had to change path to \"/content/final_dl/wsanodet_mix2.pkl\"\n",
        "  model_dict = model.load_state_dict(\n",
        "      {k.replace('module.', ''): v for k, v in torch.load('/content/final_dl/wsanodet_mix2.pkl').items()})\n",
        "\n",
        "  gt = np.load(args.gt)\n",
        "  st = time.time()\n",
        "  pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "  print('Time:{}'.format(time.time()-st))\n",
        "  print('offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIbLkkivMhx"
      },
      "source": [
        "how to save a model for the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7mSdUwSvLTq"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"/content/test.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0S5vwXjQzID"
      },
      "source": [
        "# Training HLNET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv0i1DYkQ2Qj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def CLAS(logits, label, seq_len, criterion, device, is_topk=True):\n",
        "    logits = logits.squeeze()\n",
        "    instance_logits = torch.zeros(0).to(device)  # tensor([])\n",
        "    for i in range(logits.shape[0]):\n",
        "        if is_topk:\n",
        "            tmp, _ = torch.topk(logits[i][:seq_len[i]], k=int(seq_len[i]//16+1), largest=True)\n",
        "            tmp = torch.mean(tmp).view(1)\n",
        "        else:\n",
        "            tmp = torch.mean(logits[i, :seq_len[i]]).view(1)\n",
        "        instance_logits = torch.cat((instance_logits, tmp))\n",
        "\n",
        "    instance_logits = torch.sigmoid(instance_logits)\n",
        "\n",
        "    clsloss = criterion(instance_logits, label)\n",
        "    return clsloss\n",
        "\n",
        "\n",
        "def CENTROPY(logits, logits2, seq_len, device):\n",
        "    instance_logits = torch.tensor(0).to(device)  # tensor([])\n",
        "    for i in range(logits.shape[0]):\n",
        "        tmp1 = torch.sigmoid(logits[i, :seq_len[i]]).squeeze()\n",
        "        tmp2 = torch.sigmoid(logits2[i, :seq_len[i]]).squeeze()\n",
        "        loss = torch.mean(-tmp1.detach() * torch.log(tmp2))\n",
        "        instance_logits = instance_logits + loss\n",
        "    instance_logits = instance_logits/logits.shape[0]\n",
        "    return instance_logits\n",
        "\n",
        "\n",
        "def train(dataloader, model, optimizer, criterion, device, is_topk):\n",
        "    with torch.set_grad_enabled(True):\n",
        "        model.train()\n",
        "        for i, (input, label) in enumerate(dataloader):\n",
        "            seq_len = torch.sum(torch.max(torch.abs(input), dim=2)[0]>0, 1)\n",
        "            input = input[:, :torch.max(seq_len), :]\n",
        "            input, label = input.float().to(device), label.float().to(device)\n",
        "            logits, logits2 = model(input, seq_len)\n",
        "            clsloss = CLAS(logits, label, seq_len, criterion, device, is_topk)\n",
        "            clsloss2 = CLAS(logits2, label, seq_len, criterion, device, is_topk)\n",
        "            croloss = CENTROPY(logits, logits2, seq_len, device)\n",
        "\n",
        "            total_loss = clsloss + clsloss2 + 5*croloss\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcLzmEr4HIQY",
        "outputId": "427e5241-2699-4e42-eee2-84abc2c81d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/audio.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/audio.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/list/xx/train\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/audio.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mubL6MwpIS8k",
        "outputId": "3b39b7c2-9487-45fe-b7ba-35f55dce2d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/rgb.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/rgb.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/rgb.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYz5pRU7Iyf8"
      },
      "source": [
        "## test (ignore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTEZP14qIyOD",
        "outputId": "3067bd70-3ce8-4c53-a00a-78f5f39585d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 200, 1152])\n",
            "torch.Size([5, 200, 1152])\n",
            "torch.Size([5, 200, 1152])\n"
          ]
        }
      ],
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "      self.modality = 'MIX2'\n",
        "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "      self.flow_list = '/content/final_dl/list/flow.list'\n",
        "      self.audio_list = '/content/final_dl/list/audio.list'\n",
        "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "      self.gt = '/content/final_dl/list/gt.npy'\n",
        "      self.gpus = 1\n",
        "      self.lr = 0.0001\n",
        "      self.batch_size = 128\n",
        "      self.workers = 1\n",
        "      self.model_name = 'wsanodet'\n",
        "      self.pretrained_ckpt = None\n",
        "      self.feature_size = 1152  # 1024 + 128\n",
        "      self.num_classes = 1\n",
        "      self.dataset_name = 'XD-Violence'\n",
        "      self.max_seqlen = 200\n",
        "      self.max_epoch = 50\n",
        "\n",
        "  # Create an instance of the Args class\n",
        "args = Args()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "test_loader = DataLoader(Dataset(args, mode='test'),\n",
        "                          batch_size=5, shuffle=True,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (input,label) in enumerate(test_loader):\n",
        "    input = input.to(device)\n",
        "\n",
        "    print(input.shape)\n",
        "    ############\n",
        "    ### NOTE: ## setting seq_len to None pads training data in the sequence dim to 200\n",
        "    ############\n",
        "    logits, logits2 = model(inputs=input, seq_len=None)\n",
        "    # print(logits, logits2)\n",
        "    if i == 2:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training HL NET"
      ],
      "metadata": {
        "id": "SM88zu_-ADPw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pFJMkujfFROw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb100452-678f-482c-c4c8-e63f28f752a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1d1.weight\n",
            "conv1d1.bias\n",
            "conv1d2.weight\n",
            "conv1d2.bias\n",
            "conv1d3.weight\n",
            "conv1d3.bias\n",
            "conv1d4.weight\n",
            "conv1d4.bias\n",
            "gc1.weight\n",
            "gc1.residual.weight\n",
            "gc1.residual.bias\n",
            "gc2.weight\n",
            "gc3.weight\n",
            "gc3.residual.weight\n",
            "gc3.residual.bias\n",
            "gc4.weight\n",
            "gc5.weight\n",
            "gc5.residual.weight\n",
            "gc5.residual.bias\n",
            "gc6.weight\n",
            "simAdj.weight0\n",
            "simAdj.weight1\n",
            "disAdj.sigma\n",
            "classifier.weight\n",
            "classifier.bias\n",
            "approximator.0.weight\n",
            "approximator.0.bias\n",
            "approximator.2.weight\n",
            "approximator.2.bias\n",
            "conv1d_approximator.weight\n",
            "conv1d_approximator.bias\n",
            "Random initalization: offline pr_auc:0.1815; online pr_auc:0.2136\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-17ec0503ccf9>\", line 85, in __getitem__\n    features = np.concatenate((features1[:-1], features2), axis=1)\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 35 and the array at index 1 has size 33\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-6baa33c54768>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_topk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./ckpt/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-92c6eaf947f4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, optimizer, criterion, device, is_topk)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-17ec0503ccf9>\", line 85, in __getitem__\n    features = np.concatenate((features1[:-1], features2), axis=1)\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 35 and the array at index 1 has size 33\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "# import option\n",
        "\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# torch.multiprocessing.set_start_method('spawn')\n",
        "# setup_seed(2333)\n",
        "# args = option.parser.parse_args()\n",
        "\n",
        "!export TORCH_USE_CUDA_DSA=ON\n",
        "device = torch.device(\"cuda\")\n",
        "train_loader = DataLoader(Dataset(args, mode='train'),\n",
        "                          batch_size=args.batch_size, shuffle=True,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "test_loader = DataLoader(Dataset(args, mode='test'),\n",
        "                          batch_size=5, shuffle=False,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "for name, value in model.named_parameters():\n",
        "    print(name)\n",
        "approximator_param = list(map(id, model.approximator.parameters()))\n",
        "approximator_param += list(map(id, model.conv1d_approximator.parameters()))\n",
        "base_param = filter(lambda p: id(p) not in approximator_param, model.parameters())\n",
        "\n",
        "if not os.path.exists('./ckpt'):\n",
        "    os.makedirs('./ckpt')\n",
        "optimizer = optim.Adam([{'params': base_param},\n",
        "                        {'params': model.approximator.parameters(), 'lr': args.lr / 2},\n",
        "                        {'params': model.conv1d_approximator.parameters(), 'lr': args.lr / 2},\n",
        "                        ],\n",
        "                        lr=args.lr, weight_decay=0.000)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "is_topk = True\n",
        "gt = np.load(args.gt)\n",
        "pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "print('Random initalization: offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n",
        "for epoch in range(args.max_epoch):\n",
        "    scheduler.step()\n",
        "    st = time.time()\n",
        "    train(train_loader, model, optimizer, criterion, device, is_topk)\n",
        "    if epoch % 2 == 0 and not epoch == 0:\n",
        "        torch.save(model.state_dict(), './ckpt/'+args.model_name+'{}.pkl'.format(epoch))\n",
        "\n",
        "    pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "    print('Epoch {0}/{1}: offline pr_auc:{2:.4}; online pr_auc:{3:.4}\\n'.format(epoch, args.max_epoch, pr_auc, pr_auc_online))\n",
        "torch.save(model.state_dict(), './ckpt/' + args.model_name + '.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYjtaIacOHyS"
      },
      "source": [
        "# Training VAE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE MODEL"
      ],
      "metadata": {
        "id": "FRzWrGepsWur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R8Ma-ndq_5GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Sampling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, z_means, z_log_vars):\n",
        "        epsilon = torch.randn_like(z_means, dtype=torch.float32)\n",
        "        return z_means + torch.exp(0.5 * z_log_vars) * epsilon\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1024, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, 512, kernel_size=3, stride=2, padding=1),  # (B, 1024, 200) -> (B, 512, 100)\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(512, 256, kernel_size=3, stride=2, padding=1),         # (B, 512, 100) -> (B, 256, 50)\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(256, 128, kernel_size=3, stride=2, padding=1),         # (B, 256, 50) -> (B, 128, 25)\n",
        "            nn.ReLU(True),\n",
        "            nn.Flatten()  # Flatten for fully connected layers\n",
        "        )\n",
        "\n",
        "        flattened_dim = 25 * 128  # Calculate flattened dimension\n",
        "        self.lin_mean = nn.Linear(flattened_dim, latent_dim)\n",
        "        self.lin_log_var = nn.Linear(flattened_dim, latent_dim)\n",
        "        self.sampling = Sampling()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.encoder(x)\n",
        "        z_means = self.lin_mean(x)\n",
        "        z_log_vars = self.lin_log_var(x)\n",
        "        z = self.sampling(z_means, z_log_vars)\n",
        "        return z, z_means, z_log_vars\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1024, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        flattened_dim = 25 * 128  # Must match Encoder's flattened_dim (200)\n",
        "\n",
        "        self.decoder_fc = nn.Sequential(\n",
        "            nn.Linear(latent_dim, flattened_dim),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose1d(128, 256, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 128, 25) -> (B, 256, 50)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 256, 50) -> (B, 512, 100)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(512, input_dim, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 512, 100) -> (B, 1024, 200)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.decoder_fc(x)  # Fully connected layer\n",
        "        x = x.view(-1, 128, 25)  # Reshape to match ConvTranspose1D input\n",
        "        x = self.decoder_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1024, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_dim, input_dim, seq_len)\n",
        "        self.decoder = Decoder(latent_dim, input_dim, seq_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, z_means, z_log_vars = self.encoder(x)\n",
        "        x_reconstructed = self.decoder(z)\n",
        "        x_reconstructed = x_reconstructed.view(-1, 200, 1024)\n",
        "        return x_reconstructed, z_means, z_log_vars"
      ],
      "metadata": {
        "collapsed": true,
        "id": "r7Hlyuuos9DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "      self.modality = 'RGB'\n",
        "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "      self.flow_list = '/content/final_dl/list/flow.list'\n",
        "      self.audio_list = '/content/final_dl/list/audio.list'\n",
        "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "      self.gt = '/content/final_dl/list/gt.npy'\n",
        "      self.gpus = 1\n",
        "      self.lr = 0.0001\n",
        "      self.batch_size = 1\n",
        "      self.workers = 1\n",
        "      self.model_name = 'wsanodet'\n",
        "      self.pretrained_ckpt = None\n",
        "      # self.feature_size = 1152  # 1024 + 128\n",
        "      self.feature_size = 1024\n",
        "      self.num_classes = 1\n",
        "      self.dataset_name = 'XD-Violence'\n",
        "      self.max_seqlen = 200\n",
        "      self.max_epoch = 50\n",
        "\n",
        "  # Create an instance of the Args class\n",
        "args = Args()\n",
        "\n"
      ],
      "metadata": {
        "id": "T9Bi_42c8Tts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE Train FN"
      ],
      "metadata": {
        "id": "DQrX4xU68twm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#train_loader = DataLoader(Dataset(args, mode='train'),\n",
        "#                          batch_size=args.batch_size, shuffle=True,\n",
        "#                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "with torch.set_grad_enabled(True):\n",
        "  model.train()\n",
        "  for i, (input, label) in enumerate(train_loader):\n",
        "\n",
        "    input, label = input.float().to(torch.float32), label.float().to(device)\n",
        "\n",
        "    # input and label from train dataset\n",
        "    print(\"shape of input: \", input.shape)\n",
        "    print(\"shape of label: \", label.shape)\n",
        "\n",
        "    input = input.to(device)\n",
        "\n",
        "    # size of one sample from batch\n",
        "    test_input = input[:1][::][::]\n",
        "\n",
        "    # permuting input shape to fit vae cnn layers\n",
        "    # test_input = test_input.permute(0, 2, 1)\n",
        "    # print(\"test_input shape: \", test_input.shape)\n",
        "\n",
        "    # passing through vae\n",
        "    vae = VAE(latent_dim=128, input_dim=1024, seq_len=200)\n",
        "    vae = vae.to(device)\n",
        "\n",
        "    x_reconstructed, z_means, z_log_vars = vae(test_input)\n",
        "\n",
        "    print(\"reconstructed input shape (after vae): \", x_reconstructed.shape)\n",
        "\n",
        "    ## break statement just to read out this input/output shapes in the first iteration\n",
        "    if i == 0:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "Ot6yc6GL8eTi",
        "outputId": "f089a45e-8f49-4c8b-ce8b-cd7311754928",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-17ec0503ccf9>\", line 85, in __getitem__\n    features = np.concatenate((features1[:-1], features2), axis=1)\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 212 and the array at index 1 has size 565\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-540772ae1d6e>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-17ec0503ccf9>\", line 85, in __getitem__\n    features = np.concatenate((features1[:-1], features2), axis=1)\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 212 and the array at index 1 has size 565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (input, label) in enumerate(train_loader):\n",
        "    print(input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "OPc5UhYtKwo5",
        "outputId": "303561cf-2ee8-42d0-fb47-07a32525247a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-17ec0503ccf9>\", line 85, in __getitem__\n    features = np.concatenate((features1[:-1], features2), axis=1)\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 32 and the array at index 1 has size 144\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-5cc58b98fead>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-17ec0503ccf9>\", line 85, in __getitem__\n    features = np.concatenate((features1[:-1], features2), axis=1)\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 32 and the array at index 1 has size 144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE Training func (Gus)\n"
      ],
      "metadata": {
        "id": "dUfjc_eorOqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def vae_train_loop(vae, train_loader, optimizer, device, num_epochs=50):\n",
        "    vae.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        recon_loss = 0\n",
        "        kl_loss = 0\n",
        "\n",
        "        for batch_idx, (data, _) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            recon_data, mu, logvar = vae(data)\n",
        "            recon_criterion = nn.MSELoss(reduction='sum')\n",
        "            r_loss = recon_criterion(recon_data, data)\n",
        "            kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "            loss = r_loss + kl\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            recon_loss += r_loss.item()\n",
        "            kl_loss += kl.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader.dataset)\n",
        "        avg_recon = recon_loss / len(train_loader.dataset)\n",
        "        avg_kl = kl_loss / len(train_loader.dataset)\n",
        "\n",
        "        print(f'Epoch {epoch}: Loss = {avg_loss:.4f}, Recon = {avg_recon:.4f}, KL = {avg_kl:.4f}')\n",
        "\n",
        "def extract_vae_features(vae, data_loader, device):\n",
        "    \"\"\"\n",
        "    Extract latent features using trained VAE\n",
        "    \"\"\"\n",
        "    vae.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, label in data_loader:\n",
        "            data = data.to(device)\n",
        "            # Get latent representation\n",
        "            z, _, _ = vae.encoder(data)\n",
        "            features.append(z.cpu())\n",
        "            labels.append(label)\n",
        "\n",
        "    return torch.cat(features), torch.cat(labels)\n",
        "\n",
        "class FeatureDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for VAE extracted features that maintains original binary labels\"\"\"\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features  # VAE latent features\n",
        "        self.labels = labels      # Original binary labels (0.0 or 1.0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        label = torch.tensor(float(self.labels[idx]))\n",
        "        return feature, label"
      ],
      "metadata": {
        "id": "ytqsQLyYrLyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrated Training pipeline"
      ],
      "metadata": {
        "id": "Wuy3YBP5rT11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_pipeline(vae, hlnet, train_loader, args, device):\n",
        "    \"\"\"\n",
        "    Integrate the hlnet and vae trainings to form a pipeline\n",
        "    the vae is trained first, and then features are extracted and used to create a new dataset to train the hlnet\n",
        "    \"\"\"\n",
        "\n",
        "    # Train VAE\n",
        "    vae_optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    print(\"Training VAE...\")\n",
        "    vae_train_loop(vae, train_loader, vae_optimizer, device)\n",
        "\n",
        "    # Extract features using trained VAE\n",
        "    print(\"Extracting VAE features...\")\n",
        "    features, labels = extract_vae_features(vae, train_loader, device)\n",
        "\n",
        "    # Create new dataset with VAE features\n",
        "    feature_dataset = FeatureDataset(features, labels)\n",
        "    feature_loader = DataLoader(\n",
        "        feature_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.workers\n",
        "    )\n",
        "\n",
        "    # Train HL-Net\n",
        "    print(\"Training HL-Net...\")\n",
        "    hlnet_optimizer = optim.Adam([\n",
        "        {'params': hlnet.parameters()},\n",
        "    ], lr=args.lr)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(args.max_epoch):\n",
        "        train(feature_loader, hlnet, hlnet_optimizer, criterion, device, True)\n",
        "\n",
        "        # Validation could be added here\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f\"Completed epoch {epoch}\")\n",
        "\n",
        "    return vae, hlnet\n"
      ],
      "metadata": {
        "id": "XRpOFCxHiSDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## example of run"
      ],
      "metadata": {
        "id": "90LBdhigrWxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VAE(latent_dim=128, input_dim=1024, seq_len=200).to(device)\n",
        "hlnet = Model(args).to(device)\n",
        "trained_vae, trained_hlnet = train_pipeline(vae, hlnet, train_loader, args, device)"
      ],
      "metadata": {
        "id": "WmY3o7ChkbKQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "f12582e9-9ae4-4e8a-9ea7-0990cbf5b9f2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training VAE...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-17ec0503ccf9>\", line 85, in __getitem__\n    features = np.concatenate((features1[:-1], features2), axis=1)\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 242 and the array at index 1 has size 124\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-ed662fa7d817>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhlnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrained_vae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_hlnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhlnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-5607d5f73245>\u001b[0m in \u001b[0;36mtrain_pipeline\u001b[0;34m(vae, hlnet, train_loader, args, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvae_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training VAE...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvae_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Extract features using trained VAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-d6ee729406ef>\u001b[0m in \u001b[0;36mvae_train_loop\u001b[0;34m(vae, train_loader, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-17ec0503ccf9>\", line 85, in __getitem__\n    features = np.concatenate((features1[:-1], features2), axis=1)\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 242 and the array at index 1 has size 124\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}