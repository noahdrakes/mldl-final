{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahdrakes/mldl-final/blob/main/mm_violence_det_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDOr6KirJvUe"
      },
      "source": [
        "# Multi-Modal Violence Detection Network\n",
        "\n",
        "original src code: https://github.com/Roc-Ng/XDVioDet.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhzPvDXTKKqM"
      },
      "source": [
        "### Copying Training and Testing Data\n",
        "\n",
        "The folders are pretty large (~40/50GB) so it takes a while to copy all of the data over.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4DG4K3wYK8P",
        "outputId": "51d5c5b4-0d63-4193-e1f5-a934320c5fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /mydrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/mydrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "88GWL7hscvcg",
        "outputId": "2dab1aff-3e09-46ad-80cc-02975dac0673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/mydrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /mydrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nr7SpsS-2P6x"
      },
      "outputs": [],
      "source": [
        "!unzip final_dl.zip -d /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "f2GCHGksd0mQ"
      },
      "outputs": [],
      "source": [
        "# !zip -r final_dl.zip final_dl/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zWKH9vsRatb"
      },
      "source": [
        "may need to change directory depending on where you upload the data to google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "CHQ29LtvAsNj"
      },
      "outputs": [],
      "source": [
        "# !cp -r /mydrive/MyDrive/final_dl ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N1LDIKfKhpP"
      },
      "source": [
        "## 1. Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9acUBEhKqjC"
      },
      "source": [
        "### A) Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "collapsed": true,
        "id": "IPMWSbm0LBGY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc, precision_recall_curve\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def test(dataloader, model, device, gt):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pred = torch.zeros(0).to(device)\n",
        "        pred2 = torch.zeros(0).to(device)\n",
        "        for i, input in enumerate(dataloader):\n",
        "            input = input.to(device)\n",
        "            logits, logits2 = model(inputs=input, seq_len=None)\n",
        "            logits = torch.squeeze(logits)\n",
        "            sig = torch.sigmoid(logits)\n",
        "            sig = torch.mean(sig, 0)\n",
        "            pred = torch.cat((pred, sig))\n",
        "            '''\n",
        "            online detection\n",
        "            '''\n",
        "            logits2 = torch.squeeze(logits2)\n",
        "            sig2 = torch.sigmoid(logits2)\n",
        "            sig2 = torch.mean(sig2, 0)\n",
        "\n",
        "            sig2 = torch.unsqueeze(sig2, 1) ##for audio\n",
        "            pred2 = torch.cat((pred2, sig2))\n",
        "\n",
        "            # print(\"pred:, \", pred)\n",
        "            # print(\"pred2:, \", pred2)\n",
        "\n",
        "        pred = list(pred.cpu().detach().numpy())\n",
        "        pred2 = list(pred2.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred, 16))\n",
        "        pr_auc = auc(recall, precision)\n",
        "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred2, 16))\n",
        "        pr_auc2 = auc(recall, precision)\n",
        "        return pr_auc, pr_auc2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLV269m9K7L3"
      },
      "source": [
        "### B) Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "ej2MARCmKzvk"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def random_extract(feat, t_max):\n",
        "   r = np.random.randint(len(feat)-t_max)\n",
        "   return feat[r:r+t_max]\n",
        "\n",
        "def uniform_extract(feat, t_max):\n",
        "   r = np.linspace(0, len(feat)-1, t_max, dtype=np.uint16)\n",
        "   return feat[r, :]\n",
        "\n",
        "def pad(feat, min_len):\n",
        "    if np.shape(feat)[0] <= min_len:\n",
        "       return np.pad(feat, ((0, min_len-np.shape(feat)[0]), (0, 0)), mode='constant', constant_values=0)\n",
        "    else:\n",
        "       return feat\n",
        "\n",
        "def process_feat(feat, length, is_random=True):\n",
        "    if len(feat) > length:\n",
        "        if is_random:\n",
        "            return random_extract(feat, length)\n",
        "        else:\n",
        "            return uniform_extract(feat, length)\n",
        "    else:\n",
        "        return pad(feat, length)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKIYnHpLDKS"
      },
      "source": [
        "### C) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "98z1xwOKKueM"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "\n",
        "# from utils import process_feat\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, args, transform=None, test_mode=False):\n",
        "        self.modality = args.modality\n",
        "\n",
        "        if test_mode:\n",
        "            self.rgb_list_file = args.test_rgb_list\n",
        "            self.flow_list_file = args.test_flow_list\n",
        "            self.audio_list_file = args.test_audio_list\n",
        "        else:\n",
        "            self.rgb_list_file = args.rgb_list\n",
        "            self.flow_list_file = args.flow_list\n",
        "            self.audio_list_file = args.audio_list\n",
        "        self.max_seqlen = args.max_seqlen\n",
        "        self.tranform = transform\n",
        "        self.test_mode = test_mode\n",
        "        self.normal_flag = '_label_A'\n",
        "        self._parse_list()\n",
        "\n",
        "    def _parse_list(self):\n",
        "        if self.modality == 'AUDIO':\n",
        "            self.list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'RGB':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            print(\"here\")\n",
        "            # print(self.list)\n",
        "        elif self.modality == 'FLOW':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX2':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX3':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        else:\n",
        "            assert 1 > 2, 'Modality is wrong!'\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.normal_flag in self.list[index]:\n",
        "            label = 0.0\n",
        "        else:\n",
        "            label = 1.0\n",
        "\n",
        "        if self.modality == 'AUDIO':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'RGB':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')),dtype=np.float32)\n",
        "        elif self.modality == 'FLOW':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'MIX':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX2':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX3':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            features3 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2, features3),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2, features3[:-1]), axis=1)\n",
        "        else:\n",
        "            assert 1>2, 'Modality is wrong!'\n",
        "        if self.tranform is not None:\n",
        "            features = self.tranform(features)\n",
        "        if self.test_mode:\n",
        "            return features\n",
        "\n",
        "        else:\n",
        "            features = process_feat(features, self.max_seqlen, is_random=False)\n",
        "            return features, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvvMYO_1LXTV"
      },
      "source": [
        "### D) Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "3bYn0UsFKWTy"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "from torch import FloatTensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(in_features, out_features).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
        "        self.a = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(2*out_features, 1).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        h = torch.mm(input, self.W)\n",
        "        N = h.size()[0]\n",
        "\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
        "\n",
        "class linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(linear, self).__init__()\n",
        "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.register_parameter('bias', None)\n",
        "        stdv = 1. / sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "    def forward(self, x):\n",
        "        x = x.matmul(self.weight)\n",
        "        return x\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=False, residual=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "        elif (in_features == out_features):\n",
        "            self.residual = lambda x: x\n",
        "        else:\n",
        "            # self.residual = linear(in_features, out_features)\n",
        "            self.residual = nn.Conv1d(in_channels=in_features, out_channels=out_features, kernel_size=5, padding=2)\n",
        "    def reset_parameters(self):\n",
        "        # stdv = 1. / sqrt(self.weight.size(1))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # To support batch operations\n",
        "        support = input.matmul(self.weight)\n",
        "        output = adj.matmul(support)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        if self.in_features != self.out_features and self.residual:\n",
        "            input = input.permute(0,2,1)\n",
        "            res = self.residual(input)\n",
        "            res = res.permute(0,2,1)\n",
        "            output = output + res\n",
        "        else:\n",
        "            output = output + self.residual(input)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "######################################################\n",
        "\n",
        "class SimilarityAdj(Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(SimilarityAdj, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight0 = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.weight1 = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # stdv = 1. / sqrt(self.weight0.size(1))\n",
        "        nn.init.xavier_uniform_(self.weight0)\n",
        "        nn.init.xavier_uniform_(self.weight1)\n",
        "\n",
        "    def forward(self, input, seq_len):\n",
        "        # To support batch operations\n",
        "        soft = nn.Softmax(1)\n",
        "        theta = torch.matmul(input, self.weight0)\n",
        "        phi = torch.matmul(input, self.weight0)\n",
        "        phi2 = phi.permute(0, 2, 1)\n",
        "        sim_graph = torch.matmul(theta, phi2)\n",
        "\n",
        "        theta_norm = torch.norm(theta, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        phi_norm = torch.norm(phi, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        x_norm_x = theta_norm.matmul(phi_norm.permute(0, 2, 1))\n",
        "        sim_graph = sim_graph / (x_norm_x + 1e-20)\n",
        "\n",
        "        output = torch.zeros_like(sim_graph)\n",
        "        if seq_len is None:\n",
        "            for i in range(sim_graph.shape[0]):\n",
        "                tmp = sim_graph[i]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = sim_graph[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "class DistanceAdj(Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DistanceAdj, self).__init__()\n",
        "        self.sigma = Parameter(FloatTensor(1))\n",
        "        self.sigma.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, batch_size, max_seqlen):\n",
        "        # To support batch operations\n",
        "        self.arith = np.arange(max_seqlen).reshape(-1, 1)\n",
        "        dist = pdist(self.arith, metric='cityblock').astype(np.float32)\n",
        "        self.dist = torch.from_numpy(squareform(dist)).to('cuda')\n",
        "        self.dist = torch.exp(-self.dist / torch.exp(torch.tensor(1.)))\n",
        "        self.dist = torch.unsqueeze(self.dist, 0).repeat(batch_size, 1, 1).to('cuda')\n",
        "        return self.dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44eeeOUcLquB"
      },
      "source": [
        "### E) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "q8Rjvb-yKOeZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as torch_init\n",
        "import os\n",
        "# from layers import GraphConvolution, SimilarityAdj, DistanceAdj\n",
        "\n",
        "\n",
        "def weight_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
        "        torch_init.xavier_uniform_(m.weight)\n",
        "        # m.bias.data.fill_(0.1)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        n_features = args.feature_size\n",
        "        n_class = args.num_classes\n",
        "\n",
        "        self.conv1d1 = nn.Conv1d(in_channels=n_features, out_channels=512, kernel_size=1, padding=0)\n",
        "        self.conv1d2 = nn.Conv1d(in_channels=512, out_channels=128, kernel_size=1, padding=0)\n",
        "        self.conv1d3 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=5, padding=2)\n",
        "        self.conv1d4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
        "        # Graph Convolution\n",
        "        self.gc1 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc2 = GraphConvolution(32, 32, residual=True)\n",
        "        self.gc3 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc4 = GraphConvolution(32, 32, residual=True)\n",
        "        self.gc5 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc6 = GraphConvolution(32, 32, residual=True)\n",
        "        self.simAdj = SimilarityAdj(n_features, 32)\n",
        "        self.disAdj = DistanceAdj()\n",
        "\n",
        "        self.classifier = nn.Linear(32*3, n_class)\n",
        "        self.approximator = nn.Sequential(nn.Conv1d(128, 64, 1, padding=0), nn.ReLU(),\n",
        "                                          nn.Conv1d(64, 32, 1, padding=0), nn.ReLU())\n",
        "        self.conv1d_approximator = nn.Conv1d(32, 1, 5, padding=0)\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.apply(weight_init)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, inputs, seq_len):\n",
        "        x = inputs.permute(0, 2, 1)  # for conv1d\n",
        "        x = self.relu(self.conv1d1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.conv1d2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        logits = self.approximator(x)\n",
        "        logits = F.pad(logits, (4, 0))\n",
        "        logits = self.conv1d_approximator(logits)\n",
        "        logits = logits.permute(0, 2, 1)\n",
        "        x = x.permute(0, 2, 1)  # b*t*c\n",
        "\n",
        "        ## gcn\n",
        "        scoadj = self.sadj(logits.detach(), seq_len)\n",
        "        adj = self.adj(inputs, seq_len)\n",
        "        disadj = self.disAdj(x.shape[0], x.shape[1])\n",
        "        x1_h = self.relu(self.gc1(x, adj))\n",
        "        x1_h = self.dropout(x1_h)\n",
        "        x2_h = self.relu(self.gc3(x, disadj))\n",
        "        x2_h = self.dropout(x2_h)\n",
        "        x3_h = self.relu(self.gc5(x, scoadj))\n",
        "        x3_h = self.dropout(x3_h)\n",
        "        x1 = self.relu(self.gc2(x1_h, adj))\n",
        "        x1 = self.dropout(x1)\n",
        "        x2 = self.relu(self.gc4(x2_h, disadj))\n",
        "        x2 = self.dropout(x2)\n",
        "        x3 = self.relu(self.gc6(x3_h, scoadj))\n",
        "        x3 = self.dropout(x3)\n",
        "        x = torch.cat((x1, x2, x3), 2)\n",
        "        x = self.classifier(x)\n",
        "        return x, logits\n",
        "\n",
        "    def sadj(self, logits, seq_len):\n",
        "        lens = logits.shape[1]\n",
        "        soft = nn.Softmax(1)\n",
        "        logits2 = self.sigmoid(logits).repeat(1, 1, lens)\n",
        "        tmp = logits2.permute(0, 2, 1)\n",
        "        adj = 1. - torch.abs(logits2 - tmp)\n",
        "        self.sig = lambda x:1/(1+torch.exp(-((x-0.5))/0.1))\n",
        "        adj = self.sig(adj)\n",
        "        output = torch.zeros_like(adj)\n",
        "        if seq_len is None:\n",
        "            for i in range(logits.shape[0]):\n",
        "                tmp = adj[i]\n",
        "                adj2 = soft(tmp)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = adj[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = soft(tmp)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "        return output\n",
        "\n",
        "\n",
        "    def adj(self, x, seq_len):\n",
        "        soft = nn.Softmax(1)\n",
        "        x2 = x.matmul(x.permute(0,2,1)) # B*T*T\n",
        "        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        x_norm_x = x_norm.matmul(x_norm.permute(0,2,1))\n",
        "        x2 = x2/(x_norm_x+1e-20)\n",
        "        output = torch.zeros_like(x2)\n",
        "        if seq_len is None:\n",
        "            for i in range(x.shape[0]):\n",
        "                tmp = x2[i]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = x2[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE8lvyeyTk0C",
        "outputId": "3067c6fc-1bfb-4224-8e36-a78878761a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /mydrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Current directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm5mBbv_MAtW"
      },
      "source": [
        "### E) Make List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ryqi8mHyNyo9",
        "outputId": "501c2b0b-0464-448d-b96f-cdfc281f3a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/final_dl/list/xx/test/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__vggish.npy', '/content/final_dl/list/xx/test/A.Beautiful.Mind.2001__#00-40-52_00-42-01_label_A__vggish.npy', '/content/final_dl/list/xx/test/A.Beautiful.Mind.2001__#00-50-26_00-55-16_label_A__vggish.npy', '/content/final_dl/list/xx/test/A.Beautiful.Mind.2001__#01-14-30_01-16-59_label_A__vggish.npy', '/content/final_dl/list/xx/test/About.Time.2013__#00-23-50_00-24-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/About.Time.2013__#00-30-50_00-32-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/About.Time.2013__#00-40-52_00-42-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/Bad.Boys.1995__#00-10-00_00-11-52_label_A__vggish.npy', '/content/final_dl/list/xx/test/Bad.Boys.II.2003__#01-11-16_01-14-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/Be.with.You.2018__#00-04-20_00-05-35_label_A__vggish.npy', '/content/final_dl/list/xx/test/Be.with.You.2018__#00-16-18_00-18-09_label_A__vggish.npy', '/content/final_dl/list/xx/test/Be.with.You.2018__#00-23-50_00-24-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/Be.with.You.2018__#01-14-30_01-16-59_label_A__vggish.npy', '/content/final_dl/list/xx/test/Be.with.You.2018__#01-42-06_01-45-53_label_A__vggish.npy', '/content/final_dl/list/xx/test/Before.Sunrise.1995__#00-03-00_00-04-05_label_A__vggish.npy', '/content/final_dl/list/xx/test/Before.Sunrise.1995__#00-04-20_00-05-35_label_A__vggish.npy', '/content/final_dl/list/xx/test/Before.Sunrise.1995__#00-23-50_00-24-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/Before.Sunrise.1995__#00-40-52_00-42-01_label_A__vggish.npy', '/content/final_dl/list/xx/test/Before.Sunrise.1995__#00-42-52_00-45-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/Before.Sunrise.1995__#01-36-45_01-38-10_label_A__vggish.npy', '/content/final_dl/list/xx/test/Before.Sunset.2004__#00-23-50_00-24-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/Before.Sunset.2004__#01-14-30_01-16-51_label_A__vggish.npy', '/content/final_dl/list/xx/test/Black.Hawk.Down.2001__#00-06-44_00-07-39_label_A__vggish.npy', '/content/final_dl/list/xx/test/Black.Hawk.Down.2001__#01-36-44_01-37-33_label_A__vggish.npy', '/content/final_dl/list/xx/test/Brick.Mansions.2014__#00-08-37_00-10-04_label_A__vggish.npy', '/content/final_dl/list/xx/test/Bullet.in.the.Head.1990__#00-03-59_00-04-40_label_A__vggish.npy', '/content/final_dl/list/xx/test/Bullet.in.the.Head.1990__#00-25-11_00-26-15_label_A__vggish.npy', '/content/final_dl/list/xx/test/Bullet.in.the.Head.1990__#00-51-46_00-52-50_label_A__vggish.npy', '/content/final_dl/list/xx/test/Bullet.in.the.Head.1990__#01-58-25_01-59-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/Casino.Royale.2006__#00-28-10_00-30-10_label_A__vggish.npy', '/content/final_dl/list/xx/test/City.Of.Men.2007__#01-36-01_01-37-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/City.of.God.2002__#00-33-10_00-34-50_label_A__vggish.npy', '/content/final_dl/list/xx/test/Deadpool.2.2018__#0-02-35_0-03-12_label_A__vggish.npy', '/content/final_dl/list/xx/test/Deadpool.2016__#0-18-35_0-18-55_label_A__vggish.npy', '/content/final_dl/list/xx/test/Death.Proof.2007__#00-02-04_00-05-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/Desperado.1995__#00-19-00_00-20-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/Desperado.1995__#01-22-10_01-23-17_label_A__vggish.npy', '/content/final_dl/list/xx/test/Election.2005__#00-03-35_00-05-05_label_A__vggish.npy', '/content/final_dl/list/xx/test/Fast.Five.2011__#01-59-16_02-01-12_label_A__vggish.npy', '/content/final_dl/list/xx/test/Fast.Furious.2009__#01-08-33_01-10-18_label_A__vggish.npy', '/content/final_dl/list/xx/test/Fast.Furious.6.2013__#01-37-00_01-38-05_label_A__vggish.npy', '/content/final_dl/list/xx/test/Flipped.2010__#00-08-55_00-10-50_label_A__vggish.npy', '/content/final_dl/list/xx/test/Flipped.2010__#00-42-52_00-45-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/Flipped.2010__#00-45-40_00-47-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/Flipped.2010__#01-07-30_01-09-51_label_A__vggish.npy', '/content/final_dl/list/xx/test/Flipped.2010__#01-10-30_01-13-51_label_A__vggish.npy', '/content/final_dl/list/xx/test/From.Dusk.Till.Dawn.1996__#00-31-52_00-34-21_label_A__vggish.npy', '/content/final_dl/list/xx/test/Fury.2014__#01-13-32_01-14-43_label_A__vggish.npy', '/content/final_dl/list/xx/test/Gladiator.2000__#02-43-00_02-43-59_label_A__vggish.npy', '/content/final_dl/list/xx/test/God.Bless.America.2011__#00-51-00_00-52-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/God.Bless.America.2011__#00-58-09_01-04-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/God.Bless.America.2011__#01-23-13_01-25-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/GoldenEye.1995__#00-56-11_00-56-51_label_A__vggish.npy', '/content/final_dl/list/xx/test/GoldenEye.1995__#01-02-11_01-03-43_label_A__vggish.npy', '/content/final_dl/list/xx/test/GoldenEye.1995__#01-33-40_01-34-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/Good.Will.Hunting.1997__#00-08-55_00-10-50_label_A__vggish.npy', '/content/final_dl/list/xx/test/Good.Will.Hunting.1997__#00-58-18_01-02-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/Haywire.2011__#00-02-06_00-03-29_label_A__vggish.npy', '/content/final_dl/list/xx/test/Hear.Me.2009__#00-30-50_00-32-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/Hear.Me.2009__#00-49-48_00-51-12_label_A__vggish.npy', '/content/final_dl/list/xx/test/Hot.Fuzz.2007__#00-19-20_00-22-06_label_A__vggish.npy', '/content/final_dl/list/xx/test/IP.Man.2.2010__#00-02-50_00-06-45_label_A__vggish.npy', '/content/final_dl/list/xx/test/IP.Man.2.2010__#00-54-00_00-57-12_label_A__vggish.npy', '/content/final_dl/list/xx/test/IP.Man.2.2010__#01-44-00_01-44-42_label_A__vggish.npy', '/content/final_dl/list/xx/test/Ip.Man.2008__#00-02-56_00-05-24_label_A__vggish.npy', '/content/final_dl/list/xx/test/Ip.Man.2008__#00-40-25_00-40-45_label_A__vggish.npy', '/content/final_dl/list/xx/test/Ip.Man.2008__#01-04-27_01-06-22_label_A__vggish.npy', '/content/final_dl/list/xx/test/Jason.Bourne.2016__#0-51-37_0-53-20_label_A__vggish.npy', '/content/final_dl/list/xx/test/Kill.Bill.Vol.1.2003__#00-21-42_00-22-42_label_A__vggish.npy', '/content/final_dl/list/xx/test/Kill.Bill.Vol.1.2003__#01-08-15_01-09-15_label_A__vggish.npy', '/content/final_dl/list/xx/test/Kill.Bill.Vol.2.2004__#01-03-13_01-04-13_label_A__vggish.npy', '/content/final_dl/list/xx/test/Kingsman.The.Golden.Circle.2017__#00-44-52_00-46-20_label_A__vggish.npy', '/content/final_dl/list/xx/test/Kingsman.The.Golden.Circle.2017__#02-09-30_02-10-45_label_A__vggish.npy', '/content/final_dl/list/xx/test/Kingsman.The.Secret.Service.2014__#00-25-29_00-26-24_label_A__vggish.npy', '/content/final_dl/list/xx/test/Law.Abiding.Citizen.2009__#00-00-35_00-01-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/Lord.of.War__#00-59-20_01-00-35_label_A__vggish.npy', '/content/final_dl/list/xx/test/Love.Actually.2003__#00-51-55_00-55-16_label_A__vggish.npy', '/content/final_dl/list/xx/test/Love.Actually.2003__#01-04-40_01-06-59_label_A__vggish.npy', '/content/final_dl/list/xx/test/Love.Actually.2003__#01-17-00_01-19-40_label_A__vggish.npy', '/content/final_dl/list/xx/test/Love.Actually.2003__#01-34-18_01-38-11_label_A__vggish.npy', '/content/final_dl/list/xx/test/Love.Actually.2003__#01-39-05_01-46-40_label_A__vggish.npy', '/content/final_dl/list/xx/test/Love.Death.and.Robots.S01E02__#00-03-15_00-04-20_label_A__vggish.npy', '/content/final_dl/list/xx/test/Love.Death.and.Robots.S01E04__#00-14-28_00-15-27_label_A__vggish.npy', '/content/final_dl/list/xx/test/Love.Me.If.You.Dare.2003__#00-11-20_00-15-08_label_A__vggish.npy', '/content/final_dl/list/xx/test/Love.Me.If.You.Dare.2003__#00-33-52_00-39-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/Love.Me.If.You.Dare.2003__#01-04-40_01-06-59_label_A__vggish.npy', '/content/final_dl/list/xx/test/Mission.Impossible.Ghost.Protocol.2011__#00-49-03_00-51-21_label_A__vggish.npy', '/content/final_dl/list/xx/test/Mission.Impossible.Ghost.Protocol.2011__#01-07-24_01-09-09_label_A__vggish.npy', '/content/final_dl/list/xx/test/Mission.Impossible.Ghost.Protocol.2011__#01-24-59_01-28-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/Mission.Impossible.II.2000__#00-11-37_00-15-46_label_A__vggish.npy', '/content/final_dl/list/xx/test/Mission.Impossible.II.2000__#00-24-30_00-30-10_label_A__vggish.npy', '/content/final_dl/list/xx/test/Mission.Impossible.V.Rogue.Nation.2015__#01-29-39_01-33-39_label_A__vggish.npy', '/content/final_dl/list/xx/test/New.Kids.Turbo.2010__#00-41-30_00-42-05_label_A__vggish.npy', '/content/final_dl/list/xx/test/One.Day.2011__#01-02-50_01-07-05_label_A__vggish.npy', '/content/final_dl/list/xx/test/Operation.Red.Sea.2018__#0-12-26_0-13-36_label_A__vggish.npy', '/content/final_dl/list/xx/test/Operation.Red.Sea.2018__#0-45-54_0-46-33_label_A__vggish.npy', '/content/final_dl/list/xx/test/Quantum.Of.Solace.2008__#00-47-29_00-50-52_label_A__vggish.npy', '/content/final_dl/list/xx/test/Rush.Hour.3.2007.BluRay__#01-00-40_01-04-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/Salt.2010__#00-02-10_00-03-57_label_A__vggish.npy', '/content/final_dl/list/xx/test/Salt.2010__#00-13-45_00-16-10_label_A__vggish.npy', '/content/final_dl/list/xx/test/Salt.2010__#01-16-00_01-16-50_label_A__vggish.npy', '/content/final_dl/list/xx/test/Saving.Private.Ryan.1998__#02-00-56_02-03-06_label_A__vggish.npy', '/content/final_dl/list/xx/test/Shoot.Em.Up.2007__#00-25-50_00-27-10_label_A__vggish.npy', '/content/final_dl/list/xx/test/Shoplifters.2018__#00-17-30_00-19-10_label_A__vggish.npy', '/content/final_dl/list/xx/test/Sin.City.2005__#01-43-21_01-44-21_label_A__vggish.npy', '/content/final_dl/list/xx/test/Skyfall.2012__#02-14-50_02-16-25_label_A__vggish.npy', '/content/final_dl/list/xx/test/Spectre.2015__#00-16-41_00-18-55_label_A__vggish.npy', '/content/final_dl/list/xx/test/Spectre.2015__#01-17-22_01-18-53_label_A__vggish.npy', '/content/final_dl/list/xx/test/Still.Walking.2008__#00-16-18_00-17-27_label_A__vggish.npy', '/content/final_dl/list/xx/test/Still.Walking.2008__#00-51-55_00-55-16_label_A__vggish.npy', '/content/final_dl/list/xx/test/Still.Walking.2008__#01-02-18_01-03-10_label_A__vggish.npy', '/content/final_dl/list/xx/test/Taken.2.UNRATED.EXTENDED.2012__#00-17-08_00-17-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/Taken.3.2014__#00-36-03_00-36-48_label_A__vggish.npy', '/content/final_dl/list/xx/test/Taken.3.2014__#01-20-10_01-21-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Attorney.2013__#00-47-00_00-49-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Bourne.Identity.2002__#0-29-16_0-32-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Bourne.Identity.2002__#0-57-11_0-58-45_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Bourne.Identity.2002__#01-09-16_01-11-16_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Bourne.Supremacy.2004__#0-58-18_01-00-58_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Bourne.Supremacy.2004__#01-34-52_01-35-50_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Bourne.Ultimatum.2007__#0-55-55_0-56-10_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Bourne.Ultimatum.2007__#01-32-57_01-33-53_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Fast.and.the.Furious.2001__#00-26-09_00-26-52_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Fast.and.the.Furious.2001__#01-38-50_01-39-59_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Hurt.Locker.2008__#0-29-25_0-30-15_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Hurt.Locker.2008__#0-45-39_0-47-06_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Intouchables.2011__#00-04-20_00-05-35_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Intouchables.2011__#00-51-55_00-55-16_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Intouchables.2011__#00-58-18_01-02-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Notebook.2004__#00-08-55_00-10-50_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Notebook.2004__#00-11-00_00-15-08_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Notebook.2004__#00-20-35_00-23-06_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Notebook.2004__#00-25-20_00-29-20_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Notebook.2004__#01-30-25_01-33-56_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Notebook.2004__#01-52-20_01-58-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Pursuit.of.Happyness.2006__#00-17-30_00-19-10_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Pursuit.of.Happyness.2006__#00-30-50_00-32-31_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Pursuit.of.Happyness.2006__#01-34-18_01-38-11_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Secret.Life.of.Walter.Mitty.2013__#00-08-55_00-10-50_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Secret.Life.of.Walter.Mitty.2013__#00-36-52_00-41-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Secret.Life.of.Walter.Mitty.2013__#01-01-20_01-04-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/The.Secret.Life.of.Walter.Mitty.2013__#01-23-00_01-29-08_label_A__vggish.npy', '/content/final_dl/list/xx/test/Thelma...Louise.1991__#01-53-54_01-54-50_label_A__vggish.npy', '/content/final_dl/list/xx/test/Tropa.de.Elite.2.2010__#00-05-30_00-06-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/Tropa.de.Elite.2.2010__#01-30-51_01-32-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/Tropa.de.Elite.2.2010__#01-33-31_01-34-20_label_A__vggish.npy', '/content/final_dl/list/xx/test/Tropa.de.Elite.2007__#00-10-30_00-12-08_label_A__vggish.npy', '/content/final_dl/list/xx/test/Tropa.de.Elite.2007__#00-52-33_00-54-35_label_A__vggish.npy', '/content/final_dl/list/xx/test/Yellow.Sea.2010__#00-10-16_00-11-14_label_A__vggish.npy', '/content/final_dl/list/xx/test/Young.And.Dangerous.I.1996__#0-55-09_0-56-59_label_A__vggish.npy', '/content/final_dl/list/xx/test/Young.And.Dangerous.II.1996__#0-53-25_0-54-20_label_A__vggish.npy', '/content/final_dl/list/xx/test/Young.And.Dangerous.III.1996__#00-15-52_00-17-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/Young.And.Dangerous.IV.1997__#0-10-35_0-11-50_label_A__vggish.npy', '/content/final_dl/list/xx/test/Your.Name.2016__#00-05-52_00-08-22_label_A__vggish.npy', '/content/final_dl/list/xx/test/Your.Name.2016__#01-01-20_01-04-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=-5RWxUUJ7lQ__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=-tqHsoEoUyY__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=0LEefpIPBHA__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=0vVOXlgHC1U__#00-12-00_00-18-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=0vVOXlgHC1U__#00-48-00_00-54-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=1202rYvw7go__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=1EvRIhdsX1s__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=2JJ9d97sd64__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=3MUqt3OkxIo__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=3ZVcLOjwOMk__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=3lSHv_DUbJs__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=4jVgDKbu_TI__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=5AlAh8J3WqE__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=5NZxPmTpV6Y__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=5l-nbDl0qvg__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=6cvZOGzfqWU__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=7smETZfN0XA__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=8SC55pYljMM__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=94zZ9iptP54__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=9FxmNOXF25Q__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=9eME1y6V-T4__#00-42-00_00-48-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=9eME1y6V-T4__#01-00-00_01-06-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=9eME1y6V-T4__#01-06-00_01-12-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=9eME1y6V-T4__#01-18-00_01-24-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=9eME1y6V-T4__#01-36-00_01-42-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=9eME1y6V-T4__#02-00-00_02-06-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=9qxJ6bznkNI__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=AqFWYZsOgWY__#00-21-00_00-24-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=AqFWYZsOgWY__#00-33-00_00-36-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=BL8ppFg7NKI__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=DdIA1Owbcdc__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=EToznHwS0OA__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=EeUTpfyb0qo__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=Ehh8ZdIMMj4__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=ErOcW_CSOyQ__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=GnN9VTh_s2A__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=GrHfWKeTDEI__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=H12BVHrNiXs__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=H9Y5_X1sEEA__#00-03-00_00-06-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=H9Y5_X1sEEA__#00-30-00_00-33-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=H9Y5_X1sEEA__#00-36-00_00-39-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=H9Y5_X1sEEA__#00-42-00_00-45-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=H9Y5_X1sEEA__#00-45-00_00-48-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=HTEyaXBPf9U__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=Hd_2Y29_FLU__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=HksDvkX6Bjk__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=I7oSupyExVY__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=JHsfKFF9Oag__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=JzrpCC7XmyE__#00-06-00_00-12-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=JzrpCC7XmyE__#01-06-00_01-12-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=KERh4R2BbS0__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=KEh6EOQvQO0__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=L2Qs4KyW-Co__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=LYzmr5ORvCg__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=MyM5G7fo7Gg__#00-06-00_00-09-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=MyM5G7fo7Gg__#00-18-00_00-21-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=NbNFfnVSIn8__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=NeAC-MfTZzU__#00-15-00_00-18-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=NjI-AuzKgMc__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=ODGDLQ0TT3M__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=Oh6FDdpH08A__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=Pf7WtK-YPDA__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=Q8b0XbtpFsA__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=QBfO3qfo7zg__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=QUlWIIHMIEQ__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=QiLNvC7CIuY__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=QozLwYYVotY__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=RRiN5W9XwJE__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=RarYGpz-blM__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=S1meRf5BsEw__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=SdYFFo_tsfk__#00-00-00_00-06-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=SdYFFo_tsfk__#00-18-00_00-24-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=SmFpAAHmoII__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=VGP9qh7U4H0__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=VNQ1Vkz_cM4__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=VRn8Tc02TbU__#00-18-00_00-21-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=VcsJf1q9QX8__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=W5qLlC9il3s__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=Wc2QW06H48M__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=Wp6gzFUaci8__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=X60PecZRLGE__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=Yr41_8QOTF8__#00-09-00_00-12-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=Yr41_8QOTF8__#00-21-00_00-24-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=ZpWR_YZNZTM__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=_Hn2Xlt-IWU__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=a0TQkX_zLt0__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=aBsrZKrRtqU__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=ajnl7P9bf78__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=bU86SW6L7Tc__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=cvEJ5WFk2KE__#00-06-00_00-09-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=eAgqJnO3kr8__#00-06-00_00-12-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=eAgqJnO3kr8__#00-42-00_00-48-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=eAgqJnO3kr8__#00-48-00_00-54-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=enYITYwvPAQ__#00-27-00_00-30-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=es97CGK_5XQ__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=f6j3YWgVBto__#00-01-50_00-02-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=fkps18H3SXY__#00-00-00_00-06-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=fkps18H3SXY__#00-36-00_00-42-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=fkps18H3SXY__#00-54-00_01-00-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=g4FnGZ5MAdo__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=g7fnNf88IOc__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=gGlqujwrhbI__#00-12-00_00-15-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=gxrLosPS27c__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=hdy5kZ5dwj8__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=hgoGZZSMhMQ__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=ibYV3e17R5c__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=jAvOGTD7_pU__#00-12-00_00-15-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=lVJVRywgmYM__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=ldnLxTablA8__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=lyZxvgPD4gM__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=mfFYGFJhBI4__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=n0AhGnEjako__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=p2ggHwtb-Zg__#00-21-00_00-24-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=p2ggHwtb-Zg__#00-24-00_00-27-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=pCyCi-q4MqY__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=plhVMWR33go__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=q2SP66Jb6Cw__#00-00-00_00-06-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=qV4YYnXdQCc__#00-03-00_00-06-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=qhUItxoTrmg__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=rJgg00ic1G8__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=rmDBn5Rpd3s__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=smnw0Q7AxFQ__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=tI7fktKY6OU__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=t_TZYpT6xjY__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=tlFJIG9mTGQ__#00-12-00_00-15-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=tlFJIG9mTGQ__#00-15-00_00-18-36_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=usAvzbDq6-c__#00-03-00_00-06-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=usAvzbDq6-c__#00-18-00_00-21-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=usAvzbDq6-c__#00-21-00_00-24-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=uyvM-vq5DwA__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=vFPQ_NiDBIU__#00-42-00_00-48-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=vFPQ_NiDBIU__#01-00-00_01-06-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=vFPQ_NiDBIU__#01-48-00_01-54-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=vFPQ_NiDBIU__#02-42-00_02-48-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=vHnAmx0UYbM__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=vQ9AwVrM9pU__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=vp_W8CqtCug__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=vsI_pKNcgeQ__#00-15-00_00-18-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=wQrV75N2BrI__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=x0-z4h6mFoU__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=y7JEq-kf-2I__#00-01-50_00-02-30_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=y7vIfdKwHDk__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=yZ2wlTAN49g__#00-03-00_00-06-00_label_A__vggish.npy', '/content/final_dl/list/xx/test/v=yviKz5J-pEA__#1_label_A__vggish.npy', '/content/final_dl/list/xx/test/wangted.2008__#0-27-17_0-28-07_label_A__vggish.npy']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "root_path = '/content/final_dl/list/xx/test'    ## the path of features\n",
        "files = sorted(glob.glob(os.path.join(root_path, \"*.npy\")))\n",
        "\n",
        "# print(files)\n",
        "\n",
        "# print(files)\n",
        "violents = []\n",
        "normal = []\n",
        "with open('audio_test.list', 'w+') as f:  ## the name of feature list\n",
        "    for file in files:\n",
        "        if '_label_A' in file:\n",
        "            normal.append(file)\n",
        "            # print(file)\n",
        "        else:\n",
        "            newline = file+'\\n'\n",
        "            f.write(newline)\n",
        "    for file in normal:\n",
        "        newline = file+'\\n'\n",
        "        f.write(newline)\n",
        "\n",
        "print(normal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvjQEBc_MFc7"
      },
      "source": [
        "## F) Updating the Lists files\n",
        "\n",
        "The list files are used to specify the paths of all features (audio and visual) for testing and training. The features are just the latent video and audio information for training and testing (VGGish model for audio, i3d model for video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcvPLGFjV9de",
        "outputId": "3cbc62bc-5155-4667-98cc-793ed18d305f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/rgb_test.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/rgb_test.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/dl_files/i3d-features/RGBTest\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/rgb_test.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1omdKx6CM0_",
        "outputId": "bc70d0d5-edfe-43e4-93c4-d4fb3a8bfea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/audio_test.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/audio_test.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/list/xx/test/\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/audio_test.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z-JVZlBNeg4"
      },
      "source": [
        "## Args\n",
        "\n",
        "Here are the default args that were obtained via cmd line arg parser. I just created a class 'Args' that holds the default config for the model.\n",
        "\n",
        "I think the most important args:\n",
        "\n",
        "*`Modality`*: Determines whether we want to use either audio alone, video alone, both audio and video, audio, video, and flow, etc. for training\n",
        "\n",
        "*`List`*: point to the list containing filenames for all training and testing data.\n",
        "\n",
        "*`workers`*: I believe this is the number of individual threads/processes running during training or testing. In ther model it was set to 4 by defualt but that spit out an error so it lowered it to 1. Prob a sign that we need to do heavy downsampling to compensate for lack of parallel processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "0g_4ciyOM8tl"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "      self.modality = 'MIX2'\n",
        "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "      self.flow_list = '/content/final_dl/list/flow.list'\n",
        "      self.audio_list = '/content/final_dl/list/audio.list'\n",
        "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "      self.gt = '/content/final_dl/list/gt.npy'\n",
        "      self.gpus = 1\n",
        "      self.lr = 0.0001\n",
        "      self.batch_size = 128\n",
        "      self.workers = 1\n",
        "      self.model_name = 'wsanodet'\n",
        "      self.pretrained_ckpt = None\n",
        "      self.feature_size = 1152  # 1024 + 128\n",
        "      self.num_classes = 1\n",
        "      self.dataset_name = 'XD-Violence'\n",
        "      self.max_seqlen = 200\n",
        "      self.max_epoch = 50\n",
        "\n",
        "  # Create an instance of the Args class\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NCyNXLPN5u0"
      },
      "source": [
        "## Testing PreTrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkT8DU7V3HOz",
        "outputId": "d5c631ba-a41e-4bcc-9df1-9acb3c54104f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-209-778b192ae798>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  {k.replace('module.', ''): v for k, v in torch.load('final_dl/wsanodet_mix2.pkl').items()})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time:12.470504760742188\n",
            "offline pr_auc:0.79; online pr_auc:0.7433\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "# from model import Model\n",
        "# from dataset import Dataset\n",
        "# from test import test\n",
        "# import option\n",
        "import time\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  test_loader = DataLoader(Dataset(args, test_mode=True),\n",
        "                            batch_size=5, shuffle=False,\n",
        "                            num_workers=args.workers, pin_memory=True)\n",
        "  model = Model(args)\n",
        "  model = model.to(device)\n",
        "\n",
        "  model_dict = model.load_state_dict(\n",
        "      {k.replace('module.', ''): v for k, v in torch.load('final_dl/wsanodet_mix2.pkl').items()})\n",
        "\n",
        "  gt = np.load(args.gt)\n",
        "  st = time.time()\n",
        "  pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "  print('Time:{}'.format(time.time()-st))\n",
        "  print('offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIbLkkivMhx"
      },
      "source": [
        "how to save a model for the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "k7mSdUwSvLTq"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"/content/test.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0S5vwXjQzID"
      },
      "source": [
        "# Training HLNET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "Lv0i1DYkQ2Qj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def CLAS(logits, label, seq_len, criterion, device, is_topk=True):\n",
        "    logits = logits.squeeze()\n",
        "    instance_logits = torch.zeros(0).to(device)  # tensor([])\n",
        "    for i in range(logits.shape[0]):\n",
        "        if is_topk:\n",
        "            tmp, _ = torch.topk(logits[i][:seq_len[i]], k=int(seq_len[i]//16+1), largest=True)\n",
        "            tmp = torch.mean(tmp).view(1)\n",
        "        else:\n",
        "            tmp = torch.mean(logits[i, :seq_len[i]]).view(1)\n",
        "        instance_logits = torch.cat((instance_logits, tmp))\n",
        "\n",
        "    instance_logits = torch.sigmoid(instance_logits)\n",
        "\n",
        "    clsloss = criterion(instance_logits, label)\n",
        "    return clsloss\n",
        "\n",
        "\n",
        "def CENTROPY(logits, logits2, seq_len, device):\n",
        "    instance_logits = torch.tensor(0).to(device)  # tensor([])\n",
        "    for i in range(logits.shape[0]):\n",
        "        tmp1 = torch.sigmoid(logits[i, :seq_len[i]]).squeeze()\n",
        "        tmp2 = torch.sigmoid(logits2[i, :seq_len[i]]).squeeze()\n",
        "        loss = torch.mean(-tmp1.detach() * torch.log(tmp2))\n",
        "        instance_logits = instance_logits + loss\n",
        "    instance_logits = instance_logits/logits.shape[0]\n",
        "    return instance_logits\n",
        "\n",
        "\n",
        "def train(dataloader, model, optimizer, criterion, device, is_topk):\n",
        "    with torch.set_grad_enabled(True):\n",
        "        model.train()\n",
        "        for i, (input, label) in enumerate(dataloader):\n",
        "            seq_len = torch.sum(torch.max(torch.abs(input), dim=2)[0]>0, 1)\n",
        "            input = input[:, :torch.max(seq_len), :]\n",
        "            input, label = input.float().to(device), label.float().to(device)\n",
        "            logits, logits2 = model(input, seq_len)\n",
        "            clsloss = CLAS(logits, label, seq_len, criterion, device, is_topk)\n",
        "            clsloss2 = CLAS(logits2, label, seq_len, criterion, device, is_topk)\n",
        "            croloss = CENTROPY(logits, logits2, seq_len, device)\n",
        "\n",
        "            total_loss = clsloss + clsloss2 + 5*croloss\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcLzmEr4HIQY",
        "outputId": "a2bcf406-272e-4df6-ae5b-28c782e3e9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/audio.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/audio.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/list/xx/train\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/audio.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mubL6MwpIS8k",
        "outputId": "81ebfaec-fb8c-4e61-e7ca-4e7653ed869f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/rgb.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/rgb.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/rgb.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYz5pRU7Iyf8"
      },
      "source": [
        "## test (ignore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTEZP14qIyOD",
        "outputId": "3067bd70-3ce8-4c53-a00a-78f5f39585d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 200, 1152])\n",
            "torch.Size([5, 200, 1152])\n",
            "torch.Size([5, 200, 1152])\n"
          ]
        }
      ],
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "      self.modality = 'MIX2'\n",
        "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "      self.flow_list = '/content/final_dl/list/flow.list'\n",
        "      self.audio_list = '/content/final_dl/list/audio.list'\n",
        "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "      self.gt = '/content/final_dl/list/gt.npy'\n",
        "      self.gpus = 1\n",
        "      self.lr = 0.0001\n",
        "      self.batch_size = 128\n",
        "      self.workers = 1\n",
        "      self.model_name = 'wsanodet'\n",
        "      self.pretrained_ckpt = None\n",
        "      self.feature_size = 1152  # 1024 + 128\n",
        "      self.num_classes = 1\n",
        "      self.dataset_name = 'XD-Violence'\n",
        "      self.max_seqlen = 200\n",
        "      self.max_epoch = 50\n",
        "\n",
        "  # Create an instance of the Args class\n",
        "args = Args()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "test_loader = DataLoader(Dataset(args, test_mode=False),\n",
        "                          batch_size=5, shuffle=True,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (input,label) in enumerate(test_loader):\n",
        "    input = input.to(device)\n",
        "\n",
        "    print(input.shape)\n",
        "    ############\n",
        "    ### NOTE: ## setting seq_len to None pads training data in the sequence dim to 200\n",
        "    ############\n",
        "    logits, logits2 = model(inputs=input, seq_len=None)\n",
        "    # print(logits, logits2)\n",
        "    if i == 2:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training HL NET"
      ],
      "metadata": {
        "id": "SM88zu_-ADPw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pFJMkujfFROw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "# import option\n",
        "\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# torch.multiprocessing.set_start_method('spawn')\n",
        "# setup_seed(2333)\n",
        "# args = option.parser.parse_args()\n",
        "\n",
        "!export TORCH_USE_CUDA_DSA=ON\n",
        "device = torch.device(\"cuda\")\n",
        "train_loader = DataLoader(Dataset(args, test_mode=False),\n",
        "                          batch_size=args.batch_size, shuffle=True,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "test_loader = DataLoader(Dataset(args, test_mode=True),\n",
        "                          batch_size=5, shuffle=False,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "for name, value in model.named_parameters():\n",
        "    print(name)\n",
        "approximator_param = list(map(id, model.approximator.parameters()))\n",
        "approximator_param += list(map(id, model.conv1d_approximator.parameters()))\n",
        "base_param = filter(lambda p: id(p) not in approximator_param, model.parameters())\n",
        "\n",
        "if not os.path.exists('./ckpt'):\n",
        "    os.makedirs('./ckpt')\n",
        "optimizer = optim.Adam([{'params': base_param},\n",
        "                        {'params': model.approximator.parameters(), 'lr': args.lr / 2},\n",
        "                        {'params': model.conv1d_approximator.parameters(), 'lr': args.lr / 2},\n",
        "                        ],\n",
        "                        lr=args.lr, weight_decay=0.000)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "is_topk = True\n",
        "gt = np.load(args.gt)\n",
        "pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "print('Random initalization: offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n",
        "for epoch in range(args.max_epoch):\n",
        "    scheduler.step()\n",
        "    st = time.time()\n",
        "    train(train_loader, model, optimizer, criterion, device, is_topk)\n",
        "    if epoch % 2 == 0 and not epoch == 0:\n",
        "        torch.save(model.state_dict(), './ckpt/'+args.model_name+'{}.pkl'.format(epoch))\n",
        "\n",
        "    pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "    print('Epoch {0}/{1}: offline pr_auc:{2:.4}; online pr_auc:{3:.4}\\n'.format(epoch, args.max_epoch, pr_auc, pr_auc_online))\n",
        "torch.save(model.state_dict(), './ckpt/' + args.model_name + '.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYjtaIacOHyS"
      },
      "source": [
        "# Training VAE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE MODEL"
      ],
      "metadata": {
        "id": "FRzWrGepsWur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R8Ma-ndq_5GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Sampling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, z_means, z_log_vars):\n",
        "        epsilon = torch.randn_like(z_means, dtype=torch.float32)\n",
        "        return z_means + torch.exp(0.5 * z_log_vars) * epsilon\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1024, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, 512, kernel_size=3, stride=2, padding=1),  # (B, 1024, 200) -> (B, 512, 100)\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(512, 256, kernel_size=3, stride=2, padding=1),         # (B, 512, 100) -> (B, 256, 50)\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(256, 128, kernel_size=3, stride=2, padding=1),         # (B, 256, 50) -> (B, 128, 25)\n",
        "            nn.ReLU(True),\n",
        "            nn.Flatten()  # Flatten for fully connected layers\n",
        "        )\n",
        "\n",
        "        flattened_dim = 25 * 128  # Calculate flattened dimension\n",
        "        self.lin_mean = nn.Linear(flattened_dim, latent_dim)\n",
        "        self.lin_log_var = nn.Linear(flattened_dim, latent_dim)\n",
        "        self.sampling = Sampling()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        z_means = self.lin_mean(x)\n",
        "        z_log_vars = self.lin_log_var(x)\n",
        "        z = self.sampling(z_means, z_log_vars)\n",
        "        return z, z_means, z_log_vars\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1024, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        flattened_dim = 25 * 128  # Must match Encoder's flattened_dim (200)\n",
        "\n",
        "        self.decoder_fc = nn.Sequential(\n",
        "            nn.Linear(latent_dim, flattened_dim),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose1d(128, 256, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 128, 25) -> (B, 256, 50)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 256, 50) -> (B, 512, 100)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(512, input_dim, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 512, 100) -> (B, 1024, 200)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.decoder_fc(x)  # Fully connected layer\n",
        "        x = x.view(-1, 128, 25)  # Reshape to match ConvTranspose1D input\n",
        "        x = self.decoder_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1024, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_dim, input_dim, seq_len)\n",
        "        self.decoder = Decoder(latent_dim, input_dim, seq_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, z_means, z_log_vars = self.encoder(x)\n",
        "        x_reconstructed = self.decoder(z)\n",
        "        x_reconstructed = x_reconstructed.view(-1, 200, 1024)\n",
        "        return x_reconstructed, z_means, z_log_vars"
      ],
      "metadata": {
        "collapsed": true,
        "id": "r7Hlyuuos9DQ"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "      self.modality = 'RGB'\n",
        "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "      self.flow_list = '/content/final_dl/list/flow.list'\n",
        "      self.audio_list = '/content/final_dl/list/audio.list'\n",
        "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "      self.gt = '/content/final_dl/list/gt.npy'\n",
        "      self.gpus = 1\n",
        "      self.lr = 0.0001\n",
        "      self.batch_size = 1\n",
        "      self.workers = 1\n",
        "      self.model_name = 'wsanodet'\n",
        "      self.pretrained_ckpt = None\n",
        "      # self.feature_size = 1152  # 1024 + 128\n",
        "      self.feature_size = 1024\n",
        "      self.num_classes = 1\n",
        "      self.dataset_name = 'XD-Violence'\n",
        "      self.max_seqlen = 200\n",
        "      self.max_epoch = 50\n",
        "\n",
        "  # Create an instance of the Args class\n",
        "args = Args()\n",
        "\n"
      ],
      "metadata": {
        "id": "T9Bi_42c8Tts"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE Train FN"
      ],
      "metadata": {
        "id": "DQrX4xU68twm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_loader = DataLoader(Dataset(args, test_mode=False),\n",
        "                          batch_size=args.batch_size, shuffle=True,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "with torch.set_grad_enabled(True):\n",
        "  model.train()\n",
        "  for i, (input, label) in enumerate(train_loader):\n",
        "\n",
        "    input, label = input.float().to(torch.float32), label.float().to(device)\n",
        "\n",
        "    # input and label from train dataset\n",
        "    print(\"shape of input: \", input.shape)\n",
        "    print(\"shape of label: \", label.shape)\n",
        "\n",
        "    input = input.to(device)\n",
        "\n",
        "    # size of one sample from batch\n",
        "    test_input = input[:1][::][::]\n",
        "\n",
        "    # permuting input shape to fit vae cnn layers\n",
        "    test_input = test_input.permute(0, 2, 1)\n",
        "    print(\"test_input shape: \", test_input.shape)\n",
        "\n",
        "    # passing through vae\n",
        "    vae = VAE(latent_dim=128, input_dim=1024, seq_len=200)\n",
        "    vae = vae.to(device)\n",
        "\n",
        "    x_reconstructed, z_means, z_log_vars = vae(test_input)\n",
        "\n",
        "    print(\"reconstructed input shape (after vae): \", x_reconstructed.shape)\n",
        "\n",
        "    ## break statement just to read out this input/output shapes in the first iteration\n",
        "    if i == 0:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot6yc6GL8eTi",
        "outputId": "36be32f4-6da6-4220-b546-bfdf2b23033a"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            "shape of input:  torch.Size([1, 200, 1024])\n",
            "shape of label:  torch.Size([1])\n",
            "test_input shape:  torch.Size([1, 1024, 200])\n",
            "reconstructed input shape (after vae):  torch.Size([1, 200, 1024])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1XTC5Esw4CBi_Xs00PGh-LAkkDmj1oR9v",
      "authorship_tag": "ABX9TyNECtFX8TX74NW/JHgCHVQP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}