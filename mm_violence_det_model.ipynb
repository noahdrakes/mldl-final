{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/noahdrakes/mldl-final/blob/main/mm_violence_det_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDOr6KirJvUe"
   },
   "source": [
    "# Multi-Modal Violence Detection Network\n",
    "\n",
    "original src code: https://github.com/Roc-Ng/XDVioDet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhzPvDXTKKqM"
   },
   "source": [
    "### Copying Training and Testing Data\n",
    "\n",
    "The folders are pretty large (~40/50GB) so it takes a while to copy all of the data over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4DG4K3wYK8P",
    "outputId": "4985a11f-8f3f-47ee-debd-9485e2d1aae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /mydrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/mydrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88GWL7hscvcg",
    "outputId": "1f2ac259-fbd9-441a-e9ba-1248aad33c86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mydrive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "%cd /mydrive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Nr7SpsS-2P6x"
   },
   "outputs": [],
   "source": [
    "!unzip final_dl.zip -d /content/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zWKH9vsRatb"
   },
   "source": [
    "may need to change directory depending on where you upload the data to google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CHQ29LtvAsNj"
   },
   "outputs": [],
   "source": [
    "# !cp -r /mydrive/MyDrive/final_dl ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N1LDIKfKhpP"
   },
   "source": [
    "## 1. Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9acUBEhKqjC"
   },
   "source": [
    "### A) Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "IPMWSbm0LBGY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def test(dataloader, model, device, gt):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred = torch.zeros(0).to(device)\n",
    "        pred2 = torch.zeros(0).to(device)\n",
    "        for i, input in enumerate(dataloader):\n",
    "            input = input.to(device)\n",
    "            logits, logits2 = model(inputs=input, seq_len=None)\n",
    "            logits = torch.squeeze(logits)\n",
    "            sig = torch.sigmoid(logits)\n",
    "            sig = torch.mean(sig, 0)\n",
    "            pred = torch.cat((pred, sig))\n",
    "            '''\n",
    "            online detection\n",
    "            '''\n",
    "            logits2 = torch.squeeze(logits2)\n",
    "            sig2 = torch.sigmoid(logits2)\n",
    "            sig2 = torch.mean(sig2, 0)\n",
    "\n",
    "            sig2 = torch.unsqueeze(sig2, 1) ##for audio\n",
    "            pred2 = torch.cat((pred2, sig2))\n",
    "\n",
    "            # print(\"pred:, \", pred)\n",
    "            # print(\"pred2:, \", pred2)\n",
    "\n",
    "        pred = list(pred.cpu().detach().numpy())\n",
    "        pred2 = list(pred2.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred, 16))\n",
    "        pr_auc = auc(recall, precision)\n",
    "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred2, 16))\n",
    "        pr_auc2 = auc(recall, precision)\n",
    "        return pr_auc, pr_auc2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLV269m9K7L3"
   },
   "source": [
    "### B) Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ej2MARCmKzvk"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def random_extract(feat, t_max):\n",
    "   r = np.random.randint(len(feat)-t_max)\n",
    "   return feat[r:r+t_max]\n",
    "\n",
    "def uniform_extract(feat, t_max):\n",
    "   r = np.linspace(0, len(feat)-1, t_max, dtype=np.uint16)\n",
    "   return feat[r, :]\n",
    "\n",
    "def pad(feat, min_len):\n",
    "    if np.shape(feat)[0] <= min_len:\n",
    "       return np.pad(feat, ((0, min_len-np.shape(feat)[0]), (0, 0)), mode='constant', constant_values=0)\n",
    "    else:\n",
    "       return feat\n",
    "\n",
    "def process_feat(feat, length, is_random=True):\n",
    "    if len(feat) > length:\n",
    "        if is_random:\n",
    "            return random_extract(feat, length)\n",
    "        else:\n",
    "            return uniform_extract(feat, length)\n",
    "    else:\n",
    "        return pad(feat, length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkKIYnHpLDKS"
   },
   "source": [
    "### C) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98z1xwOKKueM"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "# from utils import process_feat\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, args, transform=None, mode='train'):\n",
    "        self.modality = args.modality\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args: Arguments containing dataset paths and configuration\n",
    "            transform: Optional transforms to apply\n",
    "            mode: One of ['train', 'val', 'test'] to specify the dataset split\n",
    "        \"\"\"\n",
    "\n",
    "        if mode == 'test':\n",
    "            self.rgb_list_file = args.test_rgb_list\n",
    "            self.flow_list_file = args.test_flow_list\n",
    "            self.audio_list_file = args.test_audio_list\n",
    "        elif mode == 'val':\n",
    "            self.rgb_list_file = args.val_rgb_list\n",
    "            self.flow_list_file = args.val_flow_list\n",
    "            self.audio_list_file = args.val_audio_list\n",
    "        else: # train\n",
    "            self.rgb_list_file = args.train_rgb_list\n",
    "            self.flow_list_file = args.train_flow_list\n",
    "            self.audio_list_file = args.train_audio_list\n",
    "\n",
    "        self.max_seqlen = args.max_seqlen\n",
    "        self.tranform = transform\n",
    "        self.test_mode = (mode == 'test')\n",
    "        self.normal_flag = '_label_A'\n",
    "        self._parse_list()\n",
    "\n",
    "    def _parse_list(self):\n",
    "        if self.modality == 'AUDIO':\n",
    "            self.list = list(open(self.audio_list_file))\n",
    "        elif self.modality == 'RGB':\n",
    "            self.list = list(open(self.rgb_list_file))\n",
    "            print(\"here\")\n",
    "            # print(self.list)\n",
    "        elif self.modality == 'FLOW':\n",
    "            self.list = list(open(self.flow_list_file))\n",
    "        elif self.modality == 'MIX':\n",
    "            self.list = list(open(self.rgb_list_file))\n",
    "            self.flow_list = list(open(self.flow_list_file))\n",
    "        elif self.modality == 'MIX2':\n",
    "            self.list = list(open(self.rgb_list_file))\n",
    "            self.audio_list = list(open(self.audio_list_file))\n",
    "        elif self.modality == 'MIX3':\n",
    "            self.list = list(open(self.flow_list_file))\n",
    "            self.audio_list = list(open(self.audio_list_file))\n",
    "        elif self.modality == 'MIX_ALL':\n",
    "            self.list = list(open(self.rgb_list_file))\n",
    "            self.flow_list = list(open(self.flow_list_file))\n",
    "            self.audio_list = list(open(self.audio_list_file))\n",
    "        else:\n",
    "            assert 1 > 2, 'Modality is wrong!'\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.normal_flag in self.list[index]:\n",
    "            label = 0.0\n",
    "        else:\n",
    "            label = 1.0\n",
    "\n",
    "        if self.modality == 'AUDIO':\n",
    "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "        elif self.modality == 'RGB':\n",
    "            features = np.array(np.load(self.list[index].strip('\\n')),dtype=np.float32)\n",
    "        elif self.modality == 'FLOW':\n",
    "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "        elif self.modality == 'MIX':\n",
    "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
    "            if features1.shape[0] == features2.shape[0]:\n",
    "                features = np.concatenate((features1, features2),axis=1)\n",
    "            else:# because the frames of flow is one less than that of rgb\n",
    "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
    "        elif self.modality == 'MIX2':\n",
    "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
    "        elif self.modality == 'MIX3':\n",
    "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
    "            if features1.shape[0] == features2.shape[0]:\n",
    "                features = np.concatenate((features1, features2),axis=1)\n",
    "            else:# because the frames of flow is one less than that of rgb\n",
    "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
    "        elif self.modality == 'MIX_ALL':\n",
    "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
    "            features3 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
    "            if features1.shape[0] == features2.shape[0]:\n",
    "                features = np.concatenate((features1, features2, features3),axis=1)\n",
    "            else:# because the frames of flow is one less than that of rgb\n",
    "                features = np.concatenate((features1[:-1], features2, features3[:-1]), axis=1)\n",
    "        else:\n",
    "            assert 1>2, 'Modality is wrong!'\n",
    "        if self.tranform is not None:\n",
    "            features = self.tranform(features)\n",
    "        if self.test_mode:\n",
    "            return features\n",
    "\n",
    "        else:\n",
    "            features = process_feat(features, self.max_seqlen, is_random=False)\n",
    "            return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvvMYO_1LXTV"
   },
   "source": [
    "### D) Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bYn0UsFKWTy"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from torch import FloatTensor\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(in_features, out_features).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
    "        self.a = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(2*out_features, 1).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "class linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(linear, self).__init__()\n",
    "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
    "        self.register_parameter('bias', None)\n",
    "        stdv = 1. / sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "    def forward(self, x):\n",
    "        x = x.matmul(self.weight)\n",
    "        return x\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False, residual=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_features == out_features):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            # self.residual = linear(in_features, out_features)\n",
    "            self.residual = nn.Conv1d(in_channels=in_features, out_channels=out_features, kernel_size=5, padding=2)\n",
    "    def reset_parameters(self):\n",
    "        # stdv = 1. / sqrt(self.weight.size(1))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # To support batch operations\n",
    "        support = input.matmul(self.weight)\n",
    "        output = adj.matmul(support)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        if self.in_features != self.out_features and self.residual:\n",
    "            input = input.permute(0,2,1)\n",
    "            res = self.residual(input)\n",
    "            res = res.permute(0,2,1)\n",
    "            output = output + res\n",
    "        else:\n",
    "            output = output + self.residual(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "######################################################\n",
    "\n",
    "class SimilarityAdj(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SimilarityAdj, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weight0 = Parameter(FloatTensor(in_features, out_features))\n",
    "        self.weight1 = Parameter(FloatTensor(in_features, out_features))\n",
    "        self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # stdv = 1. / sqrt(self.weight0.size(1))\n",
    "        nn.init.xavier_uniform_(self.weight0)\n",
    "        nn.init.xavier_uniform_(self.weight1)\n",
    "\n",
    "    def forward(self, input, seq_len):\n",
    "        # To support batch operations\n",
    "        soft = nn.Softmax(1)\n",
    "        theta = torch.matmul(input, self.weight0)\n",
    "        phi = torch.matmul(input, self.weight0)\n",
    "        phi2 = phi.permute(0, 2, 1)\n",
    "        sim_graph = torch.matmul(theta, phi2)\n",
    "\n",
    "        theta_norm = torch.norm(theta, p=2, dim=2, keepdim=True)  # B*T*1\n",
    "        phi_norm = torch.norm(phi, p=2, dim=2, keepdim=True)  # B*T*1\n",
    "        x_norm_x = theta_norm.matmul(phi_norm.permute(0, 2, 1))\n",
    "        sim_graph = sim_graph / (x_norm_x + 1e-20)\n",
    "\n",
    "        output = torch.zeros_like(sim_graph)\n",
    "        if seq_len is None:\n",
    "            for i in range(sim_graph.shape[0]):\n",
    "                tmp = sim_graph[i]\n",
    "                adj2 = tmp\n",
    "                adj2 = F.threshold(adj2, 0.7, 0)\n",
    "                adj2 = soft(adj2)\n",
    "                output[i] = adj2\n",
    "        else:\n",
    "            for i in range(len(seq_len)):\n",
    "                tmp = sim_graph[i, :seq_len[i], :seq_len[i]]\n",
    "                adj2 = tmp\n",
    "                adj2 = F.threshold(adj2, 0.7, 0)\n",
    "                adj2 = soft(adj2)\n",
    "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class DistanceAdj(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistanceAdj, self).__init__()\n",
    "        self.sigma = Parameter(FloatTensor(1))\n",
    "        self.sigma.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, batch_size, max_seqlen):\n",
    "        # To support batch operations\n",
    "        self.arith = np.arange(max_seqlen).reshape(-1, 1)\n",
    "        dist = pdist(self.arith, metric='cityblock').astype(np.float32)\n",
    "        self.dist = torch.from_numpy(squareform(dist)).to('cuda')\n",
    "        self.dist = torch.exp(-self.dist / torch.exp(torch.tensor(1.)))\n",
    "        self.dist = torch.unsqueeze(self.dist, 0).repeat(batch_size, 1, 1).to('cuda')\n",
    "        return self.dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44eeeOUcLquB"
   },
   "source": [
    "### E) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8Rjvb-yKOeZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as torch_init\n",
    "import os\n",
    "# from layers import GraphConvolution, SimilarityAdj, DistanceAdj\n",
    "\n",
    "\n",
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        torch_init.xavier_uniform_(m.weight)\n",
    "        # m.bias.data.fill_(0.1)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        n_features = args.feature_size\n",
    "        n_class = args.num_classes\n",
    "\n",
    "        self.conv1d1 = nn.Conv1d(in_channels=n_features, out_channels=512, kernel_size=1, padding=0)\n",
    "        self.conv1d2 = nn.Conv1d(in_channels=512, out_channels=128, kernel_size=1, padding=0)\n",
    "        self.conv1d3 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv1d4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "        # Graph Convolution\n",
    "        self.gc1 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
    "        self.gc2 = GraphConvolution(32, 32, residual=True)\n",
    "        self.gc3 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
    "        self.gc4 = GraphConvolution(32, 32, residual=True)\n",
    "        self.gc5 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
    "        self.gc6 = GraphConvolution(32, 32, residual=True)\n",
    "        self.simAdj = SimilarityAdj(n_features, 32)\n",
    "        self.disAdj = DistanceAdj()\n",
    "\n",
    "        self.classifier = nn.Linear(32*3, n_class)\n",
    "        self.approximator = nn.Sequential(nn.Conv1d(128, 64, 1, padding=0), nn.ReLU(),\n",
    "                                          nn.Conv1d(64, 32, 1, padding=0), nn.ReLU())\n",
    "        self.conv1d_approximator = nn.Conv1d(32, 1, 5, padding=0)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.apply(weight_init)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs, seq_len):\n",
    "        x = inputs.permute(0, 2, 1)  # for conv1d\n",
    "        x = self.relu(self.conv1d1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv1d2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        logits = self.approximator(x)\n",
    "        logits = F.pad(logits, (4, 0))\n",
    "        logits = self.conv1d_approximator(logits)\n",
    "        logits = logits.permute(0, 2, 1)\n",
    "        x = x.permute(0, 2, 1)  # b*t*c\n",
    "\n",
    "        ## gcn\n",
    "        scoadj = self.sadj(logits.detach(), seq_len)\n",
    "        adj = self.adj(inputs, seq_len)\n",
    "        disadj = self.disAdj(x.shape[0], x.shape[1])\n",
    "        x1_h = self.relu(self.gc1(x, adj))\n",
    "        x1_h = self.dropout(x1_h)\n",
    "        x2_h = self.relu(self.gc3(x, disadj))\n",
    "        x2_h = self.dropout(x2_h)\n",
    "        x3_h = self.relu(self.gc5(x, scoadj))\n",
    "        x3_h = self.dropout(x3_h)\n",
    "        x1 = self.relu(self.gc2(x1_h, adj))\n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = self.relu(self.gc4(x2_h, disadj))\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.relu(self.gc6(x3_h, scoadj))\n",
    "        x3 = self.dropout(x3)\n",
    "        x = torch.cat((x1, x2, x3), 2)\n",
    "        x = self.classifier(x)\n",
    "        return x, logits\n",
    "\n",
    "    def sadj(self, logits, seq_len):\n",
    "        lens = logits.shape[1]\n",
    "        soft = nn.Softmax(1)\n",
    "        logits2 = self.sigmoid(logits).repeat(1, 1, lens)\n",
    "        tmp = logits2.permute(0, 2, 1)\n",
    "        adj = 1. - torch.abs(logits2 - tmp)\n",
    "        self.sig = lambda x:1/(1+torch.exp(-((x-0.5))/0.1))\n",
    "        adj = self.sig(adj)\n",
    "        output = torch.zeros_like(adj)\n",
    "        if seq_len is None:\n",
    "            for i in range(logits.shape[0]):\n",
    "                tmp = adj[i]\n",
    "                adj2 = soft(tmp)\n",
    "                output[i] = adj2\n",
    "        else:\n",
    "            for i in range(len(seq_len)):\n",
    "                tmp = adj[i, :seq_len[i], :seq_len[i]]\n",
    "                adj2 = soft(tmp)\n",
    "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
    "        return output\n",
    "\n",
    "\n",
    "    def adj(self, x, seq_len):\n",
    "        soft = nn.Softmax(1)\n",
    "        x2 = x.matmul(x.permute(0,2,1)) # B*T*T\n",
    "        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n",
    "        x_norm_x = x_norm.matmul(x_norm.permute(0,2,1))\n",
    "        x2 = x2/(x_norm_x+1e-20)\n",
    "        output = torch.zeros_like(x2)\n",
    "        if seq_len is None:\n",
    "            for i in range(x.shape[0]):\n",
    "                tmp = x2[i]\n",
    "                adj2 = tmp\n",
    "                adj2 = F.threshold(adj2, 0.7, 0)\n",
    "                adj2 = soft(adj2)\n",
    "                output[i] = adj2\n",
    "        else:\n",
    "            for i in range(len(seq_len)):\n",
    "                tmp = x2[i, :seq_len[i], :seq_len[i]]\n",
    "                adj2 = tmp\n",
    "                adj2 = F.threshold(adj2, 0.7, 0)\n",
    "                adj2 = soft(adj2)\n",
    "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z-JVZlBNeg4"
   },
   "source": [
    "## Args\n",
    "\n",
    "Here are the default args that were obtained via cmd line arg parser. I just created a class 'Args' that holds the default config for the model.\n",
    "\n",
    "I think the most important args:\n",
    "\n",
    "*`Modality`*: Determines whether we want to use either audio alone, video alone, both audio and video, audio, video, and flow, etc. for training\n",
    "\n",
    "*`List`*: point to the list containing filenames for all training and testing data.\n",
    "\n",
    "*`workers`*: I believe this is the number of individual threads/processes running during training or testing. In ther model it was set to 4 by defualt but that spit out an error so it lowered it to 1. Prob a sign that we need to do heavy downsampling to compensate for lack of parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0g_4ciyOM8tl"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.modality = 'MIX2'\n",
    "        # Original paths\n",
    "        self.rgb_list = '/content/final_dl/list/rgb.list'\n",
    "        self.flow_list = '/content/final_dl/list/flow.list'\n",
    "        self.audio_list = '/content/final_dl/list/audio.list'\n",
    "\n",
    "        # Train paths\n",
    "        self.train_rgb_list = '/content/final_dl/list/rgb_train.list'\n",
    "        self.train_flow_list = '/content/final_dl/list/flow_train.list'\n",
    "        self.train_audio_list = '/content/final_dl/list/audio_train.list'\n",
    "\n",
    "        # Val paths\n",
    "        self.val_rgb_list = '/content/final_dl/list/rgb_val.list'\n",
    "        self.val_flow_list = '/content/final_dl/list/flow_val.list'\n",
    "        self.val_audio_list = '/content/final_dl/list/audio_val.list'\n",
    "\n",
    "        # Test paths\n",
    "        self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
    "        self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
    "        self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
    "\n",
    "        self.gt = '/content/final_dl/list/gt.npy'\n",
    "        self.gpus = 1\n",
    "        self.lr = 0.0001\n",
    "        self.batch_size = 128\n",
    "        self.workers = 1  # Reduced from 4 to avoid memory issues\n",
    "        self.model_name = 'wsanodet'\n",
    "        self.pretrained_ckpt = None\n",
    "        self.feature_size = 1152  # 1024 + 128\n",
    "        self.num_classes = 1\n",
    "        self.dataset_name = 'XD-Violence'\n",
    "        self.max_seqlen = 200\n",
    "        self.max_epoch = 50\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xM7Rywzs4s8"
   },
   "source": [
    "## Val Split For MultiModal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRVNY_NEcEB5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "def get_video_id(filepath):\n",
    "    \"\"\"Extract consistent video ID for both RGB and audio files\"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    # Handle RGB files (end with __[0-4].npy)\n",
    "    if filename.endswith(('.npy')):\n",
    "        base = filename.rsplit('__', 1)[0]  # Split from the right on __ to remove the number or vggish\n",
    "        return base\n",
    "\n",
    "    # Fallback - just remove extension\n",
    "    return filename.split('.')[0]\n",
    "\n",
    "def create_splits(aligned_files, train_ratio=0.8, seed=42):\n",
    "    \"\"\"Split the video IDs first, then we'll expand to files in write_list_files\"\"\"\n",
    "    random.seed(seed)\n",
    "    video_ids = list(aligned_files.keys())\n",
    "    train_size = int(len(video_ids) * train_ratio)\n",
    "    train_ids = random.sample(video_ids, train_size)\n",
    "    val_ids = [vid for vid in video_ids if vid not in train_ids]\n",
    "    return {\n",
    "        'train': train_ids,\n",
    "        'val': val_ids\n",
    "    }\n",
    "def write_list_files(split_data, aligned_files, outputdir=\"/content/final_dl/list\"):\n",
    "    \"\"\"Write list files with audio files repeated to match RGB structure\"\"\"\n",
    "    #os.makedirs(outputdir, exist_ok=True)\n",
    "    for split_name, video_ids in split_data.items():\n",
    "        # RGB list - one entry per frame\n",
    "        rgb_path = os.path.join(outputdir, f'rgb_{split_name}.list')\n",
    "        with open(rgb_path, 'w') as f:\n",
    "            for vid_id in video_ids:\n",
    "                for rgb_file in aligned_files[vid_id]['rgb']:\n",
    "                    f.write(f\"{rgb_file}\\n\")\n",
    "        # Audio list - one entry per video (not per frame)\n",
    "        audio_path = os.path.join(outputdir, f'audio_{split_name}.list')\n",
    "        with open(audio_path, 'w') as f:\n",
    "            for vid_id in video_ids:\n",
    "                audio_file = aligned_files[vid_id]['audio']\n",
    "                f.write(f\"{audio_file}\\n\")  # Write once only\n",
    "\n",
    "def find_matching_files():\n",
    "    \"\"\"\n",
    "    Find and align RGB and audio feature files.\n",
    "    Only includes files that actually exist in the filesystem.\n",
    "    Returns dict mapping video IDs to their RGB and audio paths\n",
    "    \"\"\"\n",
    "    rgb_path = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
    "    audio_path = \"content/final_dl/list/xx/train\"\n",
    "\n",
    "    # Get all files that actually exist\n",
    "    rgb_files = [f for f in glob.glob(os.path.join(rgb_path, \"*.npy\")) if os.path.exists(f)]\n",
    "    audio_files = [f for f in glob.glob(os.path.join(audio_path, \"*.npy\")) if os.path.exists(f)]\n",
    "\n",
    "    # Create mappings that preserve the 5:1 ratio\n",
    "    rgb_map = {}\n",
    "    for f in rgb_files:\n",
    "        vid_id = get_video_id(f)\n",
    "        if vid_id not in rgb_map\n",
    "            rgb_map[vid_id] = []\n",
    "        rgb_map[vid_id].append(f)\n",
    "\n",
    "    # Only include audio files that exist\n",
    "    audio_map = {}\n",
    "    for f in audio_files:\n",
    "        if os.path.exists(f):  # Double check existence\n",
    "            vid_id = get_video_id(f)\n",
    "            audio_map[vid_id] = f\n",
    "\n",
    "    # Find common video IDs and verify each RGB group has exactly 5 files\n",
    "    common_ids = set(rgb_map.keys()) & set(audio_map.keys())\n",
    "    complete_ids = {vid_id for vid_id in common_ids if len(rgb_map[vid_id]) == 5}\n",
    "\n",
    "    # Create aligned mapping only for complete groups\n",
    "    aligned_files = {\n",
    "        vid_id: {\n",
    "            'rgb': sorted(rgb_map[vid_id]),  # Sort to maintain consistent ordering\n",
    "            'audio': audio_map[vid_id],\n",
    "            'is_normal': '_label_A' in rgb_map[vid_id][0]  # Check first RGB file for label\n",
    "        }\n",
    "        for vid_id in complete_ids\n",
    "    }\n",
    "\n",
    "    # Print some diagnostic information\n",
    "    print(f\"Total RGB files found: {len(rgb_files)}\")\n",
    "    print(f\"Total audio files found: {len(audio_files)}\")\n",
    "    print(f\"Video IDs with both RGB and audio: {len(common_ids)}\")\n",
    "    print(f\"Complete aligned pairs (5 RGB + 1 audio): {len(aligned_files)}\")\n",
    "\n",
    "    # Print info about incomplete groups\n",
    "    incomplete_ids = common_ids - complete_ids\n",
    "    if incomplete_ids:\n",
    "        print(\"\\nIncomplete groups:\")\n",
    "        for vid_id in incomplete_ids:\n",
    "            print(f\"{vid_id}: {len(rgb_map[vid_id])} RGB files + 1 audio file\")\n",
    "\n",
    "    return aligned_files\n",
    "\n",
    "\n",
    "aligned_files = find_matching_files()\n",
    "\n",
    "    # Create train/val splits\n",
    "split_data = create_splits(aligned_files)\n",
    "\n",
    "    # Write list files\n",
    "write_list_files(split_data, aligned_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRTYizp5JsSV",
    "outputId": "07db3535-bf02-4562-baed-967dbc1029de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3953 aligned RGB-Audio pairs\n"
     ]
    }
   ],
   "source": [
    "aligned_files = find_matching_files()\n",
    "\n",
    "    # Create train/val splits\n",
    "split_data = create_splits(aligned_files)\n",
    "\n",
    "    # Write list files\n",
    "write_list_files(split_data, aligned_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJzaD9WsT3YV"
   },
   "source": [
    "## Create Dataloaders for multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVCYAPTSTuQY",
    "outputId": "80a5a904-ce51-47bb-bc5a-9f991eec66ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loaders...\n",
      "Train loader created with 15815 samples\n",
      "Validation loader created with 3955 samples\n",
      "Test loader created with 4000 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def create_data_loaders(args):\n",
    "    \"\"\"\n",
    "    Create train, validation and test data loaders\n",
    "    \"\"\"\n",
    "    print(\"Creating data loaders...\")\n",
    "\n",
    "    # Create train loader\n",
    "    train_dataset = Dataset(args, mode='train')\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Train loader created with {len(train_dataset)} samples\")\n",
    "\n",
    "    # Create validation loader\n",
    "    val_dataset = Dataset(args, mode='val')\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,  # No need to shuffle validation data\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Validation loader created with {len(val_dataset)} samples\")\n",
    "\n",
    "    # Create test loader with smaller batch size as per original code\n",
    "    test_dataset = Dataset(args, mode='test')\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=5,  # Using smaller batch size for testing\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Test loader created with {len(test_dataset)} samples\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsN1t5SrTQoU"
   },
   "source": [
    "## VAL SPLIT FOR SINGLE MODALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7aaqgH5TP66"
   },
   "outputs": [],
   "source": [
    "def create_single_modality_data_loaders(args, modality='AUDIO'):\n",
    "    \"\"\"\n",
    "    Create train, validation and test data loaders for a single modality\n",
    "    \"\"\"\n",
    "    print(f\"Creating {modality} data loaders...\")\n",
    "\n",
    "    # Create new args with only needed attributes\n",
    "    args_new = Args()\n",
    "    args_new.modality = modality\n",
    "\n",
    "    # List files needed for train/val/test splits\n",
    "    if modality == 'AUDIO':\n",
    "        args_new.train_audio_list = args.train_audio_list\n",
    "        args_new.val_audio_list = args.val_audio_list\n",
    "        args_new.test_audio_list = args.test_audio_list\n",
    "    elif modality == 'RGB':\n",
    "        args_new.train_rgb_list = args.train_rgb_list\n",
    "        args_new.val_rgb_list = args.val_rgb_list\n",
    "        args_new.test_rgb_list = args.test_rgb_list\n",
    "    elif modality == 'FLOW':\n",
    "        args_new.train_flow_list = args.train_flow_list\n",
    "        args_new.val_flow_list = args.val_flow_list\n",
    "        args_new.test_flow_list = args.test_flow_list\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = Dataset(args_new, mode='train')\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args_new.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args_new.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Train loader created with {len(train_dataset)} samples\")\n",
    "\n",
    "    val_dataset = Dataset(args_new, mode='val')\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args_new.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args_new.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Validation loader created with {len(val_dataset)} samples\")\n",
    "\n",
    "    test_dataset = Dataset(args_new, mode='test')\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=5,\n",
    "        shuffle=False,\n",
    "        num_workers=args_new.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Test loader created with {len(test_dataset)} samples\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNM_LV6qT_q0"
   },
   "source": [
    "## Create data loader for specified modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "OtwEJzeJTr-i"
   },
   "outputs": [],
   "source": [
    "# For audio only\n",
    "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='AUDIO')\n",
    "\n",
    "# For RGB only\n",
    "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='RGB')\n",
    "\n",
    "# For flow only\n",
    "## CURRENTLY, FLOW IS NOT SUPPORTED, BUT IMPLEMENTING IT WOULD NOT BE THAT CHALLENGING. YOU WOULD SIMPLY HAVE TO\n",
    "## ADJUST THE CODE A FEW CELLS ABOVE SO TO WRITE AN EQUIVALENT OF \"find_matching_files\" FOR FLOW DATA\n",
    "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='FLOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "LWZIdanUgWCK"
   },
   "outputs": [],
   "source": [
    "# Testing the val splitter, no need to run this\n",
    "\n",
    "def inspect_batch_files(args, batch_size=1, num_batches=5):\n",
    "    \"\"\"\n",
    "    Inspect the first few batches to see which files are being loaded and their dimensions\n",
    "    \"\"\"\n",
    "    #from torch.utils.data import DataLoader\n",
    "    #from dataset import Dataset  # Your dataset class\n",
    "\n",
    "    dataset = Dataset(args, mode='train')\n",
    "\n",
    "    print(\"Inspecting individual samples:\")\n",
    "    for i in range(min(25, len(dataset))):\n",
    "        try:\n",
    "            # Get the filepaths that would be loaded\n",
    "            rgb_path = dataset.list[i].strip('\\n')\n",
    "            audio_path = dataset.audio_list[i//5].strip('\\n')\n",
    "\n",
    "            # Load the features\n",
    "            features1 = np.array(np.load(rgb_path), dtype=np.float32)\n",
    "            features2 = np.array(np.load(audio_path), dtype=np.float32)\n",
    "\n",
    "            print(f\"\\nSample {i}:\")\n",
    "            print(f\"RGB file: {os.path.basename(rgb_path)}\")\n",
    "            print(f\"RGB shape: {features1.shape}\")\n",
    "            print(f\"Audio file: {os.path.basename(audio_path)}\")\n",
    "            print(f\"Audio shape: {features2.shape}\")\n",
    "\n",
    "            # Try the concatenation\n",
    "            try:\n",
    "                if features1.shape[0] != features2.shape[0]:\n",
    "                    print(\"⚠️ Dimension mismatch!\")\n",
    "                    if features1.shape[0] - 1 == features2.shape[0]:\n",
    "                        print(\"Would work with [:-1] slice\")\n",
    "                    features = np.concatenate((features1[:-1], features2), axis=1)\n",
    "                    print(\"Concatenation successful after adjustment\")\n",
    "            except ValueError as e:\n",
    "                print(f\"❌ Concatenation failed: {str(e)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {i}: {str(e)}\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the inspection\n",
    "inspect_batch_files(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NCyNXLPN5u0"
   },
   "source": [
    "## Testing PreTrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nkT8DU7V3HOz",
    "outputId": "0daa5404-6268-4f41-88a6-74a076ea2c11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-6958b6b31f24>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  {k.replace('module.', ''): v for k, v in torch.load('/content/final_dl/wsanodet_mix2.pkl').items()})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:32.01085138320923\n",
      "offline pr_auc:0.79; online pr_auc:0.7433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "# from model import Model\n",
    "# from dataset import Dataset\n",
    "# from test import test\n",
    "# import option\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  device = torch.device(\"cuda\")\n",
    "\n",
    "  test_loader = DataLoader(Dataset(args, mode='test'),\n",
    "                            batch_size=5, shuffle=False,\n",
    "                            num_workers=args.workers, pin_memory=True)\n",
    "  model = Model(args)\n",
    "  model = model.to(device)\n",
    "  # had to change path to \"/content/final_dl/wsanodet_mix2.pkl\"\n",
    "  model_dict = model.load_state_dict(\n",
    "      {k.replace('module.', ''): v for k, v in torch.load('/content/final_dl/wsanodet_mix2.pkl').items()})\n",
    "\n",
    "  gt = np.load(args.gt)\n",
    "  st = time.time()\n",
    "  pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
    "  print('Time:{}'.format(time.time()-st))\n",
    "  print('offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NIbLkkivMhx"
   },
   "source": [
    "how to save a model for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7mSdUwSvLTq"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"/content/test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0S5vwXjQzID"
   },
   "source": [
    "# Training HLNET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lv0i1DYkQ2Qj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def CLAS(logits, label, seq_len, criterion, device, is_topk=True):\n",
    "    logits = logits.squeeze()\n",
    "    instance_logits = torch.zeros(0).to(device)  # tensor([])\n",
    "    for i in range(logits.shape[0]):\n",
    "        if is_topk:\n",
    "            tmp, _ = torch.topk(logits[i][:seq_len[i]], k=int(seq_len[i]//16+1), largest=True)\n",
    "            tmp = torch.mean(tmp).view(1)\n",
    "        else:\n",
    "            tmp = torch.mean(logits[i, :seq_len[i]]).view(1)\n",
    "        instance_logits = torch.cat((instance_logits, tmp))\n",
    "\n",
    "    instance_logits = torch.sigmoid(instance_logits)\n",
    "\n",
    "    clsloss = criterion(instance_logits, label)\n",
    "    return clsloss\n",
    "\n",
    "\n",
    "def CENTROPY(logits, logits2, seq_len, device):\n",
    "    instance_logits = torch.tensor(0).to(device)  # tensor([])\n",
    "    for i in range(logits.shape[0]):\n",
    "        tmp1 = torch.sigmoid(logits[i, :seq_len[i]]).squeeze()\n",
    "        tmp2 = torch.sigmoid(logits2[i, :seq_len[i]]).squeeze()\n",
    "        loss = torch.mean(-tmp1.detach() * torch.log(tmp2))\n",
    "        instance_logits = instance_logits + loss\n",
    "    instance_logits = instance_logits/logits.shape[0]\n",
    "    return instance_logits\n",
    "\n",
    "\n",
    "def train(dataloader, model, optimizer, criterion, device, is_topk):\n",
    "    with torch.set_grad_enabled(True):\n",
    "        model.train()\n",
    "        for i, (input, label) in enumerate(dataloader):\n",
    "            seq_len = torch.sum(torch.max(torch.abs(input), dim=2)[0]>0, 1)\n",
    "            input = input[:, :torch.max(seq_len), :]\n",
    "            input, label = input.float().to(device), label.float().to(device)\n",
    "            logits, logits2 = model(input, seq_len)\n",
    "            clsloss = CLAS(logits, label, seq_len, criterion, device, is_topk)\n",
    "            clsloss2 = CLAS(logits2, label, seq_len, criterion, device, is_topk)\n",
    "            croloss = CENTROPY(logits, logits2, seq_len, device)\n",
    "\n",
    "            total_loss = clsloss + clsloss2 + 5*croloss\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcLzmEr4HIQY",
    "outputId": "427e5241-2699-4e42-eee2-84abc2c81d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated paths have been written to /content/final_dl/list/audio.list\n"
     ]
    }
   ],
   "source": [
    "# Define the input .list file containing the original file paths\n",
    "input_list_file = \"/content/final_dl/list/audio.list\"\n",
    "\n",
    "# Define the directory to update the paths to\n",
    "new_directory = \"/content/final_dl/list/xx/train\"\n",
    "\n",
    "# Define the output .list file for the updated file paths\n",
    "output_list_file = \"/content/final_dl/list/audio.list\"\n",
    "\n",
    "# Read the original file paths from the .list file\n",
    "with open(input_list_file, \"r\") as file:\n",
    "    original_paths = file.readlines()\n",
    "\n",
    "# Process and update each file path\n",
    "updated_paths = []\n",
    "for path in original_paths:\n",
    "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
    "    if path:  # Ensure the path is not empty\n",
    "        # Extract the filename from the original path and create a new path\n",
    "        filename = path.split(\"/\")[-1]\n",
    "        updated_path = f\"{new_directory}/{filename}\"\n",
    "        updated_paths.append(updated_path)\n",
    "\n",
    "# Write the updated paths to the output .list file\n",
    "with open(output_list_file, \"w\") as file:\n",
    "    file.write(\"\\n\".join(updated_paths))\n",
    "\n",
    "print(f\"Updated paths have been written to {output_list_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mubL6MwpIS8k",
    "outputId": "3b39b7c2-9487-45fe-b7ba-35f55dce2d82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated paths have been written to /content/final_dl/list/rgb.list\n"
     ]
    }
   ],
   "source": [
    "# Define the input .list file containing the original file paths\n",
    "input_list_file = \"/content/final_dl/list/rgb.list\"\n",
    "\n",
    "# Define the directory to update the paths to\n",
    "new_directory = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
    "\n",
    "# Define the output .list file for the updated file paths\n",
    "output_list_file = \"/content/final_dl/list/rgb.list\"\n",
    "\n",
    "# Read the original file paths from the .list file\n",
    "with open(input_list_file, \"r\") as file:\n",
    "    original_paths = file.readlines()\n",
    "\n",
    "# Process and update each file path\n",
    "updated_paths = []\n",
    "for path in original_paths:\n",
    "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
    "    if path:  # Ensure the path is not empty\n",
    "        # Extract the filename from the original path and create a new path\n",
    "        filename = path.split(\"/\")[-1]\n",
    "        updated_path = f\"{new_directory}/{filename}\"\n",
    "        updated_paths.append(updated_path)\n",
    "\n",
    "# Write the updated paths to the output .list file\n",
    "with open(output_list_file, \"w\") as file:\n",
    "    file.write(\"\\n\".join(updated_paths))\n",
    "\n",
    "print(f\"Updated paths have been written to {output_list_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYz5pRU7Iyf8"
   },
   "source": [
    "## test (ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTEZP14qIyOD",
    "outputId": "3067bd70-3ce8-4c53-a00a-78f5f39585d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 200, 1152])\n",
      "torch.Size([5, 200, 1152])\n",
      "torch.Size([5, 200, 1152])\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "  def __init__(self):\n",
    "      self.modality = 'MIX2'\n",
    "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
    "      self.flow_list = '/content/final_dl/list/flow.list'\n",
    "      self.audio_list = '/content/final_dl/list/audio.list'\n",
    "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
    "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
    "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
    "      self.gt = '/content/final_dl/list/gt.npy'\n",
    "      self.gpus = 1\n",
    "      self.lr = 0.0001\n",
    "      self.batch_size = 128\n",
    "      self.workers = 1\n",
    "      self.model_name = 'wsanodet'\n",
    "      self.pretrained_ckpt = None\n",
    "      self.feature_size = 1152  # 1024 + 128\n",
    "      self.num_classes = 1\n",
    "      self.dataset_name = 'XD-Violence'\n",
    "      self.max_seqlen = 200\n",
    "      self.max_epoch = 50\n",
    "\n",
    "  # Create an instance of the Args class\n",
    "args = Args()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(args)\n",
    "model = model.cuda()\n",
    "\n",
    "test_loader = DataLoader(Dataset(args, mode='test'),\n",
    "                          batch_size=5, shuffle=True,\n",
    "                          num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "  for i, (input,label) in enumerate(test_loader):\n",
    "    input = input.to(device)\n",
    "\n",
    "    print(input.shape)\n",
    "    ############\n",
    "    ### NOTE: ## setting seq_len to None pads training data in the sequence dim to 200\n",
    "    ############\n",
    "    logits, logits2 = model(inputs=input, seq_len=None)\n",
    "    # print(logits, logits2)\n",
    "    if i == 2:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM88zu_-ADPw"
   },
   "source": [
    "## Training HL NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "pFJMkujfFROw",
    "outputId": "eb100452-678f-482c-c4c8-e63f28f752a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d1.weight\n",
      "conv1d1.bias\n",
      "conv1d2.weight\n",
      "conv1d2.bias\n",
      "conv1d3.weight\n",
      "conv1d3.bias\n",
      "conv1d4.weight\n",
      "conv1d4.bias\n",
      "gc1.weight\n",
      "gc1.residual.weight\n",
      "gc1.residual.bias\n",
      "gc2.weight\n",
      "gc3.weight\n",
      "gc3.residual.weight\n",
      "gc3.residual.bias\n",
      "gc4.weight\n",
      "gc5.weight\n",
      "gc5.residual.weight\n",
      "gc5.residual.bias\n",
      "gc6.weight\n",
      "simAdj.weight0\n",
      "simAdj.weight1\n",
      "disAdj.sigma\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "approximator.0.weight\n",
      "approximator.0.bias\n",
      "approximator.2.weight\n",
      "approximator.2.bias\n",
      "conv1d_approximator.weight\n",
      "conv1d_approximator.bias\n",
      "Random initalization: offline pr_auc:0.1815; online pr_auc:0.2136\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-17ec0503ccf9>\", line 85, in __getitem__\n    features = np.concatenate((features1[:-1], features2), axis=1)\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 35 and the array at index 1 has size 33\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-6baa33c54768>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_topk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./ckpt/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-92c6eaf947f4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, optimizer, criterion, device, is_topk)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-17ec0503ccf9>\", line 85, in __getitem__\n    features = np.concatenate((features1[:-1], features2), axis=1)\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 35 and the array at index 1 has size 33\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "# import option\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# torch.multiprocessing.set_start_method('spawn')\n",
    "# setup_seed(2333)\n",
    "# args = option.parser.parse_args()\n",
    "\n",
    "!export TORCH_USE_CUDA_DSA=ON\n",
    "device = torch.device(\"cuda\")\n",
    "train_loader = DataLoader(Dataset(args, mode='train'),\n",
    "                          batch_size=args.batch_size, shuffle=True,\n",
    "                          num_workers=args.workers, pin_memory=True)\n",
    "test_loader = DataLoader(Dataset(args, mode='test'),\n",
    "                          batch_size=5, shuffle=False,\n",
    "                          num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(args)\n",
    "model = model.cuda()\n",
    "\n",
    "for name, value in model.named_parameters():\n",
    "    print(name)\n",
    "approximator_param = list(map(id, model.approximator.parameters()))\n",
    "approximator_param += list(map(id, model.conv1d_approximator.parameters()))\n",
    "base_param = filter(lambda p: id(p) not in approximator_param, model.parameters())\n",
    "\n",
    "if not os.path.exists('./ckpt'):\n",
    "    os.makedirs('./ckpt')\n",
    "optimizer = optim.Adam([{'params': base_param},\n",
    "                        {'params': model.approximator.parameters(), 'lr': args.lr / 2},\n",
    "                        {'params': model.conv1d_approximator.parameters(), 'lr': args.lr / 2},\n",
    "                        ],\n",
    "                        lr=args.lr, weight_decay=0.000)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "is_topk = True\n",
    "gt = np.load(args.gt)\n",
    "pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
    "print('Random initalization: offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n",
    "for epoch in range(args.max_epoch):\n",
    "    scheduler.step()\n",
    "    st = time.time()\n",
    "    train(train_loader, model, optimizer, criterion, device, is_topk)\n",
    "    if epoch % 2 == 0 and not epoch == 0:\n",
    "        torch.save(model.state_dict(), './ckpt/'+args.model_name+'{}.pkl'.format(epoch))\n",
    "\n",
    "    pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
    "    print('Epoch {0}/{1}: offline pr_auc:{2:.4}; online pr_auc:{3:.4}\\n'.format(epoch, args.max_epoch, pr_auc, pr_auc_online))\n",
    "torch.save(model.state_dict(), './ckpt/' + args.model_name + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYjtaIacOHyS"
   },
   "source": [
    "# Training VAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRzWrGepsWur"
   },
   "source": [
    "## VAE MODEL\n",
    "\n",
    "Added batch normalization (Gus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "id": "r7Hlyuuos9DQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Sampling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z_means, z_log_vars):\n",
    "        epsilon = torch.randn_like(z_means, dtype=torch.float32)\n",
    "        return z_means + torch.exp(0.5 * z_log_vars) * epsilon\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=1152, seq_len=200):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Reduced number of feature maps in encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 256, kernel_size=3, stride=2, padding=1),  # Reduced from 576\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(256, 128, kernel_size=3, stride=2, padding=1),  # Reduced from 288\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(128, 64, kernel_size=3, stride=2, padding=1),  # Reduced from 144\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        flattened_dim = 64 * 25  # Updated based on reduced features\n",
    "\n",
    "        self.lin_mean = nn.Sequential(\n",
    "            nn.Linear(flattened_dim, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim)\n",
    "        )\n",
    "\n",
    "        self.lin_log_var = nn.Sequential(\n",
    "            nn.Linear(flattened_dim, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim)\n",
    "        )\n",
    "\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.encoder(x)\n",
    "        z_means = self.lin_mean(x)\n",
    "        z_log_vars = self.lin_log_var(x)\n",
    "        z = self.sampling(z_means, z_log_vars)\n",
    "        return z, z_means, z_log_vars\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=1152, seq_len=200):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        flattened_dim = 64 * 25  # Updated based on reduced features\n",
    "\n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, flattened_dim),\n",
    "            nn.BatchNorm1d(flattened_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Reduced number of feature maps in decoder\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # Reduced from 144->288\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(128, 256, kernel_size=3, stride=2, padding=1, output_padding=1),  # Reduced from 288->576\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(256, input_dim, kernel_size=3, stride=2, padding=1, output_padding=1),  # Reduced from 576->input_dim\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder_fc(x)\n",
    "        x = x.view(-1, 64, 25)  # Updated based on reduced features\n",
    "        x = self.decoder_conv(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=1152, seq_len=200):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(latent_dim, input_dim, seq_len)\n",
    "        self.decoder = Decoder(latent_dim, input_dim, seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, z_means, z_log_vars = self.encoder(x)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, z_means, z_log_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "T9Bi_42c8Tts"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "class NormalDataset(data.Dataset):\n",
    "    def __init__(self, args, transform=None, mode='train'):\n",
    "        self.modality = args.modality\n",
    "        self.normal_flag = '_label_A'\n",
    "        self.max_seqlen = args.max_seqlen\n",
    "        self.transform = transform\n",
    "        self.test_mode = (mode == 'test')\n",
    "\n",
    "        # Set appropriate file lists based on mode\n",
    "        if mode == 'test':\n",
    "            self.rgb_list_file = args.test_rgb_list\n",
    "            self.flow_list_file = args.test_flow_list\n",
    "            self.audio_list_file = args.test_audio_list\n",
    "        elif mode == 'val':\n",
    "            self.rgb_list_file = args.val_rgb_list\n",
    "            self.flow_list_file = args.val_flow_list\n",
    "            self.audio_list_file = args.val_audio_list\n",
    "        else:  # train\n",
    "            self.rgb_list_file = args.train_rgb_list\n",
    "            self.flow_list_file = args.train_flow_list\n",
    "            self.audio_list_file = args.train_audio_list\n",
    "\n",
    "        self._parse_list()\n",
    "\n",
    "    def _parse_list(self):\n",
    "        \"\"\"Parse file lists and filter for normal samples only\"\"\"\n",
    "        def filter_normal_samples(file_list):\n",
    "            return [f for f in file_list if self.normal_flag in f]\n",
    "\n",
    "        if self.modality == 'AUDIO':\n",
    "            self.list = filter_normal_samples(list(open(self.audio_list_file)))\n",
    "        elif self.modality == 'RGB':\n",
    "            self.list = filter_normal_samples(list(open(self.rgb_list_file)))\n",
    "        elif self.modality == 'FLOW':\n",
    "            self.list = filter_normal_samples(list(open(self.flow_list_file)))\n",
    "        elif self.modality == 'MIX2':\n",
    "            # For MIX2, we need to handle the 5:1 ratio between RGB and audio\n",
    "            self.list = filter_normal_samples(list(open(self.rgb_list_file)))\n",
    "            # Filter audio list and ensure alignment\n",
    "            all_audio = list(open(self.audio_list_file))\n",
    "            self.audio_list = [f for f in all_audio if self.normal_flag in f]\n",
    "\n",
    "            # Ensure RGB and audio lists are aligned (5:1 ratio)\n",
    "            rgb_video_ids = set([self._get_video_id(f) for f in self.list])\n",
    "            audio_video_ids = set([self._get_video_id(f) for f in self.audio_list])\n",
    "            common_ids = rgb_video_ids & audio_video_ids\n",
    "\n",
    "            # Filter lists to only include common videos\n",
    "            self.list = [f for f in self.list if self._get_video_id(f) in common_ids]\n",
    "            self.audio_list = [f for f in self.audio_list if self._get_video_id(f) in common_ids]\n",
    "\n",
    "    def _get_video_id(self, filepath):\n",
    "        \"\"\"Extract video ID from filepath\"\"\"\n",
    "        filename = os.path.basename(filepath.strip('\\n'))\n",
    "        return filename.split('_label')[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.modality in ['RGB', 'FLOW', 'AUDIO']:\n",
    "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "        elif self.modality == 'MIX2':\n",
    "            # Load RGB features\n",
    "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "            # Load corresponding audio features (accounting for 5:1 ratio)\n",
    "            audio_index = index // 5\n",
    "            features2 = np.array(np.load(self.audio_list[audio_index].strip('\\n')), dtype=np.float32)\n",
    "\n",
    "            # Handle potential dimension mismatch\n",
    "            if features1.shape[0] > features2.shape[0]:\n",
    "                features1 = features1[:features2.shape[0]]\n",
    "            features = np.concatenate((features1, features2), axis=1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        features = process_feat(features, self.max_seqlen, is_random=not self.test_mode)\n",
    "\n",
    "        # Always return label 0 since these are normal samples\n",
    "        return features, 0.0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list)\n",
    "\n",
    "def create_normal_data_loaders(args):\n",
    "    \"\"\"Create data loaders for normal samples only\"\"\"\n",
    "    print(\"Creating normal-only data loaders...\")\n",
    "\n",
    "    # Create train loader\n",
    "    train_dataset = NormalDataset(args, mode='train')\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Normal train loader created with {len(train_dataset)} samples\")\n",
    "\n",
    "    # Create validation loader\n",
    "    val_dataset = NormalDataset(args, mode='val')\n",
    "    val_loader = data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Normal validation loader created with {len(val_dataset)} samples\")\n",
    "\n",
    "    # Create test loader\n",
    "    test_dataset = NormalDataset(args, mode='test')\n",
    "    test_loader = data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Normal test loader created with {len(test_dataset)} samples\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def process_feat(feat, length, is_random=True):\n",
    "    \"\"\"Process features to have consistent length\"\"\"\n",
    "    if len(feat) > length:\n",
    "        if is_random:\n",
    "            r = np.random.randint(len(feat) - length)\n",
    "            return feat[r:r + length]\n",
    "        else:\n",
    "            r = np.linspace(0, len(feat) - 1, length, dtype=np.uint16)\n",
    "            return feat[r, :]\n",
    "    else:\n",
    "        return np.pad(feat, ((0, length - len(feat)), (0, 0)), mode='constant', constant_values=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUfjc_eorOqB"
   },
   "source": [
    "## VAE Training func (Gus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vE72-3Wu258p"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "def validate_vae(vae, val_loader, device):\n",
    "    \"\"\"Run validation loop and return average loss\"\"\"\n",
    "    vae.eval()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in val_loader:\n",
    "            # Only process normal samples (label == 0)\n",
    "            normal_mask = (labels == 0.0)\n",
    "            if not normal_mask.any():\n",
    "                continue\n",
    "\n",
    "            data = data[normal_mask].to(device)\n",
    "            recon_data, mu, logvar = vae(data)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            recon_criterion = torch.nn.MSELoss(reduction='sum')\n",
    "            recon_loss = recon_criterion(recon_data, data)\n",
    "\n",
    "            # KL divergence loss\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            # Total loss\n",
    "            loss = recon_loss + kl_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            n_samples += data.size(0)\n",
    "\n",
    "    # Calculate averages\n",
    "    if n_samples > 0:\n",
    "        avg_loss = total_loss / n_samples\n",
    "        avg_recon = total_recon_loss / n_samples\n",
    "        avg_kl = total_kl_loss / n_samples\n",
    "    else:\n",
    "        avg_loss = float('inf')\n",
    "        avg_recon = float('inf')\n",
    "        avg_kl = float('inf')\n",
    "\n",
    "    vae.train()\n",
    "    return avg_loss, avg_recon, avg_kl\n",
    "\n",
    "def train_vae(vae, train_loader, val_loader, args, save_dir='vae_checkpoints'):\n",
    "    \"\"\"Main training loop for VAE\"\"\"\n",
    "\n",
    "    # Create directory for saving checkpoints\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vae = vae.to(device)\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=args.lr)\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_recon': [],\n",
    "        'train_kl': [],\n",
    "        'val_loss': [],\n",
    "        'val_recon': [],\n",
    "        'val_kl': []\n",
    "    }\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(args.max_epoch):\n",
    "        # Training\n",
    "        vae.train()\n",
    "        train_loss = 0\n",
    "        train_recon = 0\n",
    "        train_kl = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            # Only process normal samples (label == 0)\n",
    "            normal_mask = (labels == 0.0)\n",
    "            if not normal_mask.any():\n",
    "                continue\n",
    "\n",
    "            data = data[normal_mask].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            recon_data, mu, logvar = vae(data)\n",
    "\n",
    "            # Losses\n",
    "            recon_criterion = torch.nn.MSELoss(reduction='sum')\n",
    "            recon_loss = recon_criterion(recon_data, data)\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = recon_loss + kl_loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record losses\n",
    "            train_loss += loss.item()\n",
    "            train_recon += recon_loss.item()\n",
    "            train_kl += kl_loss.item()\n",
    "            n_samples += data.size(0)\n",
    "\n",
    "        # Calculate average training losses\n",
    "        if n_samples > 0:\n",
    "            avg_train_loss = train_loss / n_samples\n",
    "            avg_train_recon = train_recon / n_samples\n",
    "            avg_train_kl = train_kl / n_samples\n",
    "        else:\n",
    "            print(\"Warning: No normal samples in training batch\")\n",
    "            continue\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_recon, val_kl = validate_vae(vae, val_loader, device)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch+1}/{args.max_epoch}:')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}, Recon: {avg_train_recon:.4f}, KL: {avg_train_kl:.4f}')\n",
    "        print(f'Validation - Loss: {val_loss:.4f}, Recon: {val_recon:.4f}, KL: {val_kl:.4f}\\n')\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_recon'].append(avg_train_recon)\n",
    "        history['train_kl'].append(avg_train_kl)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_recon'].append(val_recon)\n",
    "        history['val_kl'].append(val_kl)\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            save_path = os.path.join(save_dir, f'vae_{args.modality}_best_{timestamp}.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': vae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'history': history\n",
    "            }, save_path)\n",
    "            print(f'Saved best model to {save_path}')\n",
    "            torch.save(vae.state_dict(), os.path.join(save_dir, \"best_trained_vae_{args.modality}.pkl\"))\n",
    "\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping(val_loss):\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kfk8AUIdA3tK",
    "outputId": "bd55c387-7698-4ad0-8fd3-48d631892fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating normal-only data loaders...\n",
      "Normal train loader created with 8320 samples\n",
      "Normal validation loader created with 1925 samples\n",
      "Normal test loader created with 1500 samples\n",
      "Epoch 1/500:\n",
      "Training - Loss: 34135.1683, Recon: 34051.2261, KL: 83.9424\n",
      "Validation - Loss: 30960.6184, Recon: 30896.5218, KL: 64.0966\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_220222.pt\n",
      "Epoch 2/500:\n",
      "Training - Loss: 30562.9983, Recon: 30514.4390, KL: 48.5593\n",
      "Validation - Loss: 28685.0128, Recon: 28632.6743, KL: 52.3388\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_220414.pt\n",
      "Epoch 3/500:\n",
      "Training - Loss: 28327.1804, Recon: 28279.1042, KL: 48.0761\n",
      "Validation - Loss: 24782.5798, Recon: 24742.5925, KL: 39.9872\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_220606.pt\n",
      "Epoch 4/500:\n",
      "Training - Loss: 26294.7026, Recon: 26246.1888, KL: 48.5138\n",
      "Validation - Loss: 23610.0536, Recon: 23567.6082, KL: 42.4452\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_220757.pt\n",
      "Epoch 5/500:\n",
      "Training - Loss: 24593.6637, Recon: 24544.4309, KL: 49.2327\n",
      "Validation - Loss: 23564.4960, Recon: 23509.7458, KL: 54.7503\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_220948.pt\n",
      "Epoch 6/500:\n",
      "Training - Loss: 23090.1802, Recon: 23040.9680, KL: 49.2122\n",
      "Validation - Loss: 20868.3426, Recon: 20826.0625, KL: 42.2801\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_221139.pt\n",
      "Epoch 7/500:\n",
      "Training - Loss: 21668.1372, Recon: 21617.9820, KL: 50.1551\n",
      "Validation - Loss: 18868.8573, Recon: 18820.9117, KL: 47.9457\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_221333.pt\n",
      "Epoch 8/500:\n",
      "Training - Loss: 20363.8075, Recon: 20312.8618, KL: 50.9457\n",
      "Validation - Loss: 17515.2390, Recon: 17467.7554, KL: 47.4835\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_221527.pt\n",
      "Epoch 9/500:\n",
      "Training - Loss: 19331.6801, Recon: 19278.7779, KL: 52.9022\n",
      "Validation - Loss: 17613.9605, Recon: 17562.3608, KL: 51.5997\n",
      "\n",
      "Epoch 10/500:\n",
      "Training - Loss: 18387.4932, Recon: 18335.2821, KL: 52.2112\n",
      "Validation - Loss: 17464.5639, Recon: 17412.4529, KL: 52.1109\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_221915.pt\n",
      "Epoch 11/500:\n",
      "Training - Loss: 17397.8840, Recon: 17345.3816, KL: 52.5023\n",
      "Validation - Loss: 15147.3633, Recon: 15104.8017, KL: 42.5616\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_222108.pt\n",
      "Epoch 12/500:\n",
      "Training - Loss: 16503.9151, Recon: 16452.0072, KL: 51.9079\n",
      "Validation - Loss: 15382.8112, Recon: 15332.1083, KL: 50.7030\n",
      "\n",
      "Epoch 13/500:\n",
      "Training - Loss: 15702.5073, Recon: 15651.2295, KL: 51.2778\n",
      "Validation - Loss: 14848.4369, Recon: 14797.6469, KL: 50.7901\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_222458.pt\n",
      "Epoch 14/500:\n",
      "Training - Loss: 15020.6675, Recon: 14968.9099, KL: 51.7577\n",
      "Validation - Loss: 14067.9396, Recon: 14019.3370, KL: 48.6026\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_222655.pt\n",
      "Epoch 15/500:\n",
      "Training - Loss: 14380.3603, Recon: 14327.9769, KL: 52.3833\n",
      "Validation - Loss: 13174.4410, Recon: 13124.3654, KL: 50.0756\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_222848.pt\n",
      "Epoch 16/500:\n",
      "Training - Loss: 13764.8399, Recon: 13711.7036, KL: 53.1363\n",
      "Validation - Loss: 13056.1446, Recon: 13004.1539, KL: 51.9906\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_223043.pt\n",
      "Epoch 17/500:\n",
      "Training - Loss: 13362.4349, Recon: 13308.1930, KL: 54.2420\n",
      "Validation - Loss: 13007.6639, Recon: 12948.5218, KL: 59.1421\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_223237.pt\n",
      "Epoch 18/500:\n",
      "Training - Loss: 12845.6233, Recon: 12790.4368, KL: 55.1865\n",
      "Validation - Loss: 11112.0967, Recon: 11065.7407, KL: 46.3560\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_223428.pt\n",
      "Epoch 19/500:\n",
      "Training - Loss: 12416.2857, Recon: 12360.5851, KL: 55.7006\n",
      "Validation - Loss: 11483.7118, Recon: 11423.4110, KL: 60.3008\n",
      "\n",
      "Epoch 20/500:\n",
      "Training - Loss: 11962.1458, Recon: 11905.8481, KL: 56.2976\n",
      "Validation - Loss: 10759.1300, Recon: 10708.5595, KL: 50.5705\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_223822.pt\n",
      "Epoch 21/500:\n",
      "Training - Loss: 11619.3578, Recon: 11562.4150, KL: 56.9428\n",
      "Validation - Loss: 10553.5992, Recon: 10501.0263, KL: 52.5729\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_224019.pt\n",
      "Epoch 22/500:\n",
      "Training - Loss: 11248.4329, Recon: 11191.3832, KL: 57.0497\n",
      "Validation - Loss: 10351.5013, Recon: 10296.1557, KL: 55.3457\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_224215.pt\n",
      "Epoch 23/500:\n",
      "Training - Loss: 11057.0459, Recon: 10999.3209, KL: 57.7250\n",
      "Validation - Loss: 9944.8573, Recon: 9891.2715, KL: 53.5858\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_224410.pt\n",
      "Epoch 24/500:\n",
      "Training - Loss: 10681.9103, Recon: 10623.6150, KL: 58.2952\n",
      "Validation - Loss: 9734.7174, Recon: 9680.2468, KL: 54.4707\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_224608.pt\n",
      "Epoch 25/500:\n",
      "Training - Loss: 10457.1966, Recon: 10398.4245, KL: 58.7721\n",
      "Validation - Loss: 9730.8383, Recon: 9674.6437, KL: 56.1947\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_224807.pt\n",
      "Epoch 26/500:\n",
      "Training - Loss: 10216.9743, Recon: 10157.3891, KL: 59.5852\n",
      "Validation - Loss: 9384.2721, Recon: 9326.4460, KL: 57.8261\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_225004.pt\n",
      "Epoch 27/500:\n",
      "Training - Loss: 9995.0673, Recon: 9934.7313, KL: 60.3359\n",
      "Validation - Loss: 9301.0892, Recon: 9247.6577, KL: 53.4315\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_225201.pt\n",
      "Epoch 28/500:\n",
      "Training - Loss: 9800.9910, Recon: 9739.9382, KL: 61.0527\n",
      "Validation - Loss: 8886.2202, Recon: 8830.1501, KL: 56.0701\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_225357.pt\n",
      "Epoch 29/500:\n",
      "Training - Loss: 9827.8004, Recon: 9766.1780, KL: 61.6224\n",
      "Validation - Loss: 8894.2217, Recon: 8836.9446, KL: 57.2770\n",
      "\n",
      "Epoch 30/500:\n",
      "Training - Loss: 9517.6466, Recon: 9455.4413, KL: 62.2053\n",
      "Validation - Loss: 8824.5835, Recon: 8762.8391, KL: 61.7443\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_225745.pt\n",
      "Epoch 31/500:\n",
      "Training - Loss: 9348.0178, Recon: 9285.2011, KL: 62.8168\n",
      "Validation - Loss: 8479.6847, Recon: 8421.9441, KL: 57.7406\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_225938.pt\n",
      "Epoch 32/500:\n",
      "Training - Loss: 9193.1098, Recon: 9129.8291, KL: 63.2807\n",
      "Validation - Loss: 8406.7521, Recon: 8348.3003, KL: 58.4518\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_230130.pt\n",
      "Epoch 33/500:\n",
      "Training - Loss: 9105.9493, Recon: 9042.2798, KL: 63.6695\n",
      "Validation - Loss: 8311.2293, Recon: 8251.6295, KL: 59.5999\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_230328.pt\n",
      "Epoch 34/500:\n",
      "Training - Loss: 8974.7880, Recon: 8910.5150, KL: 64.2731\n",
      "Validation - Loss: 8144.1881, Recon: 8086.8358, KL: 57.3522\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_230520.pt\n",
      "Epoch 35/500:\n",
      "Training - Loss: 8841.3030, Recon: 8776.4582, KL: 64.8449\n",
      "Validation - Loss: 8129.7912, Recon: 8070.1256, KL: 59.6655\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_230714.pt\n",
      "Epoch 36/500:\n",
      "Training - Loss: 8739.0024, Recon: 8673.6743, KL: 65.3281\n",
      "Validation - Loss: 7983.6995, Recon: 7921.4844, KL: 62.2151\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_230910.pt\n",
      "Epoch 37/500:\n",
      "Training - Loss: 8677.1322, Recon: 8611.2270, KL: 65.9052\n",
      "Validation - Loss: 7909.3942, Recon: 7851.6601, KL: 57.7341\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_231111.pt\n",
      "Epoch 38/500:\n",
      "Training - Loss: 8633.8946, Recon: 8567.3686, KL: 66.5260\n",
      "Validation - Loss: 8289.5514, Recon: 8218.1629, KL: 71.3886\n",
      "\n",
      "Epoch 39/500:\n",
      "Training - Loss: 8612.2687, Recon: 8544.9342, KL: 67.3345\n",
      "Validation - Loss: 7915.0128, Recon: 7850.6468, KL: 64.3661\n",
      "\n",
      "Epoch 40/500:\n",
      "Training - Loss: 8448.0630, Recon: 8380.0912, KL: 67.9718\n",
      "Validation - Loss: 7842.1176, Recon: 7775.1716, KL: 66.9459\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_231709.pt\n",
      "Epoch 41/500:\n",
      "Training - Loss: 8430.0338, Recon: 8361.3364, KL: 68.6973\n",
      "Validation - Loss: 7782.8059, Recon: 7714.3862, KL: 68.4196\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_231906.pt\n",
      "Epoch 42/500:\n",
      "Training - Loss: 8326.4914, Recon: 8257.2769, KL: 69.2146\n",
      "Validation - Loss: 7642.8923, Recon: 7577.4046, KL: 65.4877\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_232101.pt\n",
      "Epoch 43/500:\n",
      "Training - Loss: 8272.0902, Recon: 8202.3487, KL: 69.7414\n",
      "Validation - Loss: 7593.4155, Recon: 7527.2030, KL: 66.2126\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_232255.pt\n",
      "Epoch 44/500:\n",
      "Training - Loss: 8234.3237, Recon: 8164.1248, KL: 70.1989\n",
      "Validation - Loss: 7566.9775, Recon: 7504.9325, KL: 62.0449\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_232449.pt\n",
      "Epoch 45/500:\n",
      "Training - Loss: 8140.5512, Recon: 8069.8582, KL: 70.6930\n",
      "Validation - Loss: 7521.5578, Recon: 7457.0332, KL: 64.5246\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_232644.pt\n",
      "Epoch 46/500:\n",
      "Training - Loss: 8087.0725, Recon: 8015.8720, KL: 71.2006\n",
      "Validation - Loss: 7389.6371, Recon: 7326.0368, KL: 63.6003\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_232838.pt\n",
      "Epoch 47/500:\n",
      "Training - Loss: 8041.1636, Recon: 7969.4870, KL: 71.6766\n",
      "Validation - Loss: 7394.7341, Recon: 7331.8120, KL: 62.9221\n",
      "\n",
      "Epoch 48/500:\n",
      "Training - Loss: 8042.3731, Recon: 7970.1587, KL: 72.2144\n",
      "Validation - Loss: 7457.9104, Recon: 7385.9591, KL: 71.9514\n",
      "\n",
      "Epoch 49/500:\n",
      "Training - Loss: 7974.5737, Recon: 7901.8389, KL: 72.7348\n",
      "Validation - Loss: 7326.0110, Recon: 7257.8903, KL: 68.1207\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_233427.pt\n",
      "Epoch 50/500:\n",
      "Training - Loss: 7932.0982, Recon: 7858.8691, KL: 73.2291\n",
      "Validation - Loss: 7292.5025, Recon: 7226.2018, KL: 66.3006\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_233620.pt\n",
      "Epoch 51/500:\n",
      "Training - Loss: 7914.9250, Recon: 7841.1556, KL: 73.7694\n",
      "Validation - Loss: 7301.6119, Recon: 7232.5812, KL: 69.0307\n",
      "\n",
      "Epoch 52/500:\n",
      "Training - Loss: 7862.2040, Recon: 7787.9460, KL: 74.2580\n",
      "Validation - Loss: 7358.9172, Recon: 7284.7060, KL: 74.2111\n",
      "\n",
      "Epoch 53/500:\n",
      "Training - Loss: 7839.5136, Recon: 7764.7443, KL: 74.7692\n",
      "Validation - Loss: 7236.5269, Recon: 7167.1856, KL: 69.3413\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_234208.pt\n",
      "Epoch 54/500:\n",
      "Training - Loss: 7813.5244, Recon: 7738.2832, KL: 75.2412\n",
      "Validation - Loss: 7297.2079, Recon: 7222.5631, KL: 74.6447\n",
      "\n",
      "Epoch 55/500:\n",
      "Training - Loss: 7771.0533, Recon: 7695.3372, KL: 75.7161\n",
      "Validation - Loss: 7173.4493, Recon: 7106.3754, KL: 67.0738\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_234617.pt\n",
      "Epoch 56/500:\n",
      "Training - Loss: 7734.8756, Recon: 7658.7185, KL: 76.1571\n",
      "Validation - Loss: 7138.9714, Recon: 7067.6085, KL: 71.3629\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_234823.pt\n",
      "Epoch 57/500:\n",
      "Training - Loss: 7717.0482, Recon: 7640.3982, KL: 76.6499\n",
      "Validation - Loss: 7157.9613, Recon: 7086.3627, KL: 71.5986\n",
      "\n",
      "Epoch 58/500:\n",
      "Training - Loss: 7697.9440, Recon: 7620.8499, KL: 77.0941\n",
      "Validation - Loss: 7098.1714, Recon: 7026.6105, KL: 71.5609\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_235226.pt\n",
      "Epoch 59/500:\n",
      "Training - Loss: 7669.8983, Recon: 7592.4424, KL: 77.4559\n",
      "Validation - Loss: 7117.7067, Recon: 7045.6292, KL: 72.0775\n",
      "\n",
      "Epoch 60/500:\n",
      "Training - Loss: 7658.7250, Recon: 7580.8876, KL: 77.8375\n",
      "Validation - Loss: 7058.5874, Recon: 6990.1631, KL: 68.4243\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241208_235614.pt\n",
      "Epoch 61/500:\n",
      "Training - Loss: 7651.2646, Recon: 7573.0446, KL: 78.2200\n",
      "Validation - Loss: 7095.8841, Recon: 7022.2988, KL: 73.5853\n",
      "\n",
      "Epoch 62/500:\n",
      "Training - Loss: 7647.3390, Recon: 7568.7553, KL: 78.5837\n",
      "Validation - Loss: 7861.8191, Recon: 7761.5063, KL: 100.3128\n",
      "\n",
      "Epoch 63/500:\n",
      "Training - Loss: 7638.2276, Recon: 7559.2054, KL: 79.0222\n",
      "Validation - Loss: 7049.9984, Recon: 6978.6766, KL: 71.3218\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_000203.pt\n",
      "Epoch 64/500:\n",
      "Training - Loss: 7603.4970, Recon: 7524.1159, KL: 79.3811\n",
      "Validation - Loss: 7031.3282, Recon: 6958.1010, KL: 73.2271\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_000356.pt\n",
      "Epoch 65/500:\n",
      "Training - Loss: 7586.8199, Recon: 7507.0844, KL: 79.7355\n",
      "Validation - Loss: 7047.8938, Recon: 6975.7407, KL: 72.1531\n",
      "\n",
      "Epoch 66/500:\n",
      "Training - Loss: 7567.7429, Recon: 7487.6092, KL: 80.1336\n",
      "Validation - Loss: 7014.7967, Recon: 6938.5286, KL: 76.2681\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_000750.pt\n",
      "Epoch 67/500:\n",
      "Training - Loss: 7551.8756, Recon: 7471.3795, KL: 80.4960\n",
      "Validation - Loss: 6985.2422, Recon: 6910.4034, KL: 74.8388\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_000945.pt\n",
      "Epoch 68/500:\n",
      "Training - Loss: 7545.9592, Recon: 7465.0769, KL: 80.8823\n",
      "Validation - Loss: 7842.7866, Recon: 7742.9370, KL: 99.8495\n",
      "\n",
      "Epoch 69/500:\n",
      "Training - Loss: 7578.7163, Recon: 7497.5150, KL: 81.2012\n",
      "Validation - Loss: 7000.7747, Recon: 6925.0419, KL: 75.7328\n",
      "\n",
      "Epoch 70/500:\n",
      "Training - Loss: 7519.3685, Recon: 7437.8059, KL: 81.5627\n",
      "Validation - Loss: 6962.6827, Recon: 6885.9081, KL: 76.7746\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_001546.pt\n",
      "Epoch 71/500:\n",
      "Training - Loss: 7510.8276, Recon: 7428.9131, KL: 81.9145\n",
      "Validation - Loss: 6935.5477, Recon: 6859.6570, KL: 75.8907\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_001747.pt\n",
      "Epoch 72/500:\n",
      "Training - Loss: 7481.5109, Recon: 7399.2116, KL: 82.2993\n",
      "Validation - Loss: 6933.9642, Recon: 6855.2536, KL: 78.7107\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_001943.pt\n",
      "Epoch 73/500:\n",
      "Training - Loss: 7473.8741, Recon: 7391.2300, KL: 82.6441\n",
      "Validation - Loss: 6921.0756, Recon: 6842.9636, KL: 78.1120\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_002137.pt\n",
      "Epoch 74/500:\n",
      "Training - Loss: 7464.9333, Recon: 7381.9664, KL: 82.9669\n",
      "Validation - Loss: 6922.3837, Recon: 6845.0495, KL: 77.3343\n",
      "\n",
      "Epoch 75/500:\n",
      "Training - Loss: 7466.3606, Recon: 7383.0448, KL: 83.3158\n",
      "Validation - Loss: 6939.4739, Recon: 6861.7907, KL: 77.6832\n",
      "\n",
      "Epoch 76/500:\n",
      "Training - Loss: 7472.5909, Recon: 7388.9383, KL: 83.6527\n",
      "Validation - Loss: 6905.3978, Recon: 6826.3622, KL: 79.0357\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_002735.pt\n",
      "Epoch 77/500:\n",
      "Training - Loss: 7431.6906, Recon: 7347.6682, KL: 84.0223\n",
      "Validation - Loss: 6912.4276, Recon: 6833.8532, KL: 78.5744\n",
      "\n",
      "Epoch 78/500:\n",
      "Training - Loss: 7429.2369, Recon: 7344.8447, KL: 84.3922\n",
      "Validation - Loss: 6914.5232, Recon: 6832.4403, KL: 82.0829\n",
      "\n",
      "Epoch 79/500:\n",
      "Training - Loss: 7412.5658, Recon: 7327.8662, KL: 84.6996\n",
      "Validation - Loss: 6881.6089, Recon: 6802.5741, KL: 79.0348\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_003335.pt\n",
      "Epoch 80/500:\n",
      "Training - Loss: 7411.2779, Recon: 7326.3060, KL: 84.9719\n",
      "Validation - Loss: 6881.6665, Recon: 6802.8761, KL: 78.7905\n",
      "\n",
      "Epoch 81/500:\n",
      "Training - Loss: 7398.2283, Recon: 7312.9642, KL: 85.2641\n",
      "Validation - Loss: 6869.6588, Recon: 6789.3576, KL: 80.3012\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_003731.pt\n",
      "Epoch 82/500:\n",
      "Training - Loss: 7394.0980, Recon: 7308.5367, KL: 85.5613\n",
      "Validation - Loss: 7048.5754, Recon: 6962.7011, KL: 85.8743\n",
      "\n",
      "Epoch 83/500:\n",
      "Training - Loss: 7397.1550, Recon: 7311.4040, KL: 85.7510\n",
      "Validation - Loss: 6860.8734, Recon: 6780.4066, KL: 80.4668\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_004126.pt\n",
      "Epoch 84/500:\n",
      "Training - Loss: 7379.6055, Recon: 7293.4931, KL: 86.1124\n",
      "Validation - Loss: 6883.3199, Recon: 6802.8611, KL: 80.4588\n",
      "\n",
      "Epoch 85/500:\n",
      "Training - Loss: 7364.3736, Recon: 7277.9359, KL: 86.4376\n",
      "Validation - Loss: 6837.5227, Recon: 6756.9040, KL: 80.6188\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_004530.pt\n",
      "Epoch 86/500:\n",
      "Training - Loss: 7357.5482, Recon: 7270.7885, KL: 86.7598\n",
      "Validation - Loss: 6866.4016, Recon: 6781.4101, KL: 84.9915\n",
      "\n",
      "Epoch 87/500:\n",
      "Training - Loss: 7352.0442, Recon: 7264.9713, KL: 87.0729\n",
      "Validation - Loss: 6905.8062, Recon: 6821.7029, KL: 84.1034\n",
      "\n",
      "Epoch 88/500:\n",
      "Training - Loss: 7347.5205, Recon: 7260.1210, KL: 87.3996\n",
      "Validation - Loss: 6866.8692, Recon: 6783.4215, KL: 83.4477\n",
      "\n",
      "Epoch 89/500:\n",
      "Training - Loss: 7347.5758, Recon: 7259.8537, KL: 87.7221\n",
      "Validation - Loss: 6880.2991, Recon: 6796.4711, KL: 83.8280\n",
      "\n",
      "Epoch 90/500:\n",
      "Training - Loss: 7333.5233, Recon: 7245.4410, KL: 88.0823\n",
      "Validation - Loss: 6828.6975, Recon: 6745.1169, KL: 83.5805\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_005528.pt\n",
      "Epoch 91/500:\n",
      "Training - Loss: 7330.3000, Recon: 7241.8428, KL: 88.4572\n",
      "Validation - Loss: 6873.3797, Recon: 6788.7103, KL: 84.6693\n",
      "\n",
      "Epoch 92/500:\n",
      "Training - Loss: 7314.5890, Recon: 7225.7970, KL: 88.7919\n",
      "Validation - Loss: 6809.2795, Recon: 6726.7608, KL: 82.5187\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_005942.pt\n",
      "Epoch 93/500:\n",
      "Training - Loss: 7315.0120, Recon: 7225.9126, KL: 89.0995\n",
      "Validation - Loss: 7292.4401, Recon: 7201.8504, KL: 90.5897\n",
      "\n",
      "Epoch 94/500:\n",
      "Training - Loss: 7320.3006, Recon: 7231.0286, KL: 89.2720\n",
      "Validation - Loss: 6809.6065, Recon: 6726.2705, KL: 83.3360\n",
      "\n",
      "Epoch 95/500:\n",
      "Training - Loss: 7308.3325, Recon: 7218.7633, KL: 89.5691\n",
      "Validation - Loss: 6821.0078, Recon: 6736.2414, KL: 84.7664\n",
      "\n",
      "Epoch 96/500:\n",
      "Training - Loss: 7300.0044, Recon: 7210.1265, KL: 89.8779\n",
      "Validation - Loss: 6781.5551, Recon: 6696.6467, KL: 84.9085\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_010748.pt\n",
      "Epoch 97/500:\n",
      "Training - Loss: 7277.2864, Recon: 7187.1071, KL: 90.1793\n",
      "Validation - Loss: 6790.3789, Recon: 6706.6126, KL: 83.7663\n",
      "\n",
      "Epoch 98/500:\n",
      "Training - Loss: 7286.2124, Recon: 7195.7503, KL: 90.4621\n",
      "Validation - Loss: 6779.6552, Recon: 6694.8584, KL: 84.7967\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_011137.pt\n",
      "Epoch 99/500:\n",
      "Training - Loss: 7258.4718, Recon: 7167.7659, KL: 90.7058\n",
      "Validation - Loss: 6786.8783, Recon: 6700.0126, KL: 86.8656\n",
      "\n",
      "Epoch 100/500:\n",
      "Training - Loss: 7275.0875, Recon: 7184.1007, KL: 90.9869\n",
      "Validation - Loss: 6794.8689, Recon: 6707.9928, KL: 86.8761\n",
      "\n",
      "Epoch 101/500:\n",
      "Training - Loss: 7239.1174, Recon: 7147.8413, KL: 91.2761\n",
      "Validation - Loss: 6788.3857, Recon: 6701.8323, KL: 86.5535\n",
      "\n",
      "Epoch 102/500:\n",
      "Training - Loss: 7259.7515, Recon: 7168.1877, KL: 91.5638\n",
      "Validation - Loss: 6776.3552, Recon: 6691.6158, KL: 84.7394\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_011941.pt\n",
      "Epoch 103/500:\n",
      "Training - Loss: 7244.6641, Recon: 7152.8195, KL: 91.8446\n",
      "Validation - Loss: 6798.8473, Recon: 6711.1429, KL: 87.7044\n",
      "\n",
      "Epoch 104/500:\n",
      "Training - Loss: 7241.6128, Recon: 7149.5125, KL: 92.1004\n",
      "Validation - Loss: 6764.8471, Recon: 6678.8606, KL: 85.9865\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_012336.pt\n",
      "Epoch 105/500:\n",
      "Training - Loss: 7239.6505, Recon: 7147.2701, KL: 92.3804\n",
      "Validation - Loss: 6871.2840, Recon: 6780.4112, KL: 90.8728\n",
      "\n",
      "Epoch 106/500:\n",
      "Training - Loss: 7235.1494, Recon: 7142.5749, KL: 92.5745\n",
      "Validation - Loss: 6755.1967, Recon: 6668.0051, KL: 87.1916\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_012741.pt\n",
      "Epoch 107/500:\n",
      "Training - Loss: 7298.9050, Recon: 7206.2317, KL: 92.6733\n",
      "Validation - Loss: 6777.7869, Recon: 6692.5935, KL: 85.1934\n",
      "\n",
      "Epoch 108/500:\n",
      "Training - Loss: 7237.7850, Recon: 7144.9600, KL: 92.8250\n",
      "Validation - Loss: 6767.2664, Recon: 6678.9880, KL: 88.2784\n",
      "\n",
      "Epoch 109/500:\n",
      "Training - Loss: 7233.4938, Recon: 7140.4521, KL: 93.0417\n",
      "Validation - Loss: 6737.3251, Recon: 6649.1066, KL: 88.2184\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_013329.pt\n",
      "Epoch 110/500:\n",
      "Training - Loss: 7205.3483, Recon: 7112.0514, KL: 93.2969\n",
      "Validation - Loss: 6761.4670, Recon: 6672.6437, KL: 88.8233\n",
      "\n",
      "Epoch 111/500:\n",
      "Training - Loss: 7204.3705, Recon: 7110.8232, KL: 93.5473\n",
      "Validation - Loss: 6717.0417, Recon: 6628.1364, KL: 88.9052\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_013721.pt\n",
      "Epoch 112/500:\n",
      "Training - Loss: 7215.0736, Recon: 7121.3068, KL: 93.7667\n",
      "Validation - Loss: 6715.5006, Recon: 6627.9593, KL: 87.5412\n",
      "\n",
      "Saved best model to vae_checkpoints/vae_best_20241209_013915.pt\n",
      "Epoch 113/500:\n",
      "Training - Loss: 7199.7201, Recon: 7105.7500, KL: 93.9701\n",
      "Validation - Loss: 6732.7766, Recon: 6644.2745, KL: 88.5021\n",
      "\n",
      "Epoch 114/500:\n",
      "Training - Loss: 7203.6307, Recon: 7109.4253, KL: 94.2054\n",
      "Validation - Loss: 6737.3940, Recon: 6647.4589, KL: 89.9351\n",
      "\n",
      "Epoch 115/500:\n",
      "Training - Loss: 7195.9886, Recon: 7101.5945, KL: 94.3941\n",
      "Validation - Loss: 6726.3522, Recon: 6637.7752, KL: 88.5770\n",
      "\n",
      "Epoch 116/500:\n",
      "Training - Loss: 7190.5350, Recon: 7095.9529, KL: 94.5821\n",
      "Validation - Loss: 6747.4838, Recon: 6657.7854, KL: 89.6984\n",
      "\n",
      "Epoch 117/500:\n",
      "Training - Loss: 7177.9691, Recon: 7083.2190, KL: 94.7502\n",
      "Validation - Loss: 6761.8224, Recon: 6671.2791, KL: 90.5432\n",
      "\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# adjust args needed for VAE\n",
    "args_vae = Args()\n",
    "args_vae.feature_size = 1152  # 1024 (RGB) + 128 (audio)\n",
    "args_vae.batch_size = 64\n",
    "args_vae.modality = 'MIX2'\n",
    "args_vae.max_epoch = 500\n",
    "args_vae.lr = 0.0005\n",
    "\n",
    "# initialize VAE with correct input dimension\n",
    "vae = VAE(latent_dim=64, input_dim=args_vae.feature_size, seq_len=200)\n",
    "\n",
    "# Create normal-only dataloaders\n",
    "normal_train_loader, normal_val_loader, normal_test_loader = create_normal_data_loaders(args_vae)\n",
    "\n",
    "# Train the VAE\n",
    "trained_vae = train_vae(vae, normal_train_loader, normal_val_loader, args_vae)\n",
    "# Save the model\n",
    "torch.save(trained_vae.state_dict(), \"/content/last_trained_vae.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWA-0KtnE-y0",
    "outputId": "adcffa06-469e-4e29-9ef7-b03dcacce021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting dataset with MIX2 modality...\n",
      "Creating NormalDataset...\n",
      "Total number of samples in dataset: 8250\n",
      "\n",
      "Number of files in main list: 8250\n",
      "Number of files in audio list: 1650\n",
      "\n",
      "Inspecting individual samples:\n",
      "\n",
      "Sample 0:\n",
      "RGB file: One.Day.2011__#00-20-35_00-23-06_label_A__0.npy\n",
      "RGB shape: (226, 1024)\n",
      "Audio file: One.Day.2011__#00-20-35_00-23-06_label_A__vggish.npy\n",
      "Audio shape: (226, 128)\n",
      "Concatenated shape: (226, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "RGB file: One.Day.2011__#00-20-35_00-23-06_label_A__1.npy\n",
      "RGB shape: (226, 1024)\n",
      "Audio file: One.Day.2011__#00-20-35_00-23-06_label_A__vggish.npy\n",
      "Audio shape: (226, 128)\n",
      "Concatenated shape: (226, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "RGB file: One.Day.2011__#00-20-35_00-23-06_label_A__2.npy\n",
      "RGB shape: (226, 1024)\n",
      "Audio file: One.Day.2011__#00-20-35_00-23-06_label_A__vggish.npy\n",
      "Audio shape: (226, 128)\n",
      "Concatenated shape: (226, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "RGB file: One.Day.2011__#00-20-35_00-23-06_label_A__3.npy\n",
      "RGB shape: (226, 1024)\n",
      "Audio file: One.Day.2011__#00-20-35_00-23-06_label_A__vggish.npy\n",
      "Audio shape: (226, 128)\n",
      "Concatenated shape: (226, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 4:\n",
      "RGB file: One.Day.2011__#00-20-35_00-23-06_label_A__4.npy\n",
      "RGB shape: (226, 1024)\n",
      "Audio file: One.Day.2011__#00-20-35_00-23-06_label_A__vggish.npy\n",
      "Audio shape: (226, 128)\n",
      "Concatenated shape: (226, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 5:\n",
      "RGB file: v=LDAwF7mnYE8__#1_label_A__0.npy\n",
      "RGB shape: (722, 1024)\n",
      "Audio file: v=LDAwF7mnYE8__#1_label_A__vggish.npy\n",
      "Audio shape: (722, 128)\n",
      "Concatenated shape: (722, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 6:\n",
      "RGB file: v=LDAwF7mnYE8__#1_label_A__1.npy\n",
      "RGB shape: (722, 1024)\n",
      "Audio file: v=LDAwF7mnYE8__#1_label_A__vggish.npy\n",
      "Audio shape: (722, 128)\n",
      "Concatenated shape: (722, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 7:\n",
      "RGB file: v=LDAwF7mnYE8__#1_label_A__2.npy\n",
      "RGB shape: (722, 1024)\n",
      "Audio file: v=LDAwF7mnYE8__#1_label_A__vggish.npy\n",
      "Audio shape: (722, 128)\n",
      "Concatenated shape: (722, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 8:\n",
      "RGB file: v=LDAwF7mnYE8__#1_label_A__3.npy\n",
      "RGB shape: (722, 1024)\n",
      "Audio file: v=LDAwF7mnYE8__#1_label_A__vggish.npy\n",
      "Audio shape: (722, 128)\n",
      "Concatenated shape: (722, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 9:\n",
      "RGB file: v=LDAwF7mnYE8__#1_label_A__4.npy\n",
      "RGB shape: (722, 1024)\n",
      "Audio file: v=LDAwF7mnYE8__#1_label_A__vggish.npy\n",
      "Audio shape: (722, 128)\n",
      "Concatenated shape: (722, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 10:\n",
      "RGB file: v=enYITYwvPAQ__#00-09-00_00-12-00_label_A__0.npy\n",
      "RGB shape: (270, 1024)\n",
      "Audio file: v=enYITYwvPAQ__#00-09-00_00-12-00_label_A__vggish.npy\n",
      "Audio shape: (270, 128)\n",
      "Concatenated shape: (270, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 11:\n",
      "RGB file: v=enYITYwvPAQ__#00-09-00_00-12-00_label_A__1.npy\n",
      "RGB shape: (270, 1024)\n",
      "Audio file: v=enYITYwvPAQ__#00-09-00_00-12-00_label_A__vggish.npy\n",
      "Audio shape: (270, 128)\n",
      "Concatenated shape: (270, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 12:\n",
      "RGB file: v=enYITYwvPAQ__#00-09-00_00-12-00_label_A__2.npy\n",
      "RGB shape: (270, 1024)\n",
      "Audio file: v=enYITYwvPAQ__#00-09-00_00-12-00_label_A__vggish.npy\n",
      "Audio shape: (270, 128)\n",
      "Concatenated shape: (270, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 13:\n",
      "RGB file: v=enYITYwvPAQ__#00-09-00_00-12-00_label_A__3.npy\n",
      "RGB shape: (270, 1024)\n",
      "Audio file: v=enYITYwvPAQ__#00-09-00_00-12-00_label_A__vggish.npy\n",
      "Audio shape: (270, 128)\n",
      "Concatenated shape: (270, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 14:\n",
      "RGB file: v=enYITYwvPAQ__#00-09-00_00-12-00_label_A__4.npy\n",
      "RGB shape: (270, 1024)\n",
      "Audio file: v=enYITYwvPAQ__#00-09-00_00-12-00_label_A__vggish.npy\n",
      "Audio shape: (270, 128)\n",
      "Concatenated shape: (270, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 15:\n",
      "RGB file: v=MEJxdpOILcw__#1_label_A__0.npy\n",
      "RGB shape: (288, 1024)\n",
      "Audio file: v=MEJxdpOILcw__#1_label_A__vggish.npy\n",
      "Audio shape: (288, 128)\n",
      "Concatenated shape: (288, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 16:\n",
      "RGB file: v=MEJxdpOILcw__#1_label_A__1.npy\n",
      "RGB shape: (288, 1024)\n",
      "Audio file: v=MEJxdpOILcw__#1_label_A__vggish.npy\n",
      "Audio shape: (288, 128)\n",
      "Concatenated shape: (288, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 17:\n",
      "RGB file: v=MEJxdpOILcw__#1_label_A__2.npy\n",
      "RGB shape: (288, 1024)\n",
      "Audio file: v=MEJxdpOILcw__#1_label_A__vggish.npy\n",
      "Audio shape: (288, 128)\n",
      "Concatenated shape: (288, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 18:\n",
      "RGB file: v=MEJxdpOILcw__#1_label_A__3.npy\n",
      "RGB shape: (288, 1024)\n",
      "Audio file: v=MEJxdpOILcw__#1_label_A__vggish.npy\n",
      "Audio shape: (288, 128)\n",
      "Concatenated shape: (288, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 19:\n",
      "RGB file: v=MEJxdpOILcw__#1_label_A__4.npy\n",
      "RGB shape: (288, 1024)\n",
      "Audio file: v=MEJxdpOILcw__#1_label_A__vggish.npy\n",
      "Audio shape: (288, 128)\n",
      "Concatenated shape: (288, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 20:\n",
      "RGB file: v=3ReHk7UqqhA__#1_label_A__0.npy\n",
      "RGB shape: (241, 1024)\n",
      "Audio file: v=3ReHk7UqqhA__#1_label_A__vggish.npy\n",
      "Audio shape: (241, 128)\n",
      "Concatenated shape: (241, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 21:\n",
      "RGB file: v=3ReHk7UqqhA__#1_label_A__1.npy\n",
      "RGB shape: (241, 1024)\n",
      "Audio file: v=3ReHk7UqqhA__#1_label_A__vggish.npy\n",
      "Audio shape: (241, 128)\n",
      "Concatenated shape: (241, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 22:\n",
      "RGB file: v=3ReHk7UqqhA__#1_label_A__2.npy\n",
      "RGB shape: (241, 1024)\n",
      "Audio file: v=3ReHk7UqqhA__#1_label_A__vggish.npy\n",
      "Audio shape: (241, 128)\n",
      "Concatenated shape: (241, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 23:\n",
      "RGB file: v=3ReHk7UqqhA__#1_label_A__3.npy\n",
      "RGB shape: (241, 1024)\n",
      "Audio file: v=3ReHk7UqqhA__#1_label_A__vggish.npy\n",
      "Audio shape: (241, 128)\n",
      "Concatenated shape: (241, 1152)\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 24:\n",
      "RGB file: v=3ReHk7UqqhA__#1_label_A__4.npy\n",
      "RGB shape: (241, 1024)\n",
      "Audio file: v=3ReHk7UqqhA__#1_label_A__vggish.npy\n",
      "Audio shape: (241, 128)\n",
      "Concatenated shape: (241, 1152)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def inspect_normal_dataset(args, num_samples=25):\n",
    "    \"\"\"\n",
    "    Inspect the normal samples being loaded from the dataset\n",
    "    \"\"\"\n",
    "    print(\"Creating NormalDataset...\")\n",
    "    dataset = NormalDataset(args, mode='train')\n",
    "    print(f\"Total number of samples in dataset: {len(dataset)}\")\n",
    "    print(f\"\\nNumber of files in main list: {len(dataset.list)}\")\n",
    "    if hasattr(dataset, 'audio_list'):\n",
    "        print(f\"Number of files in audio list: {len(dataset.audio_list)}\")\n",
    "\n",
    "    print(\"\\nInspecting individual samples:\")\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        try:\n",
    "            # Get the filepaths that would be loaded\n",
    "            rgb_path = dataset.list[i].strip('\\n')\n",
    "            print(f\"\\nSample {i}:\")\n",
    "            print(f\"RGB file: {os.path.basename(rgb_path)}\")\n",
    "\n",
    "            # Load RGB features\n",
    "            features1 = np.array(np.load(rgb_path), dtype=np.float32)\n",
    "            print(f\"RGB shape: {features1.shape}\")\n",
    "\n",
    "            # If MIX2 modality, also show audio information\n",
    "            if dataset.modality == 'MIX2':\n",
    "                audio_index = i // 5\n",
    "                if audio_index < len(dataset.audio_list):\n",
    "                    audio_path = dataset.audio_list[audio_index].strip('\\n')\n",
    "                    print(f\"Audio file: {os.path.basename(audio_path)}\")\n",
    "                    features2 = np.array(np.load(audio_path), dtype=np.float32)\n",
    "                    print(f\"Audio shape: {features2.shape}\")\n",
    "\n",
    "                    # Try the concatenation\n",
    "                    try:\n",
    "                        if features1.shape[0] != features2.shape[0]:\n",
    "                            print(\"⚠️ Dimension mismatch!\")\n",
    "                            print(f\"RGB frames: {features1.shape[0]}, Audio frames: {features2.shape[0]}\")\n",
    "                            if features1.shape[0] - 1 == features2.shape[0]:\n",
    "                                print(\"Would work with [:-1] slice\")\n",
    "                                features = np.concatenate((features1[:-1], features2), axis=1)\n",
    "                                print(f\"Concatenated shape after adjustment: {features.shape}\")\n",
    "                            else:\n",
    "                                print(\"Cannot be fixed with simple slice\")\n",
    "                        else:\n",
    "                            features = np.concatenate((features1, features2), axis=1)\n",
    "                            print(f\"Concatenated shape: {features.shape}\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"❌ Concatenation failed: {str(e)}\")\n",
    "                else:\n",
    "                    print(\"⚠️ No corresponding audio file (index out of range)\")\n",
    "\n",
    "            # Check if this is actually a normal sample\n",
    "            if '_label_A' not in rgb_path:\n",
    "                print(\"⚠️ WARNING: This doesn't appear to be a normal sample!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {i}: {str(e)}\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the inspection\n",
    "print(\"Inspecting dataset with MIX2 modality...\")\n",
    "args_check = args_vae\n",
    "args_check.modality = 'MIX2'\n",
    "args_check.batch_size = 64  # Smaller batch size for inspection\n",
    "inspect_normal_dataset(args_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0x7LRQ1OEV7a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
