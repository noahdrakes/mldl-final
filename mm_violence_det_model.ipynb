{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/noahdrakes/mldl-final/blob/main/mm_violence_det_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDOr6KirJvUe"
   },
   "source": [
    "# Multi-Modal Violence Detection Network\n",
    "\n",
    "original src code: https://github.com/Roc-Ng/XDVioDet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhzPvDXTKKqM"
   },
   "source": [
    "### Copying Training and Testing Data\n",
    "\n",
    "The folders are pretty large (~40/50GB) so it takes a while to copy all of the data over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4DG4K3wYK8P",
    "outputId": "8ddea142-823d-4f47-a04e-17e1b3c4d4cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /mydrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/mydrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88GWL7hscvcg",
    "outputId": "2b6bfcc6-8d7f-4796-f72c-e9dc85f6caff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mydrive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "%cd /mydrive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Nr7SpsS-2P6x"
   },
   "outputs": [],
   "source": [
    "!unzip final_dl.zip -d /content/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zWKH9vsRatb"
   },
   "source": [
    "may need to change directory depending on where you upload the data to google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CHQ29LtvAsNj"
   },
   "outputs": [],
   "source": [
    "# !cp -r /mydrive/MyDrive/final_dl ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N1LDIKfKhpP"
   },
   "source": [
    "## 1. Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9acUBEhKqjC"
   },
   "source": [
    "### A) Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "IPMWSbm0LBGY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def test(dataloader, model, device, gt):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred = torch.zeros(0).to(device)\n",
    "        pred2 = torch.zeros(0).to(device)\n",
    "        for i, input in enumerate(dataloader):\n",
    "            input = input.to(device)\n",
    "            logits, logits2 = model(inputs=input, seq_len=None)\n",
    "            logits = torch.squeeze(logits)\n",
    "            sig = torch.sigmoid(logits)\n",
    "            sig = torch.mean(sig, 0)\n",
    "            pred = torch.cat((pred, sig))\n",
    "            '''\n",
    "            online detection\n",
    "            '''\n",
    "            logits2 = torch.squeeze(logits2)\n",
    "            sig2 = torch.sigmoid(logits2)\n",
    "            sig2 = torch.mean(sig2, 0)\n",
    "\n",
    "            sig2 = torch.unsqueeze(sig2, 1) ##for audio\n",
    "            pred2 = torch.cat((pred2, sig2))\n",
    "\n",
    "            # print(\"pred:, \", pred)\n",
    "            # print(\"pred2:, \", pred2)\n",
    "\n",
    "        pred = list(pred.cpu().detach().numpy())\n",
    "        pred2 = list(pred2.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred, 16))\n",
    "        pr_auc = auc(recall, precision)\n",
    "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred2, 16))\n",
    "        pr_auc2 = auc(recall, precision)\n",
    "        return pr_auc, pr_auc2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLV269m9K7L3"
   },
   "source": [
    "### B) Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ej2MARCmKzvk"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def random_extract(feat, t_max):\n",
    "   r = np.random.randint(len(feat)-t_max)\n",
    "   return feat[r:r+t_max]\n",
    "\n",
    "def uniform_extract(feat, t_max):\n",
    "   r = np.linspace(0, len(feat)-1, t_max, dtype=np.uint16)\n",
    "   return feat[r, :]\n",
    "\n",
    "def pad(feat, min_len):\n",
    "    if np.shape(feat)[0] <= min_len:\n",
    "       return np.pad(feat, ((0, min_len-np.shape(feat)[0]), (0, 0)), mode='constant', constant_values=0)\n",
    "    else:\n",
    "       return feat\n",
    "\n",
    "def process_feat(feat, length, is_random=True):\n",
    "    if len(feat) > length:\n",
    "        if is_random:\n",
    "            return random_extract(feat, length)\n",
    "        else:\n",
    "            return uniform_extract(feat, length)\n",
    "    else:\n",
    "        return pad(feat, length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkKIYnHpLDKS"
   },
   "source": [
    "### C) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98z1xwOKKueM"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "# from utils import process_feat\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, args, transform=None, mode='train'):\n",
    "        self.modality = args.modality\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args: Arguments containing dataset paths and configuration\n",
    "            transform: Optional transforms to apply\n",
    "            mode: One of ['train', 'val', 'test'] to specify the dataset split\n",
    "        \"\"\"\n",
    "\n",
    "        if mode == 'test':\n",
    "            self.rgb_list_file = args.test_rgb_list\n",
    "            self.flow_list_file = args.test_flow_list\n",
    "            self.audio_list_file = args.test_audio_list\n",
    "        elif mode == 'val':\n",
    "            self.rgb_list_file = args.val_rgb_list\n",
    "            self.flow_list_file = args.val_flow_list\n",
    "            self.audio_list_file = args.val_audio_list\n",
    "        else: # train\n",
    "            self.rgb_list_file = args.train_rgb_list\n",
    "            self.flow_list_file = args.train_flow_list\n",
    "            self.audio_list_file = args.train_audio_list\n",
    "\n",
    "        self.max_seqlen = args.max_seqlen\n",
    "        self.tranform = transform\n",
    "        self.test_mode = (mode == 'test')\n",
    "        self.normal_flag = '_label_A'\n",
    "        self._parse_list()\n",
    "\n",
    "    def _parse_list(self):\n",
    "        if self.modality == 'AUDIO':\n",
    "            self.list = list(open(self.audio_list_file))\n",
    "        elif self.modality == 'RGB':\n",
    "            self.list = list(open(self.rgb_list_file))\n",
    "            print(\"here\")\n",
    "            # print(self.list)\n",
    "        elif self.modality == 'FLOW':\n",
    "            self.list = list(open(self.flow_list_file))\n",
    "        elif self.modality == 'MIX':\n",
    "            self.list = list(open(self.rgb_list_file))\n",
    "            self.flow_list = list(open(self.flow_list_file))\n",
    "        elif self.modality == 'MIX2':\n",
    "            self.list = list(open(self.rgb_list_file))\n",
    "            self.audio_list = list(open(self.audio_list_file))\n",
    "        elif self.modality == 'MIX3':\n",
    "            self.list = list(open(self.flow_list_file))\n",
    "            self.audio_list = list(open(self.audio_list_file))\n",
    "        elif self.modality == 'MIX_ALL':\n",
    "            self.list = list(open(self.rgb_list_file))\n",
    "            self.flow_list = list(open(self.flow_list_file))\n",
    "            self.audio_list = list(open(self.audio_list_file))\n",
    "        else:\n",
    "            assert 1 > 2, 'Modality is wrong!'\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.normal_flag in self.list[index]:\n",
    "            label = 0.0\n",
    "        else:\n",
    "            label = 1.0\n",
    "\n",
    "        if self.modality == 'AUDIO':\n",
    "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "        elif self.modality == 'RGB':\n",
    "            features = np.array(np.load(self.list[index].strip('\\n')),dtype=np.float32)\n",
    "        elif self.modality == 'FLOW':\n",
    "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "        elif self.modality == 'MIX':\n",
    "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
    "            if features1.shape[0] == features2.shape[0]:\n",
    "                features = np.concatenate((features1, features2),axis=1)\n",
    "            else:# because the frames of flow is one less than that of rgb\n",
    "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
    "        elif self.modality == 'MIX2':\n",
    "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
    "        elif self.modality == 'MIX3':\n",
    "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
    "            if features1.shape[0] == features2.shape[0]:\n",
    "                features = np.concatenate((features1, features2),axis=1)\n",
    "            else:# because the frames of flow is one less than that of rgb\n",
    "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
    "        elif self.modality == 'MIX_ALL':\n",
    "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
    "            features3 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
    "            if features1.shape[0] == features2.shape[0]:\n",
    "                features = np.concatenate((features1, features2, features3),axis=1)\n",
    "            else:# because the frames of flow is one less than that of rgb\n",
    "                features = np.concatenate((features1[:-1], features2, features3[:-1]), axis=1)\n",
    "        else:\n",
    "            assert 1>2, 'Modality is wrong!'\n",
    "        if self.tranform is not None:\n",
    "            features = self.tranform(features)\n",
    "        if self.test_mode:\n",
    "            return features\n",
    "\n",
    "        else:\n",
    "            features = process_feat(features, self.max_seqlen, is_random=False)\n",
    "            return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvvMYO_1LXTV"
   },
   "source": [
    "### D) Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bYn0UsFKWTy"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from torch import FloatTensor\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(in_features, out_features).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
    "        self.a = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(2*out_features, 1).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "class linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(linear, self).__init__()\n",
    "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
    "        self.register_parameter('bias', None)\n",
    "        stdv = 1. / sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "    def forward(self, x):\n",
    "        x = x.matmul(self.weight)\n",
    "        return x\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False, residual=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_features == out_features):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            # self.residual = linear(in_features, out_features)\n",
    "            self.residual = nn.Conv1d(in_channels=in_features, out_channels=out_features, kernel_size=5, padding=2)\n",
    "    def reset_parameters(self):\n",
    "        # stdv = 1. / sqrt(self.weight.size(1))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # To support batch operations\n",
    "        support = input.matmul(self.weight)\n",
    "        output = adj.matmul(support)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        if self.in_features != self.out_features and self.residual:\n",
    "            input = input.permute(0,2,1)\n",
    "            res = self.residual(input)\n",
    "            res = res.permute(0,2,1)\n",
    "            output = output + res\n",
    "        else:\n",
    "            output = output + self.residual(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "######################################################\n",
    "\n",
    "class SimilarityAdj(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SimilarityAdj, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weight0 = Parameter(FloatTensor(in_features, out_features))\n",
    "        self.weight1 = Parameter(FloatTensor(in_features, out_features))\n",
    "        self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # stdv = 1. / sqrt(self.weight0.size(1))\n",
    "        nn.init.xavier_uniform_(self.weight0)\n",
    "        nn.init.xavier_uniform_(self.weight1)\n",
    "\n",
    "    def forward(self, input, seq_len):\n",
    "        # To support batch operations\n",
    "        soft = nn.Softmax(1)\n",
    "        theta = torch.matmul(input, self.weight0)\n",
    "        phi = torch.matmul(input, self.weight0)\n",
    "        phi2 = phi.permute(0, 2, 1)\n",
    "        sim_graph = torch.matmul(theta, phi2)\n",
    "\n",
    "        theta_norm = torch.norm(theta, p=2, dim=2, keepdim=True)  # B*T*1\n",
    "        phi_norm = torch.norm(phi, p=2, dim=2, keepdim=True)  # B*T*1\n",
    "        x_norm_x = theta_norm.matmul(phi_norm.permute(0, 2, 1))\n",
    "        sim_graph = sim_graph / (x_norm_x + 1e-20)\n",
    "\n",
    "        output = torch.zeros_like(sim_graph)\n",
    "        if seq_len is None:\n",
    "            for i in range(sim_graph.shape[0]):\n",
    "                tmp = sim_graph[i]\n",
    "                adj2 = tmp\n",
    "                adj2 = F.threshold(adj2, 0.7, 0)\n",
    "                adj2 = soft(adj2)\n",
    "                output[i] = adj2\n",
    "        else:\n",
    "            for i in range(len(seq_len)):\n",
    "                tmp = sim_graph[i, :seq_len[i], :seq_len[i]]\n",
    "                adj2 = tmp\n",
    "                adj2 = F.threshold(adj2, 0.7, 0)\n",
    "                adj2 = soft(adj2)\n",
    "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class DistanceAdj(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistanceAdj, self).__init__()\n",
    "        self.sigma = Parameter(FloatTensor(1))\n",
    "        self.sigma.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, batch_size, max_seqlen):\n",
    "        # To support batch operations\n",
    "        self.arith = np.arange(max_seqlen).reshape(-1, 1)\n",
    "        dist = pdist(self.arith, metric='cityblock').astype(np.float32)\n",
    "        self.dist = torch.from_numpy(squareform(dist)).to('cuda')\n",
    "        self.dist = torch.exp(-self.dist / torch.exp(torch.tensor(1.)))\n",
    "        self.dist = torch.unsqueeze(self.dist, 0).repeat(batch_size, 1, 1).to('cuda')\n",
    "        return self.dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44eeeOUcLquB"
   },
   "source": [
    "### E) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8Rjvb-yKOeZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as torch_init\n",
    "import os\n",
    "# from layers import GraphConvolution, SimilarityAdj, DistanceAdj\n",
    "\n",
    "\n",
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        torch_init.xavier_uniform_(m.weight)\n",
    "        # m.bias.data.fill_(0.1)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        n_features = args.feature_size\n",
    "        n_class = args.num_classes\n",
    "\n",
    "        self.conv1d1 = nn.Conv1d(in_channels=n_features, out_channels=512, kernel_size=1, padding=0)\n",
    "        self.conv1d2 = nn.Conv1d(in_channels=512, out_channels=128, kernel_size=1, padding=0)\n",
    "        self.conv1d3 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv1d4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "        # Graph Convolution\n",
    "        self.gc1 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
    "        self.gc2 = GraphConvolution(32, 32, residual=True)\n",
    "        self.gc3 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
    "        self.gc4 = GraphConvolution(32, 32, residual=True)\n",
    "        self.gc5 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
    "        self.gc6 = GraphConvolution(32, 32, residual=True)\n",
    "        self.simAdj = SimilarityAdj(n_features, 32)\n",
    "        self.disAdj = DistanceAdj()\n",
    "\n",
    "        self.classifier = nn.Linear(32*3, n_class)\n",
    "        self.approximator = nn.Sequential(nn.Conv1d(128, 64, 1, padding=0), nn.ReLU(),\n",
    "                                          nn.Conv1d(64, 32, 1, padding=0), nn.ReLU())\n",
    "        self.conv1d_approximator = nn.Conv1d(32, 1, 5, padding=0)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.apply(weight_init)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs, seq_len):\n",
    "        x = inputs.permute(0, 2, 1)  # for conv1d\n",
    "        x = self.relu(self.conv1d1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv1d2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        logits = self.approximator(x)\n",
    "        logits = F.pad(logits, (4, 0))\n",
    "        logits = self.conv1d_approximator(logits)\n",
    "        logits = logits.permute(0, 2, 1)\n",
    "        x = x.permute(0, 2, 1)  # b*t*c\n",
    "\n",
    "        ## gcn\n",
    "        scoadj = self.sadj(logits.detach(), seq_len)\n",
    "        adj = self.adj(inputs, seq_len)\n",
    "        disadj = self.disAdj(x.shape[0], x.shape[1])\n",
    "        x1_h = self.relu(self.gc1(x, adj))\n",
    "        x1_h = self.dropout(x1_h)\n",
    "        x2_h = self.relu(self.gc3(x, disadj))\n",
    "        x2_h = self.dropout(x2_h)\n",
    "        x3_h = self.relu(self.gc5(x, scoadj))\n",
    "        x3_h = self.dropout(x3_h)\n",
    "        x1 = self.relu(self.gc2(x1_h, adj))\n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = self.relu(self.gc4(x2_h, disadj))\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.relu(self.gc6(x3_h, scoadj))\n",
    "        x3 = self.dropout(x3)\n",
    "        x = torch.cat((x1, x2, x3), 2)\n",
    "        x = self.classifier(x)\n",
    "        return x, logits\n",
    "\n",
    "    def sadj(self, logits, seq_len):\n",
    "        lens = logits.shape[1]\n",
    "        soft = nn.Softmax(1)\n",
    "        logits2 = self.sigmoid(logits).repeat(1, 1, lens)\n",
    "        tmp = logits2.permute(0, 2, 1)\n",
    "        adj = 1. - torch.abs(logits2 - tmp)\n",
    "        self.sig = lambda x:1/(1+torch.exp(-((x-0.5))/0.1))\n",
    "        adj = self.sig(adj)\n",
    "        output = torch.zeros_like(adj)\n",
    "        if seq_len is None:\n",
    "            for i in range(logits.shape[0]):\n",
    "                tmp = adj[i]\n",
    "                adj2 = soft(tmp)\n",
    "                output[i] = adj2\n",
    "        else:\n",
    "            for i in range(len(seq_len)):\n",
    "                tmp = adj[i, :seq_len[i], :seq_len[i]]\n",
    "                adj2 = soft(tmp)\n",
    "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
    "        return output\n",
    "\n",
    "\n",
    "    def adj(self, x, seq_len):\n",
    "        soft = nn.Softmax(1)\n",
    "        x2 = x.matmul(x.permute(0,2,1)) # B*T*T\n",
    "        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n",
    "        x_norm_x = x_norm.matmul(x_norm.permute(0,2,1))\n",
    "        x2 = x2/(x_norm_x+1e-20)\n",
    "        output = torch.zeros_like(x2)\n",
    "        if seq_len is None:\n",
    "            for i in range(x.shape[0]):\n",
    "                tmp = x2[i]\n",
    "                adj2 = tmp\n",
    "                adj2 = F.threshold(adj2, 0.7, 0)\n",
    "                adj2 = soft(adj2)\n",
    "                output[i] = adj2\n",
    "        else:\n",
    "            for i in range(len(seq_len)):\n",
    "                tmp = x2[i, :seq_len[i], :seq_len[i]]\n",
    "                adj2 = tmp\n",
    "                adj2 = F.threshold(adj2, 0.7, 0)\n",
    "                adj2 = soft(adj2)\n",
    "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oE8lvyeyTk0C",
    "outputId": "7aaaea0c-4443-4d71-eb00-aaa01e97667f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /mydrive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z-JVZlBNeg4"
   },
   "source": [
    "## Args\n",
    "\n",
    "Here are the default args that were obtained via cmd line arg parser. I just created a class 'Args' that holds the default config for the model.\n",
    "\n",
    "I think the most important args:\n",
    "\n",
    "*`Modality`*: Determines whether we want to use either audio alone, video alone, both audio and video, audio, video, and flow, etc. for training\n",
    "\n",
    "*`List`*: point to the list containing filenames for all training and testing data.\n",
    "\n",
    "*`workers`*: I believe this is the number of individual threads/processes running during training or testing. In ther model it was set to 4 by defualt but that spit out an error so it lowered it to 1. Prob a sign that we need to do heavy downsampling to compensate for lack of parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g_4ciyOM8tl"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.modality = 'MIX2'\n",
    "        # Original paths\n",
    "        self.rgb_list = '/content/final_dl/list/rgb.list'\n",
    "        self.flow_list = '/content/final_dl/list/flow.list'\n",
    "        self.audio_list = '/content/final_dl/list/audio.list'\n",
    "\n",
    "        # Train paths\n",
    "        self.train_rgb_list = '/content/final_dl/list/rgb_train.list'\n",
    "        self.train_flow_list = '/content/final_dl/list/flow_train.list'\n",
    "        self.train_audio_list = '/content/final_dl/list/audio_train.list'\n",
    "\n",
    "        # Val paths\n",
    "        self.val_rgb_list = '/content/final_dl/list/rgb_val.list'\n",
    "        self.val_flow_list = '/content/final_dl/list/flow_val.list'\n",
    "        self.val_audio_list = '/content/final_dl/list/audio_val.list'\n",
    "\n",
    "        # Test paths\n",
    "        self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
    "        self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
    "        self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
    "\n",
    "        self.gt = '/content/final_dl/list/gt.npy'\n",
    "        self.gpus = 1\n",
    "        self.lr = 0.0001\n",
    "        self.batch_size = 128\n",
    "        self.workers = 1  # Reduced from 4 to avoid memory issues\n",
    "        self.model_name = 'wsanodet'\n",
    "        self.pretrained_ckpt = None\n",
    "        self.feature_size = 1152  # 1024 + 128\n",
    "        self.num_classes = 1\n",
    "        self.dataset_name = 'XD-Violence'\n",
    "        self.max_seqlen = 200\n",
    "        self.max_epoch = 50\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xM7Rywzs4s8"
   },
   "source": [
    "## Val Split For MultiModal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRVNY_NEcEB5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def get_video_id(filepath):\n",
    "    \"\"\"Extract video ID from filepath based on common prefix before _label\n",
    "    e.g., \"/path/to/video123_label_A.npy\" -> \"video123\"\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "    if '_label' in filename:\n",
    "        return filename.split('_label')[0]\n",
    "    return filename.split('.')[0]\n",
    "\n",
    "def find_matching_files():\n",
    "    \"\"\"\n",
    "    Find and align RGB and audio feature files.\n",
    "    Returns dict mapping video IDs to their RGB and audio paths\n",
    "    \"\"\"\n",
    "    rgb_path = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
    "    audio_path = \"/content/final_dl/list/xx/train\"\n",
    "\n",
    "    # Get all files\n",
    "    rgb_files = glob.glob(os.path.join(rgb_path, \"*.npy\"))\n",
    "    audio_files = glob.glob(os.path.join(audio_path, \"*.npy\"))\n",
    "\n",
    "    # Create mappings that preserve the 5:1 ratio\n",
    "    rgb_map = {}\n",
    "    for f in rgb_files:\n",
    "        vid_id = get_video_id(f)\n",
    "        if vid_id not in rgb_map:\n",
    "            rgb_map[vid_id] = []\n",
    "        rgb_map[vid_id].append(f)\n",
    "\n",
    "    audio_map = {get_video_id(f): f for f in audio_files}\n",
    "\n",
    "    # Find common video IDs\n",
    "    common_ids = set(rgb_map.keys()) & set(audio_map.keys())\n",
    "\n",
    "    # Create aligned mapping\n",
    "    aligned_files = {\n",
    "        vid_id: {\n",
    "            'rgb': sorted(rgb_map[vid_id]),  # Sort to maintain consistent ordering\n",
    "            'audio': audio_map[vid_id],\n",
    "            'is_normal': '_label_A' in rgb_map[vid_id][0]  # Check first RGB file for label\n",
    "        }\n",
    "        for vid_id in common_ids\n",
    "    }\n",
    "\n",
    "    print(f\"Found {len(aligned_files)} aligned RGB-Audio pairs\")\n",
    "    return aligned_files\n",
    "\n",
    "def create_splits(aligned_files, train_ratio=0.8, seed=42):\n",
    "    \"\"\"Split the video IDs first, then we'll expand to files in write_list_files\"\"\"\n",
    "    random.seed(seed)\n",
    "    video_ids = list(aligned_files.keys())\n",
    "    train_size = int(len(video_ids) * train_ratio)\n",
    "    train_ids = random.sample(video_ids, train_size)\n",
    "    val_ids = [vid for vid in video_ids if vid not in train_ids]\n",
    "\n",
    "    return {\n",
    "        'train': train_ids,\n",
    "        'val': val_ids\n",
    "    }\n",
    "\n",
    "def write_list_files(split_data, aligned_files, output_dir=\"/content/final_dl/list\"):\n",
    "    \"\"\"Write list files with audio files repeated to match RGB structure\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for split_name, video_ids in split_data.items():\n",
    "        # RGB list - one entry per frame\n",
    "        rgb_path = os.path.join(output_dir, f'rgb_{split_name}.list')\n",
    "        with open(rgb_path, 'w') as f:\n",
    "            for vid_id in video_ids:\n",
    "                for rgb_file in aligned_files[vid_id]['rgb']:\n",
    "                    f.write(f\"{rgb_file}\\n\")\n",
    "\n",
    "        # Audio list - one entry per video (not per frame)\n",
    "        audio_path = os.path.join(output_dir, f'audio_{split_name}.list')\n",
    "        with open(audio_path, 'w') as f:\n",
    "            for vid_id in video_ids:\n",
    "                audio_file = aligned_files[vid_id]['audio']\n",
    "                f.write(f\"{audio_file}\\n\")  # Write once only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJzaD9WsT3YV"
   },
   "source": [
    "## Create Dataloaders for multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVCYAPTSTuQY",
    "outputId": "71945685-3bce-40a0-c190-9fb7c20fde9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3953 aligned RGB-Audio pairs\n"
     ]
    }
   ],
   "source": [
    "aligned_files = find_matching_files()\n",
    "\n",
    "    # Create train/val splits\n",
    "split_data = create_splits(aligned_files)\n",
    "\n",
    "    # Write list files\n",
    "write_list_files(split_data, aligned_files)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "def create_data_loaders(args):\n",
    "    \"\"\"\n",
    "    Create train, validation and test data loaders\n",
    "    \"\"\"\n",
    "    print(\"Creating data loaders...\")\n",
    "\n",
    "    # Create train loader\n",
    "    train_dataset = Dataset(args, mode='train')\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Train loader created with {len(train_dataset)} samples\")\n",
    "\n",
    "    # Create validation loader\n",
    "    val_dataset = Dataset(args, mode='val')\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,  # No need to shuffle validation data\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Validation loader created with {len(val_dataset)} samples\")\n",
    "\n",
    "    # Create test loader with smaller batch size as per original code\n",
    "    test_dataset = Dataset(args, mode='test')\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=5,  # Using smaller batch size for testing\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Test loader created with {len(test_dataset)} samples\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsN1t5SrTQoU"
   },
   "source": [
    "## VAL SPLIT FOR SINGLE MODALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7aaqgH5TP66"
   },
   "outputs": [],
   "source": [
    "def create_single_modality_data_loaders(args, modality='AUDIO'):\n",
    "    \"\"\"\n",
    "    Create train, validation and test data loaders for a single modality\n",
    "    \"\"\"\n",
    "    print(f\"Creating {modality} data loaders...\")\n",
    "\n",
    "    # Create new args with only needed attributes\n",
    "    args_new = Args()\n",
    "    args_new.modality = modality\n",
    "\n",
    "    # List files needed for train/val/test splits\n",
    "    if modality == 'AUDIO':\n",
    "        args_new.train_audio_list = args.train_audio_list\n",
    "        args_new.val_audio_list = args.val_audio_list\n",
    "        args_new.test_audio_list = args.test_audio_list\n",
    "    elif modality == 'RGB':\n",
    "        args_new.train_rgb_list = args.train_rgb_list\n",
    "        args_new.val_rgb_list = args.val_rgb_list\n",
    "        args_new.test_rgb_list = args.test_rgb_list\n",
    "    elif modality == 'FLOW':\n",
    "        args_new.train_flow_list = args.train_flow_list\n",
    "        args_new.val_flow_list = args.val_flow_list\n",
    "        args_new.test_flow_list = args.test_flow_list\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = Dataset(args_new, mode='train')\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args_new.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args_new.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Train loader created with {len(train_dataset)} samples\")\n",
    "\n",
    "    val_dataset = Dataset(args_new, mode='val')\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args_new.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args_new.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Validation loader created with {len(val_dataset)} samples\")\n",
    "\n",
    "    test_dataset = Dataset(args_new, mode='test')\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=5,\n",
    "        shuffle=False,\n",
    "        num_workers=args_new.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Test loader created with {len(test_dataset)} samples\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNM_LV6qT_q0"
   },
   "source": [
    "## Create data loader for specified modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "collapsed": true,
    "id": "OtwEJzeJTr-i",
    "outputId": "c316512e-d61f-4270-dc0d-e0451e4e61ab"
   },
   "outputs": [],
   "source": [
    "# For audio only\n",
    "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='AUDIO')\n",
    "\n",
    "# For RGB only\n",
    "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='RGB')\n",
    "\n",
    "# For flow only\n",
    "## CURRENTLY, FLOW IS NOT SUPPORTED, BUT IMPLEMENTING IT WOULD NOT BE THAT CHALLENGING. YOU WOULD SIMPLY HAVE TO\n",
    "## ADJUST THE CODE A FEW CELLS ABOVE SO TO WRITE AN EQUIVALENT OF \"find_matching_files\" FOR FLOW DATA\n",
    "#train_loader, val_loader, test_loader = create_single_modality_data_loaders(args, modality='FLOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "LWZIdanUgWCK",
    "outputId": "9d999110-e468-4630-8098-5a3bf0e3e7dd"
   },
   "outputs": [],
   "source": [
    "# Testing the val splitter, no need to run this\n",
    "\n",
    "def inspect_batch_files(args, batch_size=1, num_batches=5):\n",
    "    \"\"\"\n",
    "    Inspect the first few batches to see which files are being loaded and their dimensions\n",
    "    \"\"\"\n",
    "    #from torch.utils.data import DataLoader\n",
    "    #from dataset import Dataset  # Your dataset class\n",
    "\n",
    "    dataset = Dataset(args, mode='train')\n",
    "\n",
    "    print(\"Inspecting individual samples:\")\n",
    "    for i in range(min(25, len(dataset))):\n",
    "        try:\n",
    "            # Get the filepaths that would be loaded\n",
    "            rgb_path = dataset.list[i].strip('\\n')\n",
    "            audio_path = dataset.audio_list[i//5].strip('\\n')\n",
    "\n",
    "            # Load the features\n",
    "            features1 = np.array(np.load(rgb_path), dtype=np.float32)\n",
    "            features2 = np.array(np.load(audio_path), dtype=np.float32)\n",
    "\n",
    "            print(f\"\\nSample {i}:\")\n",
    "            print(f\"RGB file: {os.path.basename(rgb_path)}\")\n",
    "            print(f\"RGB shape: {features1.shape}\")\n",
    "            print(f\"Audio file: {os.path.basename(audio_path)}\")\n",
    "            print(f\"Audio shape: {features2.shape}\")\n",
    "\n",
    "            # Try the concatenation\n",
    "            try:\n",
    "                if features1.shape[0] != features2.shape[0]:\n",
    "                    print(\"⚠️ Dimension mismatch!\")\n",
    "                    if features1.shape[0] - 1 == features2.shape[0]:\n",
    "                        print(\"Would work with [:-1] slice\")\n",
    "                    features = np.concatenate((features1[:-1], features2), axis=1)\n",
    "                    print(\"Concatenation successful after adjustment\")\n",
    "            except ValueError as e:\n",
    "                print(f\"❌ Concatenation failed: {str(e)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {i}: {str(e)}\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the inspection\n",
    "inspect_batch_files(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NCyNXLPN5u0"
   },
   "source": [
    "## Testing PreTrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nkT8DU7V3HOz",
    "outputId": "0daa5404-6268-4f41-88a6-74a076ea2c11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-6958b6b31f24>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  {k.replace('module.', ''): v for k, v in torch.load('/content/final_dl/wsanodet_mix2.pkl').items()})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:32.01085138320923\n",
      "offline pr_auc:0.79; online pr_auc:0.7433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "# from model import Model\n",
    "# from dataset import Dataset\n",
    "# from test import test\n",
    "# import option\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  device = torch.device(\"cuda\")\n",
    "\n",
    "  test_loader = DataLoader(Dataset(args, mode='test'),\n",
    "                            batch_size=5, shuffle=False,\n",
    "                            num_workers=args.workers, pin_memory=True)\n",
    "  model = Model(args)\n",
    "  model = model.to(device)\n",
    "  # had to change path to \"/content/final_dl/wsanodet_mix2.pkl\"\n",
    "  model_dict = model.load_state_dict(\n",
    "      {k.replace('module.', ''): v for k, v in torch.load('/content/final_dl/wsanodet_mix2.pkl').items()})\n",
    "\n",
    "  gt = np.load(args.gt)\n",
    "  st = time.time()\n",
    "  pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
    "  print('Time:{}'.format(time.time()-st))\n",
    "  print('offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NIbLkkivMhx"
   },
   "source": [
    "how to save a model for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7mSdUwSvLTq"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"/content/test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0S5vwXjQzID"
   },
   "source": [
    "# Training HLNET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lv0i1DYkQ2Qj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def CLAS(logits, label, seq_len, criterion, device, is_topk=True):\n",
    "    logits = logits.squeeze()\n",
    "    instance_logits = torch.zeros(0).to(device)  # tensor([])\n",
    "    for i in range(logits.shape[0]):\n",
    "        if is_topk:\n",
    "            tmp, _ = torch.topk(logits[i][:seq_len[i]], k=int(seq_len[i]//16+1), largest=True)\n",
    "            tmp = torch.mean(tmp).view(1)\n",
    "        else:\n",
    "            tmp = torch.mean(logits[i, :seq_len[i]]).view(1)\n",
    "        instance_logits = torch.cat((instance_logits, tmp))\n",
    "\n",
    "    instance_logits = torch.sigmoid(instance_logits)\n",
    "\n",
    "    clsloss = criterion(instance_logits, label)\n",
    "    return clsloss\n",
    "\n",
    "\n",
    "def CENTROPY(logits, logits2, seq_len, device):\n",
    "    instance_logits = torch.tensor(0).to(device)  # tensor([])\n",
    "    for i in range(logits.shape[0]):\n",
    "        tmp1 = torch.sigmoid(logits[i, :seq_len[i]]).squeeze()\n",
    "        tmp2 = torch.sigmoid(logits2[i, :seq_len[i]]).squeeze()\n",
    "        loss = torch.mean(-tmp1.detach() * torch.log(tmp2))\n",
    "        instance_logits = instance_logits + loss\n",
    "    instance_logits = instance_logits/logits.shape[0]\n",
    "    return instance_logits\n",
    "\n",
    "\n",
    "def train(dataloader, model, optimizer, criterion, device, is_topk):\n",
    "    with torch.set_grad_enabled(True):\n",
    "        model.train()\n",
    "        for i, (input, label) in enumerate(dataloader):\n",
    "            seq_len = torch.sum(torch.max(torch.abs(input), dim=2)[0]>0, 1)\n",
    "            input = input[:, :torch.max(seq_len), :]\n",
    "            input, label = input.float().to(device), label.float().to(device)\n",
    "            logits, logits2 = model(input, seq_len)\n",
    "            clsloss = CLAS(logits, label, seq_len, criterion, device, is_topk)\n",
    "            clsloss2 = CLAS(logits2, label, seq_len, criterion, device, is_topk)\n",
    "            croloss = CENTROPY(logits, logits2, seq_len, device)\n",
    "\n",
    "            total_loss = clsloss + clsloss2 + 5*croloss\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcLzmEr4HIQY",
    "outputId": "427e5241-2699-4e42-eee2-84abc2c81d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated paths have been written to /content/final_dl/list/audio.list\n"
     ]
    }
   ],
   "source": [
    "# Define the input .list file containing the original file paths\n",
    "input_list_file = \"/content/final_dl/list/audio.list\"\n",
    "\n",
    "# Define the directory to update the paths to\n",
    "new_directory = \"/content/final_dl/list/xx/train\"\n",
    "\n",
    "# Define the output .list file for the updated file paths\n",
    "output_list_file = \"/content/final_dl/list/audio.list\"\n",
    "\n",
    "# Read the original file paths from the .list file\n",
    "with open(input_list_file, \"r\") as file:\n",
    "    original_paths = file.readlines()\n",
    "\n",
    "# Process and update each file path\n",
    "updated_paths = []\n",
    "for path in original_paths:\n",
    "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
    "    if path:  # Ensure the path is not empty\n",
    "        # Extract the filename from the original path and create a new path\n",
    "        filename = path.split(\"/\")[-1]\n",
    "        updated_path = f\"{new_directory}/{filename}\"\n",
    "        updated_paths.append(updated_path)\n",
    "\n",
    "# Write the updated paths to the output .list file\n",
    "with open(output_list_file, \"w\") as file:\n",
    "    file.write(\"\\n\".join(updated_paths))\n",
    "\n",
    "print(f\"Updated paths have been written to {output_list_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mubL6MwpIS8k",
    "outputId": "3b39b7c2-9487-45fe-b7ba-35f55dce2d82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated paths have been written to /content/final_dl/list/rgb.list\n"
     ]
    }
   ],
   "source": [
    "# Define the input .list file containing the original file paths\n",
    "input_list_file = \"/content/final_dl/list/rgb.list\"\n",
    "\n",
    "# Define the directory to update the paths to\n",
    "new_directory = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
    "\n",
    "# Define the output .list file for the updated file paths\n",
    "output_list_file = \"/content/final_dl/list/rgb.list\"\n",
    "\n",
    "# Read the original file paths from the .list file\n",
    "with open(input_list_file, \"r\") as file:\n",
    "    original_paths = file.readlines()\n",
    "\n",
    "# Process and update each file path\n",
    "updated_paths = []\n",
    "for path in original_paths:\n",
    "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
    "    if path:  # Ensure the path is not empty\n",
    "        # Extract the filename from the original path and create a new path\n",
    "        filename = path.split(\"/\")[-1]\n",
    "        updated_path = f\"{new_directory}/{filename}\"\n",
    "        updated_paths.append(updated_path)\n",
    "\n",
    "# Write the updated paths to the output .list file\n",
    "with open(output_list_file, \"w\") as file:\n",
    "    file.write(\"\\n\".join(updated_paths))\n",
    "\n",
    "print(f\"Updated paths have been written to {output_list_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYz5pRU7Iyf8"
   },
   "source": [
    "## test (ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTEZP14qIyOD",
    "outputId": "3067bd70-3ce8-4c53-a00a-78f5f39585d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 200, 1152])\n",
      "torch.Size([5, 200, 1152])\n",
      "torch.Size([5, 200, 1152])\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "  def __init__(self):\n",
    "      self.modality = 'MIX2'\n",
    "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
    "      self.flow_list = '/content/final_dl/list/flow.list'\n",
    "      self.audio_list = '/content/final_dl/list/audio.list'\n",
    "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
    "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
    "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
    "      self.gt = '/content/final_dl/list/gt.npy'\n",
    "      self.gpus = 1\n",
    "      self.lr = 0.0001\n",
    "      self.batch_size = 128\n",
    "      self.workers = 1\n",
    "      self.model_name = 'wsanodet'\n",
    "      self.pretrained_ckpt = None\n",
    "      self.feature_size = 1152  # 1024 + 128\n",
    "      self.num_classes = 1\n",
    "      self.dataset_name = 'XD-Violence'\n",
    "      self.max_seqlen = 200\n",
    "      self.max_epoch = 50\n",
    "\n",
    "  # Create an instance of the Args class\n",
    "args = Args()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(args)\n",
    "model = model.cuda()\n",
    "\n",
    "test_loader = DataLoader(Dataset(args, mode='test'),\n",
    "                          batch_size=5, shuffle=True,\n",
    "                          num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "  for i, (input,label) in enumerate(test_loader):\n",
    "    input = input.to(device)\n",
    "\n",
    "    print(input.shape)\n",
    "    ############\n",
    "    ### NOTE: ## setting seq_len to None pads training data in the sequence dim to 200\n",
    "    ############\n",
    "    logits, logits2 = model(inputs=input, seq_len=None)\n",
    "    # print(logits, logits2)\n",
    "    if i == 2:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM88zu_-ADPw"
   },
   "source": [
    "## Training HL NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "pFJMkujfFROw",
    "outputId": "eb100452-678f-482c-c4c8-e63f28f752a6"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "# import option\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# torch.multiprocessing.set_start_method('spawn')\n",
    "# setup_seed(2333)\n",
    "# args = option.parser.parse_args()\n",
    "\n",
    "!export TORCH_USE_CUDA_DSA=ON\n",
    "device = torch.device(\"cuda\")\n",
    "train_loader = DataLoader(Dataset(args, mode='train'),\n",
    "                          batch_size=args.batch_size, shuffle=True,\n",
    "                          num_workers=args.workers, pin_memory=True)\n",
    "test_loader = DataLoader(Dataset(args, mode='test'),\n",
    "                          batch_size=5, shuffle=False,\n",
    "                          num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(args)\n",
    "model = model.cuda()\n",
    "\n",
    "for name, value in model.named_parameters():\n",
    "    print(name)\n",
    "approximator_param = list(map(id, model.approximator.parameters()))\n",
    "approximator_param += list(map(id, model.conv1d_approximator.parameters()))\n",
    "base_param = filter(lambda p: id(p) not in approximator_param, model.parameters())\n",
    "\n",
    "if not os.path.exists('./ckpt'):\n",
    "    os.makedirs('./ckpt')\n",
    "optimizer = optim.Adam([{'params': base_param},\n",
    "                        {'params': model.approximator.parameters(), 'lr': args.lr / 2},\n",
    "                        {'params': model.conv1d_approximator.parameters(), 'lr': args.lr / 2},\n",
    "                        ],\n",
    "                        lr=args.lr, weight_decay=0.000)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "is_topk = True\n",
    "gt = np.load(args.gt)\n",
    "pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
    "print('Random initalization: offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n",
    "for epoch in range(args.max_epoch):\n",
    "    scheduler.step()\n",
    "    st = time.time()\n",
    "    train(train_loader, model, optimizer, criterion, device, is_topk)\n",
    "    if epoch % 2 == 0 and not epoch == 0:\n",
    "        torch.save(model.state_dict(), './ckpt/'+args.model_name+'{}.pkl'.format(epoch))\n",
    "\n",
    "    pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
    "    print('Epoch {0}/{1}: offline pr_auc:{2:.4}; online pr_auc:{3:.4}\\n'.format(epoch, args.max_epoch, pr_auc, pr_auc_online))\n",
    "torch.save(model.state_dict(), './ckpt/' + args.model_name + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYjtaIacOHyS"
   },
   "source": [
    "# Training VAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRzWrGepsWur"
   },
   "source": [
    "## VAE MODEL\n",
    "\n",
    "added batch normalization (gus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Sampling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z_means, z_log_vars):\n",
    "        epsilon = torch.randn_like(z_means, dtype=torch.float32)\n",
    "        return z_means + torch.exp(0.5 * z_log_vars) * epsilon\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=1152, seq_len=200):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Modified encoder with batch normalization\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 576, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(576),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(576, 288, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(288),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(288, 144, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(144),\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        flattened_dim = 144 * 25\n",
    "        \n",
    "        # Added batch norm for the linear layers\n",
    "        self.lin_mean = nn.Sequential(\n",
    "            nn.Linear(flattened_dim, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.lin_log_var = nn.Sequential(\n",
    "            nn.Linear(flattened_dim, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (B, 200, 1152) -> (B, 1152, 200)\n",
    "        x = self.encoder(x)\n",
    "        z_means = self.lin_mean(x)\n",
    "        z_log_vars = self.lin_log_var(x)\n",
    "        z = self.sampling(z_means, z_log_vars)\n",
    "        return z, z_means, z_log_vars\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=1152, seq_len=200):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        flattened_dim = 144 * 25\n",
    "\n",
    "        # Added batch norm to decoder FC layers\n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, flattened_dim),\n",
    "            nn.BatchNorm1d(flattened_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Added batch norm to decoder conv layers\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose1d(144, 288, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm1d(288),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(288, 576, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm1d(576),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(576, input_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Sigmoid()  # No batch norm before sigmoid\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder_fc(x)\n",
    "        x = x.view(-1, 144, 25)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = x.permute(0, 2, 1)  # (B, 1152, 200) -> (B, 200, 1152)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim=1152, seq_len=200):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(latent_dim, input_dim, seq_len)\n",
    "        self.decoder = Decoder(latent_dim, input_dim, seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, z_means, z_log_vars = self.encoder(x)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, z_means, z_log_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "class NormalDataset(data.Dataset):\n",
    "    def __init__(self, args, transform=None, mode='train'):\n",
    "        self.modality = args.modality\n",
    "        self.normal_flag = '_label_A'\n",
    "        self.max_seqlen = args.max_seqlen\n",
    "        self.transform = transform\n",
    "        self.test_mode = (mode == 'test')\n",
    "        \n",
    "        # Set appropriate file lists based on mode\n",
    "        if mode == 'test':\n",
    "            self.rgb_list_file = args.test_rgb_list\n",
    "            self.flow_list_file = args.test_flow_list\n",
    "            self.audio_list_file = args.test_audio_list\n",
    "        elif mode == 'val':\n",
    "            self.rgb_list_file = args.val_rgb_list\n",
    "            self.flow_list_file = args.val_flow_list\n",
    "            self.audio_list_file = args.val_audio_list\n",
    "        else:  # train\n",
    "            self.rgb_list_file = args.train_rgb_list\n",
    "            self.flow_list_file = args.train_flow_list\n",
    "            self.audio_list_file = args.train_audio_list\n",
    "            \n",
    "        self._parse_list()\n",
    "        \n",
    "    def _parse_list(self):\n",
    "        \"\"\"Parse file lists and filter for normal samples only\"\"\"\n",
    "        def filter_normal_samples(file_list):\n",
    "            return [f for f in file_list if self.normal_flag in f]\n",
    "            \n",
    "        if self.modality == 'AUDIO':\n",
    "            self.list = filter_normal_samples(list(open(self.audio_list_file)))\n",
    "        elif self.modality == 'RGB':\n",
    "            self.list = filter_normal_samples(list(open(self.rgb_list_file)))\n",
    "        elif self.modality == 'FLOW':\n",
    "            self.list = filter_normal_samples(list(open(self.flow_list_file)))\n",
    "        elif self.modality == 'MIX2':\n",
    "            # For MIX2, we need to handle the 5:1 ratio between RGB and audio\n",
    "            self.list = filter_normal_samples(list(open(self.rgb_list_file)))\n",
    "            # Filter audio list and ensure alignment\n",
    "            all_audio = list(open(self.audio_list_file))\n",
    "            self.audio_list = [f for f in all_audio if self.normal_flag in f]\n",
    "            \n",
    "            # Ensure RGB and audio lists are aligned (5:1 ratio)\n",
    "            rgb_video_ids = set([self._get_video_id(f) for f in self.list])\n",
    "            audio_video_ids = set([self._get_video_id(f) for f in self.audio_list])\n",
    "            common_ids = rgb_video_ids & audio_video_ids\n",
    "            \n",
    "            # Filter lists to only include common videos\n",
    "            self.list = [f for f in self.list if self._get_video_id(f) in common_ids]\n",
    "            self.audio_list = [f for f in self.audio_list if self._get_video_id(f) in common_ids]\n",
    "    \n",
    "    def _get_video_id(self, filepath):\n",
    "        \"\"\"Extract video ID from filepath\"\"\"\n",
    "        filename = os.path.basename(filepath.strip('\\n'))\n",
    "        return filename.split('_label')[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.modality in ['RGB', 'FLOW', 'AUDIO']:\n",
    "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "        elif self.modality == 'MIX2':\n",
    "            # Load RGB features\n",
    "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
    "            # Load corresponding audio features (accounting for 5:1 ratio)\n",
    "            audio_index = index // 5\n",
    "            features2 = np.array(np.load(self.audio_list[audio_index].strip('\\n')), dtype=np.float32)\n",
    "            \n",
    "            # Handle potential dimension mismatch\n",
    "            if features1.shape[0] > features2.shape[0]:\n",
    "                features1 = features1[:features2.shape[0]]\n",
    "            features = np.concatenate((features1, features2), axis=1)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "            \n",
    "        features = process_feat(features, self.max_seqlen, is_random=not self.test_mode)\n",
    "        \n",
    "        # Always return label 0 since these are normal samples\n",
    "        return features, 0.0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.list)\n",
    "\n",
    "def create_normal_data_loaders(args):\n",
    "    \"\"\"Create data loaders for normal samples only\"\"\"\n",
    "    print(\"Creating normal-only data loaders...\")\n",
    "    \n",
    "    # Create train loader\n",
    "    train_dataset = NormalDataset(args, mode='train')\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Normal train loader created with {len(train_dataset)} samples\")\n",
    "    \n",
    "    # Create validation loader\n",
    "    val_dataset = NormalDataset(args, mode='val')\n",
    "    val_loader = data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Normal validation loader created with {len(val_dataset)} samples\")\n",
    "    \n",
    "    # Create test loader\n",
    "    test_dataset = NormalDataset(args, mode='test')\n",
    "    test_loader = data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"Normal test loader created with {len(test_dataset)} samples\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def process_feat(feat, length, is_random=True):\n",
    "    \"\"\"Process features to have consistent length\"\"\"\n",
    "    if len(feat) > length:\n",
    "        if is_random:\n",
    "            r = np.random.randint(len(feat) - length)\n",
    "            return feat[r:r + length]\n",
    "        else:\n",
    "            r = np.linspace(0, len(feat) - 1, length, dtype=np.uint16)\n",
    "            return feat[r, :]\n",
    "    else:\n",
    "        return np.pad(feat, ((0, length - len(feat)), (0, 0)), mode='constant', constant_values=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUfjc_eorOqB"
   },
   "source": [
    "## VAE Training func (Gus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "def validate_vae(vae, val_loader, device):\n",
    "    \"\"\"Run validation loop and return average loss\"\"\"\n",
    "    vae.eval()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in val_loader:\n",
    "            # Only process normal samples (label == 0)\n",
    "            normal_mask = (labels == 0.0)\n",
    "            if not normal_mask.any():\n",
    "                continue\n",
    "\n",
    "            data = data[normal_mask].to(device)\n",
    "            recon_data, mu, logvar = vae(data)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            recon_criterion = torch.nn.MSELoss(reduction='sum')\n",
    "            recon_loss = recon_criterion(recon_data, data)\n",
    "\n",
    "            # KL divergence loss\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            # Total loss\n",
    "            loss = recon_loss + kl_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            n_samples += data.size(0)\n",
    "\n",
    "    # Calculate averages\n",
    "    if n_samples > 0:\n",
    "        avg_loss = total_loss / n_samples\n",
    "        avg_recon = total_recon_loss / n_samples\n",
    "        avg_kl = total_kl_loss / n_samples\n",
    "    else:\n",
    "        avg_loss = float('inf')\n",
    "        avg_recon = float('inf')\n",
    "        avg_kl = float('inf')\n",
    "\n",
    "    vae.train()\n",
    "    return avg_loss, avg_recon, avg_kl\n",
    "\n",
    "def train_vae(vae, train_loader, val_loader, args, save_dir='vae_checkpoints'):\n",
    "    \"\"\"Main training loop for VAE\"\"\"\n",
    "\n",
    "    # Create directory for saving checkpoints\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vae = vae.to(device)\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=args.lr)\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(args.max_epoch):\n",
    "        # Training\n",
    "        vae.train()\n",
    "        train_loss = 0\n",
    "        train_recon = 0\n",
    "        train_kl = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            # Only process normal samples (label == 0)\n",
    "            normal_mask = (labels == 0.0)\n",
    "            if not normal_mask.any():\n",
    "                continue\n",
    "\n",
    "            data = data[normal_mask].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            recon_data, mu, logvar = vae(data)\n",
    "\n",
    "            # Losses\n",
    "            recon_criterion = torch.nn.MSELoss(reduction='sum')\n",
    "            recon_loss = recon_criterion(recon_data, data)\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = recon_loss + kl_loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record losses\n",
    "            train_loss += loss.item()\n",
    "            train_recon += recon_loss.item()\n",
    "            train_kl += kl_loss.item()\n",
    "            n_samples += data.size(0)\n",
    "\n",
    "        # Calculate average training losses\n",
    "        if n_samples > 0:\n",
    "            avg_train_loss = train_loss / n_samples\n",
    "            avg_train_recon = train_recon / n_samples\n",
    "            avg_train_kl = train_kl / n_samples\n",
    "        else:\n",
    "            print(\"Warning: No normal samples in training batch\")\n",
    "            continue\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_recon, val_kl = validate_vae(vae, val_loader, device)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch+1}/{args.max_epoch}:')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}, Recon: {avg_train_recon:.4f}, KL: {avg_train_kl:.4f}')\n",
    "        print(f'Validation - Loss: {val_loss:.4f}, Recon: {val_recon:.4f}, KL: {val_kl:.4f}\\n')\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            save_path = os.path.join(save_dir, f'vae_best_{timestamp}.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': vae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, save_path)\n",
    "            print(f'Saved best model to {save_path}')\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping(val_loss):\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust args needed for VAE\n",
    "args_vae = Args()\n",
    "args_vae.feature_size = 1152  # 1024 (RGB) + 128 (audio)\n",
    "args_vae.batch_size = 64\n",
    "args_vae.modality = 'MIX2'\n",
    "\n",
    "# initialize VAE with correct input dimension\n",
    "vae = VAE(latent_dim=64, input_dim=args_vae.feature_size, seq_len=200)\n",
    "\n",
    "# Create normal-only dataloaders\n",
    "normal_train_loader, normal_val_loader, normal_test_loader = create_normal_data_loaders(args_vae)\n",
    "\n",
    "# Train the VAE\n",
    "trained_vae = train_vae(vae, normal_train_loader, normal_val_loader, args_vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_normal_dataset(args, num_samples=25):\n",
    "    \"\"\"\n",
    "    Inspect the normal samples being loaded from the dataset\n",
    "    \"\"\"\n",
    "    print(\"Creating NormalDataset...\")\n",
    "    dataset = NormalDataset(args, mode='train')\n",
    "    print(f\"Total number of samples in dataset: {len(dataset)}\")\n",
    "    print(f\"\\nNumber of files in main list: {len(dataset.list)}\")\n",
    "    if hasattr(dataset, 'audio_list'):\n",
    "        print(f\"Number of files in audio list: {len(dataset.audio_list)}\")\n",
    "\n",
    "    print(\"\\nInspecting individual samples:\")\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        try:\n",
    "            # Get the filepaths that would be loaded\n",
    "            rgb_path = dataset.list[i].strip('\\n')\n",
    "            print(f\"\\nSample {i}:\")\n",
    "            print(f\"RGB file: {os.path.basename(rgb_path)}\")\n",
    "\n",
    "            # Load RGB features\n",
    "            features1 = np.array(np.load(rgb_path), dtype=np.float32)\n",
    "            print(f\"RGB shape: {features1.shape}\")\n",
    "\n",
    "            # If MIX2 modality, also show audio information\n",
    "            if dataset.modality == 'MIX2':\n",
    "                audio_index = i // 5\n",
    "                if audio_index < len(dataset.audio_list):\n",
    "                    audio_path = dataset.audio_list[audio_index].strip('\\n')\n",
    "                    print(f\"Audio file: {os.path.basename(audio_path)}\")\n",
    "                    features2 = np.array(np.load(audio_path), dtype=np.float32)\n",
    "                    print(f\"Audio shape: {features2.shape}\")\n",
    "\n",
    "                    # Try the concatenation\n",
    "                    try:\n",
    "                        if features1.shape[0] != features2.shape[0]:\n",
    "                            print(\"⚠️ Dimension mismatch!\")\n",
    "                            print(f\"RGB frames: {features1.shape[0]}, Audio frames: {features2.shape[0]}\")\n",
    "                            if features1.shape[0] - 1 == features2.shape[0]:\n",
    "                                print(\"Would work with [:-1] slice\")\n",
    "                                features = np.concatenate((features1[:-1], features2), axis=1)\n",
    "                                print(f\"Concatenated shape after adjustment: {features.shape}\")\n",
    "                            else:\n",
    "                                print(\"Cannot be fixed with simple slice\")\n",
    "                        else:\n",
    "                            features = np.concatenate((features1, features2), axis=1)\n",
    "                            print(f\"Concatenated shape: {features.shape}\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"❌ Concatenation failed: {str(e)}\")\n",
    "                else:\n",
    "                    print(\"⚠️ No corresponding audio file (index out of range)\")\n",
    "\n",
    "            # Check if this is actually a normal sample\n",
    "            if '_label_A' not in rgb_path:\n",
    "                print(\"⚠️ WARNING: This doesn't appear to be a normal sample!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {i}: {str(e)}\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the inspection\n",
    "print(\"Inspecting dataset with MIX2 modality...\")\n",
    "args_check = args_vae\n",
    "args_check.modality = 'MIX2'\n",
    "args_check.batch_size = 64  # Smaller batch size for inspection\n",
    "inspect_normal_dataset(args_check)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
