{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahdrakes/mldl-final/blob/main/mm_violence_det_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDOr6KirJvUe"
      },
      "source": [
        "# Multi-Modal Violence Detection Network\n",
        "\n",
        "original src code: https://github.com/Roc-Ng/XDVioDet.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhzPvDXTKKqM"
      },
      "source": [
        "### Copying Training and Testing Data\n",
        "\n",
        "The folders are pretty large (~40/50GB) so it takes a while to copy all of the data over.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4DG4K3wYK8P",
        "outputId": "51d5c5b4-0d63-4193-e1f5-a934320c5fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /mydrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/mydrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "88GWL7hscvcg",
        "outputId": "2dab1aff-3e09-46ad-80cc-02975dac0673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/mydrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /mydrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nr7SpsS-2P6x"
      },
      "outputs": [],
      "source": [
        "!unzip final_dl.zip -d /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "f2GCHGksd0mQ"
      },
      "outputs": [],
      "source": [
        "# !zip -r final_dl.zip final_dl/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zWKH9vsRatb"
      },
      "source": [
        "may need to change directory depending on where you upload the data to google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "CHQ29LtvAsNj"
      },
      "outputs": [],
      "source": [
        "# !cp -r /mydrive/MyDrive/final_dl ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N1LDIKfKhpP"
      },
      "source": [
        "## 1. Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9acUBEhKqjC"
      },
      "source": [
        "### A) Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "collapsed": true,
        "id": "IPMWSbm0LBGY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc, precision_recall_curve\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def test(dataloader, model, device, gt):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pred = torch.zeros(0).to(device)\n",
        "        pred2 = torch.zeros(0).to(device)\n",
        "        for i, input in enumerate(dataloader):\n",
        "            input = input.to(device)\n",
        "            logits, logits2 = model(inputs=input, seq_len=None)\n",
        "            logits = torch.squeeze(logits)\n",
        "            sig = torch.sigmoid(logits)\n",
        "            sig = torch.mean(sig, 0)\n",
        "            pred = torch.cat((pred, sig))\n",
        "            '''\n",
        "            online detection\n",
        "            '''\n",
        "            logits2 = torch.squeeze(logits2)\n",
        "            sig2 = torch.sigmoid(logits2)\n",
        "            sig2 = torch.mean(sig2, 0)\n",
        "\n",
        "            sig2 = torch.unsqueeze(sig2, 1) ##for audio\n",
        "            pred2 = torch.cat((pred2, sig2))\n",
        "\n",
        "            # print(\"pred:, \", pred)\n",
        "            # print(\"pred2:, \", pred2)\n",
        "\n",
        "        pred = list(pred.cpu().detach().numpy())\n",
        "        pred2 = list(pred2.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred, 16))\n",
        "        pr_auc = auc(recall, precision)\n",
        "        precision, recall, th = precision_recall_curve(list(gt), np.repeat(pred2, 16))\n",
        "        pr_auc2 = auc(recall, precision)\n",
        "        return pr_auc, pr_auc2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLV269m9K7L3"
      },
      "source": [
        "### B) Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "ej2MARCmKzvk"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def random_extract(feat, t_max):\n",
        "   r = np.random.randint(len(feat)-t_max)\n",
        "   return feat[r:r+t_max]\n",
        "\n",
        "def uniform_extract(feat, t_max):\n",
        "   r = np.linspace(0, len(feat)-1, t_max, dtype=np.uint16)\n",
        "   return feat[r, :]\n",
        "\n",
        "def pad(feat, min_len):\n",
        "    if np.shape(feat)[0] <= min_len:\n",
        "       return np.pad(feat, ((0, min_len-np.shape(feat)[0]), (0, 0)), mode='constant', constant_values=0)\n",
        "    else:\n",
        "       return feat\n",
        "\n",
        "def process_feat(feat, length, is_random=True):\n",
        "    if len(feat) > length:\n",
        "        if is_random:\n",
        "            return random_extract(feat, length)\n",
        "        else:\n",
        "            return uniform_extract(feat, length)\n",
        "    else:\n",
        "        return pad(feat, length)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKIYnHpLDKS"
      },
      "source": [
        "### C) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "98z1xwOKKueM"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "\n",
        "# from utils import process_feat\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, args, transform=None, test_mode=False):\n",
        "        self.modality = args.modality\n",
        "\n",
        "        if test_mode:\n",
        "            self.rgb_list_file = args.test_rgb_list\n",
        "            self.flow_list_file = args.test_flow_list\n",
        "            self.audio_list_file = args.test_audio_list\n",
        "        else:\n",
        "            self.rgb_list_file = args.rgb_list\n",
        "            self.flow_list_file = args.flow_list\n",
        "            self.audio_list_file = args.audio_list\n",
        "        self.max_seqlen = args.max_seqlen\n",
        "        self.tranform = transform\n",
        "        self.test_mode = test_mode\n",
        "        self.normal_flag = '_label_A'\n",
        "        self._parse_list()\n",
        "\n",
        "    def _parse_list(self):\n",
        "        if self.modality == 'AUDIO':\n",
        "            self.list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'RGB':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            print(\"here\")\n",
        "            # print(self.list)\n",
        "        elif self.modality == 'FLOW':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "        elif self.modality == 'MIX2':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX3':\n",
        "            self.list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            self.list = list(open(self.rgb_list_file))\n",
        "            self.flow_list = list(open(self.flow_list_file))\n",
        "            self.audio_list = list(open(self.audio_list_file))\n",
        "        else:\n",
        "            assert 1 > 2, 'Modality is wrong!'\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.normal_flag in self.list[index]:\n",
        "            label = 0.0\n",
        "        else:\n",
        "            label = 1.0\n",
        "\n",
        "        if self.modality == 'AUDIO':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'RGB':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')),dtype=np.float32)\n",
        "        elif self.modality == 'FLOW':\n",
        "            features = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "        elif self.modality == 'MIX':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX2':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX3':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2), axis=1)\n",
        "        elif self.modality == 'MIX_ALL':\n",
        "            features1 = np.array(np.load(self.list[index].strip('\\n')), dtype=np.float32)\n",
        "            features2 = np.array(np.load(self.flow_list[index].strip('\\n')), dtype=np.float32)\n",
        "            features3 = np.array(np.load(self.audio_list[index//5].strip('\\n')), dtype=np.float32)\n",
        "            if features1.shape[0] == features2.shape[0]:\n",
        "                features = np.concatenate((features1, features2, features3),axis=1)\n",
        "            else:# because the frames of flow is one less than that of rgb\n",
        "                features = np.concatenate((features1[:-1], features2, features3[:-1]), axis=1)\n",
        "        else:\n",
        "            assert 1>2, 'Modality is wrong!'\n",
        "        if self.tranform is not None:\n",
        "            features = self.tranform(features)\n",
        "        if self.test_mode:\n",
        "            return features\n",
        "\n",
        "        else:\n",
        "            features = process_feat(features, self.max_seqlen, is_random=False)\n",
        "            return features, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvvMYO_1LXTV"
      },
      "source": [
        "### D) Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "3bYn0UsFKWTy"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "from torch import FloatTensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(in_features, out_features).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
        "        self.a = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(2*out_features, 1).type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor), gain=np.sqrt(2.0)), requires_grad=True)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        h = torch.mm(input, self.W)\n",
        "        N = h.size()[0]\n",
        "\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
        "\n",
        "class linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(linear, self).__init__()\n",
        "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.register_parameter('bias', None)\n",
        "        stdv = 1. / sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "    def forward(self, x):\n",
        "        x = x.matmul(self.weight)\n",
        "        return x\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=False, residual=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(FloatTensor(in_features, out_features))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "        elif (in_features == out_features):\n",
        "            self.residual = lambda x: x\n",
        "        else:\n",
        "            # self.residual = linear(in_features, out_features)\n",
        "            self.residual = nn.Conv1d(in_channels=in_features, out_channels=out_features, kernel_size=5, padding=2)\n",
        "    def reset_parameters(self):\n",
        "        # stdv = 1. / sqrt(self.weight.size(1))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # To support batch operations\n",
        "        support = input.matmul(self.weight)\n",
        "        output = adj.matmul(support)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        if self.in_features != self.out_features and self.residual:\n",
        "            input = input.permute(0,2,1)\n",
        "            res = self.residual(input)\n",
        "            res = res.permute(0,2,1)\n",
        "            output = output + res\n",
        "        else:\n",
        "            output = output + self.residual(input)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "######################################################\n",
        "\n",
        "class SimilarityAdj(Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(SimilarityAdj, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight0 = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.weight1 = Parameter(FloatTensor(in_features, out_features))\n",
        "        self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # stdv = 1. / sqrt(self.weight0.size(1))\n",
        "        nn.init.xavier_uniform_(self.weight0)\n",
        "        nn.init.xavier_uniform_(self.weight1)\n",
        "\n",
        "    def forward(self, input, seq_len):\n",
        "        # To support batch operations\n",
        "        soft = nn.Softmax(1)\n",
        "        theta = torch.matmul(input, self.weight0)\n",
        "        phi = torch.matmul(input, self.weight0)\n",
        "        phi2 = phi.permute(0, 2, 1)\n",
        "        sim_graph = torch.matmul(theta, phi2)\n",
        "\n",
        "        theta_norm = torch.norm(theta, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        phi_norm = torch.norm(phi, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        x_norm_x = theta_norm.matmul(phi_norm.permute(0, 2, 1))\n",
        "        sim_graph = sim_graph / (x_norm_x + 1e-20)\n",
        "\n",
        "        output = torch.zeros_like(sim_graph)\n",
        "        if seq_len is None:\n",
        "            for i in range(sim_graph.shape[0]):\n",
        "                tmp = sim_graph[i]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = sim_graph[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "class DistanceAdj(Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DistanceAdj, self).__init__()\n",
        "        self.sigma = Parameter(FloatTensor(1))\n",
        "        self.sigma.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, batch_size, max_seqlen):\n",
        "        # To support batch operations\n",
        "        self.arith = np.arange(max_seqlen).reshape(-1, 1)\n",
        "        dist = pdist(self.arith, metric='cityblock').astype(np.float32)\n",
        "        self.dist = torch.from_numpy(squareform(dist)).to('cuda')\n",
        "        self.dist = torch.exp(-self.dist / torch.exp(torch.tensor(1.)))\n",
        "        self.dist = torch.unsqueeze(self.dist, 0).repeat(batch_size, 1, 1).to('cuda')\n",
        "        return self.dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44eeeOUcLquB"
      },
      "source": [
        "### E) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "q8Rjvb-yKOeZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as torch_init\n",
        "import os\n",
        "# from layers import GraphConvolution, SimilarityAdj, DistanceAdj\n",
        "\n",
        "\n",
        "def weight_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
        "        torch_init.xavier_uniform_(m.weight)\n",
        "        # m.bias.data.fill_(0.1)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        n_features = args.feature_size\n",
        "        n_class = args.num_classes\n",
        "\n",
        "        self.conv1d1 = nn.Conv1d(in_channels=n_features, out_channels=512, kernel_size=1, padding=0)\n",
        "        self.conv1d2 = nn.Conv1d(in_channels=512, out_channels=128, kernel_size=1, padding=0)\n",
        "        self.conv1d3 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=5, padding=2)\n",
        "        self.conv1d4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
        "        # Graph Convolution\n",
        "        self.gc1 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc2 = GraphConvolution(32, 32, residual=True)\n",
        "        self.gc3 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc4 = GraphConvolution(32, 32, residual=True)\n",
        "        self.gc5 = GraphConvolution(128, 32, residual=True)  # nn.Linear(128, 32)\n",
        "        self.gc6 = GraphConvolution(32, 32, residual=True)\n",
        "        self.simAdj = SimilarityAdj(n_features, 32)\n",
        "        self.disAdj = DistanceAdj()\n",
        "\n",
        "        self.classifier = nn.Linear(32*3, n_class)\n",
        "        self.approximator = nn.Sequential(nn.Conv1d(128, 64, 1, padding=0), nn.ReLU(),\n",
        "                                          nn.Conv1d(64, 32, 1, padding=0), nn.ReLU())\n",
        "        self.conv1d_approximator = nn.Conv1d(32, 1, 5, padding=0)\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.apply(weight_init)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, inputs, seq_len):\n",
        "        x = inputs.permute(0, 2, 1)  # for conv1d\n",
        "        x = self.relu(self.conv1d1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.conv1d2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        logits = self.approximator(x)\n",
        "        logits = F.pad(logits, (4, 0))\n",
        "        logits = self.conv1d_approximator(logits)\n",
        "        logits = logits.permute(0, 2, 1)\n",
        "        x = x.permute(0, 2, 1)  # b*t*c\n",
        "\n",
        "        ## gcn\n",
        "        scoadj = self.sadj(logits.detach(), seq_len)\n",
        "        adj = self.adj(inputs, seq_len)\n",
        "        disadj = self.disAdj(x.shape[0], x.shape[1])\n",
        "        x1_h = self.relu(self.gc1(x, adj))\n",
        "        x1_h = self.dropout(x1_h)\n",
        "        x2_h = self.relu(self.gc3(x, disadj))\n",
        "        x2_h = self.dropout(x2_h)\n",
        "        x3_h = self.relu(self.gc5(x, scoadj))\n",
        "        x3_h = self.dropout(x3_h)\n",
        "        x1 = self.relu(self.gc2(x1_h, adj))\n",
        "        x1 = self.dropout(x1)\n",
        "        x2 = self.relu(self.gc4(x2_h, disadj))\n",
        "        x2 = self.dropout(x2)\n",
        "        x3 = self.relu(self.gc6(x3_h, scoadj))\n",
        "        x3 = self.dropout(x3)\n",
        "        x = torch.cat((x1, x2, x3), 2)\n",
        "        x = self.classifier(x)\n",
        "        return x, logits\n",
        "\n",
        "    def sadj(self, logits, seq_len):\n",
        "        lens = logits.shape[1]\n",
        "        soft = nn.Softmax(1)\n",
        "        logits2 = self.sigmoid(logits).repeat(1, 1, lens)\n",
        "        tmp = logits2.permute(0, 2, 1)\n",
        "        adj = 1. - torch.abs(logits2 - tmp)\n",
        "        self.sig = lambda x:1/(1+torch.exp(-((x-0.5))/0.1))\n",
        "        adj = self.sig(adj)\n",
        "        output = torch.zeros_like(adj)\n",
        "        if seq_len is None:\n",
        "            for i in range(logits.shape[0]):\n",
        "                tmp = adj[i]\n",
        "                adj2 = soft(tmp)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = adj[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = soft(tmp)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "        return output\n",
        "\n",
        "\n",
        "    def adj(self, x, seq_len):\n",
        "        soft = nn.Softmax(1)\n",
        "        x2 = x.matmul(x.permute(0,2,1)) # B*T*T\n",
        "        x_norm = torch.norm(x, p=2, dim=2, keepdim=True)  # B*T*1\n",
        "        x_norm_x = x_norm.matmul(x_norm.permute(0,2,1))\n",
        "        x2 = x2/(x_norm_x+1e-20)\n",
        "        output = torch.zeros_like(x2)\n",
        "        if seq_len is None:\n",
        "            for i in range(x.shape[0]):\n",
        "                tmp = x2[i]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i] = adj2\n",
        "        else:\n",
        "            for i in range(len(seq_len)):\n",
        "                tmp = x2[i, :seq_len[i], :seq_len[i]]\n",
        "                adj2 = tmp\n",
        "                adj2 = F.threshold(adj2, 0.7, 0)\n",
        "                adj2 = soft(adj2)\n",
        "                output[i, :seq_len[i], :seq_len[i]] = adj2\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE8lvyeyTk0C",
        "outputId": "3067c6fc-1bfb-4224-8e36-a78878761a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /mydrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Current directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm5mBbv_MAtW"
      },
      "source": [
        "### E) Make List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ryqi8mHyNyo9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "root_path = '/content/final_dl/list/xx/test'    ## the path of features\n",
        "files = sorted(glob.glob(os.path.join(root_path, \"*.npy\")))\n",
        "\n",
        "# print(files)\n",
        "\n",
        "# print(files)\n",
        "violents = []\n",
        "normal = []\n",
        "with open('audio_test.list', 'w+') as f:  ## the name of feature list\n",
        "    for file in files:\n",
        "        if '_label_A' in file:\n",
        "            normal.append(file)\n",
        "            # print(file)\n",
        "        else:\n",
        "            newline = file+'\\n'\n",
        "            f.write(newline)\n",
        "    for file in normal:\n",
        "        newline = file+'\\n'\n",
        "        f.write(newline)\n",
        "\n",
        "print(normal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvjQEBc_MFc7"
      },
      "source": [
        "## F) Updating the Lists files\n",
        "\n",
        "The list files are used to specify the paths of all features (audio and visual) for testing and training. The features are just the latent video and audio information for training and testing (VGGish model for audio, i3d model for video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcvPLGFjV9de",
        "outputId": "3cbc62bc-5155-4667-98cc-793ed18d305f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/rgb_test.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/rgb_test.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/dl_files/i3d-features/RGBTest\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/rgb_test.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1omdKx6CM0_",
        "outputId": "bc70d0d5-edfe-43e4-93c4-d4fb3a8bfea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/audio_test.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/audio_test.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/list/xx/test/\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/audio_test.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z-JVZlBNeg4"
      },
      "source": [
        "## Args\n",
        "\n",
        "Here are the default args that were obtained via cmd line arg parser. I just created a class 'Args' that holds the default config for the model.\n",
        "\n",
        "I think the most important args:\n",
        "\n",
        "*`Modality`*: Determines whether we want to use either audio alone, video alone, both audio and video, audio, video, and flow, etc. for training\n",
        "\n",
        "*`List`*: point to the list containing filenames for all training and testing data.\n",
        "\n",
        "*`workers`*: I believe this is the number of individual threads/processes running during training or testing. In ther model it was set to 4 by defualt but that spit out an error so it lowered it to 1. Prob a sign that we need to do heavy downsampling to compensate for lack of parallel processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "0g_4ciyOM8tl"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "      self.modality = 'MIX2'\n",
        "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "      self.flow_list = '/content/final_dl/list/flow.list'\n",
        "      self.audio_list = '/content/final_dl/list/audio.list'\n",
        "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "      self.gt = '/content/final_dl/list/gt.npy'\n",
        "      self.gpus = 1\n",
        "      self.lr = 0.0001\n",
        "      self.batch_size = 128\n",
        "      self.workers = 1\n",
        "      self.model_name = 'wsanodet'\n",
        "      self.pretrained_ckpt = None\n",
        "      self.feature_size = 1152  # 1024 + 128\n",
        "      self.num_classes = 1\n",
        "      self.dataset_name = 'XD-Violence'\n",
        "      self.max_seqlen = 200\n",
        "      self.max_epoch = 50\n",
        "\n",
        "  # Create an instance of the Args class\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NCyNXLPN5u0"
      },
      "source": [
        "## Testing PreTrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkT8DU7V3HOz",
        "outputId": "d5c631ba-a41e-4bcc-9df1-9acb3c54104f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-209-778b192ae798>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  {k.replace('module.', ''): v for k, v in torch.load('final_dl/wsanodet_mix2.pkl').items()})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time:12.470504760742188\n",
            "offline pr_auc:0.79; online pr_auc:0.7433\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "# from model import Model\n",
        "# from dataset import Dataset\n",
        "# from test import test\n",
        "# import option\n",
        "import time\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  test_loader = DataLoader(Dataset(args, test_mode=True),\n",
        "                            batch_size=5, shuffle=False,\n",
        "                            num_workers=args.workers, pin_memory=True)\n",
        "  model = Model(args)\n",
        "  model = model.to(device)\n",
        "\n",
        "  model_dict = model.load_state_dict(\n",
        "      {k.replace('module.', ''): v for k, v in torch.load('final_dl/wsanodet_mix2.pkl').items()})\n",
        "\n",
        "  gt = np.load(args.gt)\n",
        "  st = time.time()\n",
        "  pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "  print('Time:{}'.format(time.time()-st))\n",
        "  print('offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIbLkkivMhx"
      },
      "source": [
        "how to save a model for the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "k7mSdUwSvLTq"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"/content/test.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0S5vwXjQzID"
      },
      "source": [
        "# Training HLNET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "Lv0i1DYkQ2Qj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def CLAS(logits, label, seq_len, criterion, device, is_topk=True):\n",
        "    logits = logits.squeeze()\n",
        "    instance_logits = torch.zeros(0).to(device)  # tensor([])\n",
        "    for i in range(logits.shape[0]):\n",
        "        if is_topk:\n",
        "            tmp, _ = torch.topk(logits[i][:seq_len[i]], k=int(seq_len[i]//16+1), largest=True)\n",
        "            tmp = torch.mean(tmp).view(1)\n",
        "        else:\n",
        "            tmp = torch.mean(logits[i, :seq_len[i]]).view(1)\n",
        "        instance_logits = torch.cat((instance_logits, tmp))\n",
        "\n",
        "    instance_logits = torch.sigmoid(instance_logits)\n",
        "\n",
        "    clsloss = criterion(instance_logits, label)\n",
        "    return clsloss\n",
        "\n",
        "\n",
        "def CENTROPY(logits, logits2, seq_len, device):\n",
        "    instance_logits = torch.tensor(0).to(device)  # tensor([])\n",
        "    for i in range(logits.shape[0]):\n",
        "        tmp1 = torch.sigmoid(logits[i, :seq_len[i]]).squeeze()\n",
        "        tmp2 = torch.sigmoid(logits2[i, :seq_len[i]]).squeeze()\n",
        "        loss = torch.mean(-tmp1.detach() * torch.log(tmp2))\n",
        "        instance_logits = instance_logits + loss\n",
        "    instance_logits = instance_logits/logits.shape[0]\n",
        "    return instance_logits\n",
        "\n",
        "\n",
        "def train(dataloader, model, optimizer, criterion, device, is_topk):\n",
        "    with torch.set_grad_enabled(True):\n",
        "        model.train()\n",
        "        for i, (input, label) in enumerate(dataloader):\n",
        "            seq_len = torch.sum(torch.max(torch.abs(input), dim=2)[0]>0, 1)\n",
        "            input = input[:, :torch.max(seq_len), :]\n",
        "            input, label = input.float().to(device), label.float().to(device)\n",
        "            logits, logits2 = model(input, seq_len)\n",
        "            clsloss = CLAS(logits, label, seq_len, criterion, device, is_topk)\n",
        "            clsloss2 = CLAS(logits2, label, seq_len, criterion, device, is_topk)\n",
        "            croloss = CENTROPY(logits, logits2, seq_len, device)\n",
        "\n",
        "            total_loss = clsloss + clsloss2 + 5*croloss\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcLzmEr4HIQY",
        "outputId": "a2bcf406-272e-4df6-ae5b-28c782e3e9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/audio.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/audio.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/list/xx/train\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/audio.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mubL6MwpIS8k",
        "outputId": "81ebfaec-fb8c-4e61-e7ca-4e7653ed869f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated paths have been written to /content/final_dl/list/rgb.list\n"
          ]
        }
      ],
      "source": [
        "# Define the input .list file containing the original file paths\n",
        "input_list_file = \"/content/final_dl/list/rgb.list\"\n",
        "\n",
        "# Define the directory to update the paths to\n",
        "new_directory = \"/content/final_dl/dl_files/i3d-features/RGB\"\n",
        "\n",
        "# Define the output .list file for the updated file paths\n",
        "output_list_file = \"/content/final_dl/list/rgb.list\"\n",
        "\n",
        "# Read the original file paths from the .list file\n",
        "with open(input_list_file, \"r\") as file:\n",
        "    original_paths = file.readlines()\n",
        "\n",
        "# Process and update each file path\n",
        "updated_paths = []\n",
        "for path in original_paths:\n",
        "    path = path.strip()  # Remove any leading/trailing whitespace or newlines\n",
        "    if path:  # Ensure the path is not empty\n",
        "        # Extract the filename from the original path and create a new path\n",
        "        filename = path.split(\"/\")[-1]\n",
        "        updated_path = f\"{new_directory}/{filename}\"\n",
        "        updated_paths.append(updated_path)\n",
        "\n",
        "# Write the updated paths to the output .list file\n",
        "with open(output_list_file, \"w\") as file:\n",
        "    file.write(\"\\n\".join(updated_paths))\n",
        "\n",
        "print(f\"Updated paths have been written to {output_list_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYz5pRU7Iyf8"
      },
      "source": [
        "## test (ignore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTEZP14qIyOD",
        "outputId": "3067bd70-3ce8-4c53-a00a-78f5f39585d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 200, 1152])\n",
            "torch.Size([5, 200, 1152])\n",
            "torch.Size([5, 200, 1152])\n"
          ]
        }
      ],
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "      self.modality = 'MIX2'\n",
        "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "      self.flow_list = '/content/final_dl/list/flow.list'\n",
        "      self.audio_list = '/content/final_dl/list/audio.list'\n",
        "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "      self.gt = '/content/final_dl/list/gt.npy'\n",
        "      self.gpus = 1\n",
        "      self.lr = 0.0001\n",
        "      self.batch_size = 128\n",
        "      self.workers = 1\n",
        "      self.model_name = 'wsanodet'\n",
        "      self.pretrained_ckpt = None\n",
        "      self.feature_size = 1152  # 1024 + 128\n",
        "      self.num_classes = 1\n",
        "      self.dataset_name = 'XD-Violence'\n",
        "      self.max_seqlen = 200\n",
        "      self.max_epoch = 50\n",
        "\n",
        "  # Create an instance of the Args class\n",
        "args = Args()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "test_loader = DataLoader(Dataset(args, test_mode=False),\n",
        "                          batch_size=5, shuffle=True,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (input,label) in enumerate(test_loader):\n",
        "    input = input.to(device)\n",
        "\n",
        "    print(input.shape)\n",
        "    ############\n",
        "    ### NOTE: ## setting seq_len to None pads training data in the sequence dim to 200\n",
        "    ############\n",
        "    logits, logits2 = model(inputs=input, seq_len=None)\n",
        "    # print(logits, logits2)\n",
        "    if i == 2:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training HL NET"
      ],
      "metadata": {
        "id": "SM88zu_-ADPw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pFJMkujfFROw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "# import option\n",
        "\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# torch.multiprocessing.set_start_method('spawn')\n",
        "# setup_seed(2333)\n",
        "# args = option.parser.parse_args()\n",
        "\n",
        "!export TORCH_USE_CUDA_DSA=ON\n",
        "device = torch.device(\"cuda\")\n",
        "train_loader = DataLoader(Dataset(args, test_mode=False),\n",
        "                          batch_size=args.batch_size, shuffle=True,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "test_loader = DataLoader(Dataset(args, test_mode=True),\n",
        "                          batch_size=5, shuffle=False,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "for name, value in model.named_parameters():\n",
        "    print(name)\n",
        "approximator_param = list(map(id, model.approximator.parameters()))\n",
        "approximator_param += list(map(id, model.conv1d_approximator.parameters()))\n",
        "base_param = filter(lambda p: id(p) not in approximator_param, model.parameters())\n",
        "\n",
        "if not os.path.exists('./ckpt'):\n",
        "    os.makedirs('./ckpt')\n",
        "optimizer = optim.Adam([{'params': base_param},\n",
        "                        {'params': model.approximator.parameters(), 'lr': args.lr / 2},\n",
        "                        {'params': model.conv1d_approximator.parameters(), 'lr': args.lr / 2},\n",
        "                        ],\n",
        "                        lr=args.lr, weight_decay=0.000)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "is_topk = True\n",
        "gt = np.load(args.gt)\n",
        "pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "print('Random initalization: offline pr_auc:{0:.4}; online pr_auc:{1:.4}\\n'.format(pr_auc, pr_auc_online))\n",
        "for epoch in range(args.max_epoch):\n",
        "    scheduler.step()\n",
        "    st = time.time()\n",
        "    train(train_loader, model, optimizer, criterion, device, is_topk)\n",
        "    if epoch % 2 == 0 and not epoch == 0:\n",
        "        torch.save(model.state_dict(), './ckpt/'+args.model_name+'{}.pkl'.format(epoch))\n",
        "\n",
        "    pr_auc, pr_auc_online = test(test_loader, model, device, gt)\n",
        "    print('Epoch {0}/{1}: offline pr_auc:{2:.4}; online pr_auc:{3:.4}\\n'.format(epoch, args.max_epoch, pr_auc, pr_auc_online))\n",
        "torch.save(model.state_dict(), './ckpt/' + args.model_name + '.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYjtaIacOHyS"
      },
      "source": [
        "# Training VAE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE MODEL"
      ],
      "metadata": {
        "id": "FRzWrGepsWur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R8Ma-ndq_5GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Sampling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, z_means, z_log_vars):\n",
        "        epsilon = torch.randn_like(z_means, dtype=torch.float32)\n",
        "        return z_means + torch.exp(0.5 * z_log_vars) * epsilon\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1024, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, 512, kernel_size=3, stride=2, padding=1),  # (B, 1024, 200) -> (B, 512, 100)\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(512, 256, kernel_size=3, stride=2, padding=1),         # (B, 512, 100) -> (B, 256, 50)\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(256, 128, kernel_size=3, stride=2, padding=1),         # (B, 256, 50) -> (B, 128, 25)\n",
        "            nn.ReLU(True),\n",
        "            nn.Flatten()  # Flatten for fully connected layers\n",
        "        )\n",
        "\n",
        "        flattened_dim = 25 * 128  # Calculate flattened dimension\n",
        "        self.lin_mean = nn.Linear(flattened_dim, latent_dim)\n",
        "        self.lin_log_var = nn.Linear(flattened_dim, latent_dim)\n",
        "        self.sampling = Sampling()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        z_means = self.lin_mean(x)\n",
        "        z_log_vars = self.lin_log_var(x)\n",
        "        z = self.sampling(z_means, z_log_vars)\n",
        "        return z, z_means, z_log_vars\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1024, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        flattened_dim = 25 * 128  # Must match Encoder's flattened_dim (200)\n",
        "\n",
        "        self.decoder_fc = nn.Sequential(\n",
        "            nn.Linear(latent_dim, flattened_dim),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose1d(128, 256, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 128, 25) -> (B, 256, 50)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 256, 50) -> (B, 512, 100)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(512, input_dim, kernel_size=3, stride=2, padding=1, output_padding=1),  # (B, 512, 100) -> (B, 1024, 200)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.decoder_fc(x)  # Fully connected layer\n",
        "        x = x.view(-1, 128, 25)  # Reshape to match ConvTranspose1D input\n",
        "        x = self.decoder_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim, input_dim=1024, seq_len=200):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_dim, input_dim, seq_len)\n",
        "        self.decoder = Decoder(latent_dim, input_dim, seq_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, z_means, z_log_vars = self.encoder(x)\n",
        "        x_reconstructed = self.decoder(z)\n",
        "        x_reconstructed = x_reconstructed.view(-1, 200, 1024)\n",
        "        return x_reconstructed, z_means, z_log_vars"
      ],
      "metadata": {
        "collapsed": true,
        "id": "r7Hlyuuos9DQ"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "      self.modality = 'RGB'\n",
        "      self.rgb_list = '/content/final_dl/list/rgb.list'\n",
        "      self.flow_list = '/content/final_dl/list/flow.list'\n",
        "      self.audio_list = '/content/final_dl/list/audio.list'\n",
        "      self.test_rgb_list = '/content/final_dl/list/rgb_test.list'\n",
        "      self.test_flow_list = '/content/final_dl/list/flow_test.list'\n",
        "      self.test_audio_list = '/content/final_dl/list/audio_test.list'\n",
        "      self.gt = '/content/final_dl/list/gt.npy'\n",
        "      self.gpus = 1\n",
        "      self.lr = 0.0001\n",
        "      self.batch_size = 1\n",
        "      self.workers = 1\n",
        "      self.model_name = 'wsanodet'\n",
        "      self.pretrained_ckpt = None\n",
        "      # self.feature_size = 1152  # 1024 + 128\n",
        "      self.feature_size = 1024\n",
        "      self.num_classes = 1\n",
        "      self.dataset_name = 'XD-Violence'\n",
        "      self.max_seqlen = 200\n",
        "      self.max_epoch = 50\n",
        "\n",
        "  # Create an instance of the Args class\n",
        "args = Args()\n",
        "\n"
      ],
      "metadata": {
        "id": "T9Bi_42c8Tts"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE Train FN"
      ],
      "metadata": {
        "id": "DQrX4xU68twm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_loader = DataLoader(Dataset(args, test_mode=False),\n",
        "                          batch_size=args.batch_size, shuffle=True,\n",
        "                          num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(args)\n",
        "model = model.cuda()\n",
        "\n",
        "with torch.set_grad_enabled(True):\n",
        "  model.train()\n",
        "  for i, (input, label) in enumerate(train_loader):\n",
        "\n",
        "    input, label = input.float().to(torch.float32), label.float().to(device)\n",
        "\n",
        "    # input and label from train dataset\n",
        "    print(\"shape of input: \", input.shape)\n",
        "    print(\"shape of label: \", label.shape)\n",
        "\n",
        "    input = input.to(device)\n",
        "\n",
        "    # size of one sample from batch\n",
        "    test_input = input[:1][::][::]\n",
        "\n",
        "    # permuting input shape to fit vae cnn layers\n",
        "    test_input = test_input.permute(0, 2, 1)\n",
        "    print(\"test_input shape: \", test_input.shape)\n",
        "\n",
        "    # passing through vae\n",
        "    vae = VAE(latent_dim=128, input_dim=1024, seq_len=200)\n",
        "    vae = vae.to(device)\n",
        "\n",
        "    x_reconstructed, z_means, z_log_vars = vae(test_input)\n",
        "\n",
        "    print(\"reconstructed input shape (after vae): \", x_reconstructed.shape)\n",
        "\n",
        "    ## break statement just to read out this input/output shapes in the first iteration\n",
        "    if i == 0:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot6yc6GL8eTi",
        "outputId": "36be32f4-6da6-4220-b546-bfdf2b23033a"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n",
            "shape of input:  torch.Size([1, 200, 1024])\n",
            "shape of label:  torch.Size([1])\n",
            "test_input shape:  torch.Size([1, 1024, 200])\n",
            "reconstructed input shape (after vae):  torch.Size([1, 200, 1024])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1XTC5Esw4CBi_Xs00PGh-LAkkDmj1oR9v",
      "authorship_tag": "ABX9TyNECtFX8TX74NW/JHgCHVQP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}